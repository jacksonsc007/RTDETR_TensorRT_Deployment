[96m[INFO] Origianl output [0m
['labels', 'boxes', 'scores']
[W] 'colored' module is not installed, will not use colors when logging. To enable colors, please install the 'colored' module: python3 -m pip install colored
[V] Marking all ONNX tensors as outputs
[V] Loaded Module: onnx | Version: 1.15.0 | Path: ['/root/workspace/development/RTDETR_TensorRT_Deployment/.venv/lib/python3.10/site-packages/onnx']
[96m[INFO] output to be compared [0m
['/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0',
 '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0',
 'labels',
 'boxes',
 'scores']
[I] trt-runner-N0-05/22/25-09:24:08     | Activating and starting inference
[V] Loaded Module: tensorrt | Version: 10.7.0 | Path: ['/root/workspace/development/RTDETR_TensorRT_Deployment/.venv/lib/python3.10/site-packages/tensorrt']
[V] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.
[X] CUDA lazy loading is enabled.
[X] Registered plugin creator - ::ROIAlign_TRT version 2
[X] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1
[X] Registered plugin creator - ::BatchedNMS_TRT version 1
[X] Registered plugin creator - ::BatchTilePlugin_TRT version 1
[X] Registered plugin creator - ::Clip_TRT version 1
[X] Registered plugin creator - ::CoordConvAC version 1
[X] Registered plugin creator - ::CropAndResizeDynamic version 1
[X] Registered plugin creator - ::CropAndResize version 1
[X] Registered plugin creator - ::DecodeBbox3DPlugin version 1
[X] Registered plugin creator - ::DetectionLayer_TRT version 1
[X] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1
[X] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1
[X] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1
[X] Registered plugin creator - ::EfficientNMS_TRT version 1
[X] Registered plugin creator - ::FlattenConcat_TRT version 1
[X] Registered plugin creator - ::GenerateDetection_TRT version 1
[X] Registered plugin creator - ::GridAnchor_TRT version 1
[X] Registered plugin creator - ::GridAnchorRect_TRT version 1
[X] Registered plugin creator - ::InstanceNormalization_TRT version 1
[X] Registered plugin creator - ::InstanceNormalization_TRT version 2
[X] Registered plugin creator - ::InstanceNormalization_TRT version 3
[X] Registered plugin creator - ::LReLU_TRT version 1
[X] Registered plugin creator - ::ModulatedDeformConv2d version 1
[X] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1
[X] Registered plugin creator - ::MultilevelProposeROI_TRT version 1
[X] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1
[X] Registered plugin creator - ::NMSDynamic_TRT version 1
[X] Registered plugin creator - ::NMS_TRT version 1
[X] Registered plugin creator - ::Normalize_TRT version 1
[X] Registered plugin creator - ::PillarScatterPlugin version 1
[X] Registered plugin creator - ::PriorBox_TRT version 1
[X] Registered plugin creator - ::ProposalDynamic version 1
[X] Registered plugin creator - ::ProposalLayer_TRT version 1
[X] Registered plugin creator - ::Proposal version 1
[X] Registered plugin creator - ::PyramidROIAlign_TRT version 1
[X] Registered plugin creator - ::Region_TRT version 1
[X] Registered plugin creator - ::Reorg_TRT version 2
[X] Registered plugin creator - ::Reorg_TRT version 1
[X] Registered plugin creator - ::ResizeNearest_TRT version 1
[X] Registered plugin creator - ::ROIAlign_TRT version 1
[X] Registered plugin creator - ::RPROI_TRT version 1
[X] Registered plugin creator - ::ScatterElements version 1
[X] Registered plugin creator - ::ScatterElements version 2
[X] Registered plugin creator - ::ScatterND version 1
[X] Registered plugin creator - ::SpecialSlice_TRT version 1
[X] Registered plugin creator - ::Split version 1
[X] Registered plugin creator - ::VoxelGeneratorPlugin version 1
[V] ----------------------------------------------------------------
[V] Input filename:   default_mtq_int8_q_qint8break_fusion-output_modified.onnx
[V] ONNX IR version:  0.0.8
[V] Opset version:    17
[V] Producer name:    pytorch
[V] Producer version: 2.5.0
[V] Domain:           
[V] Model version:    0
[V] Doc string:       
[V] ----------------------------------------------------------------
[X] Adding network input: images with dtype: float32, dimensions: (1, 3, 640, 640)
[X] Registering tensor: images for ONNX tensor: images
[X] Adding network input: orig_target_sizes with dtype: int64, dimensions: (1, 2)
[W] ModelImporter.cpp:459: Make sure input orig_target_sizes has Int64 binding.
[X] Registering tensor: orig_target_sizes for ONNX tensor: orig_target_sizes
[X] Importing initializer: model.backbone.conv1.conv1_1.conv.weight
[X] Importing initializer: model.backbone.conv1.conv1_1.norm.weight
[X] Importing initializer: model.backbone.conv1.conv1_1.norm.bias
[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_mean
[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_var
[X] Importing initializer: model.backbone.conv1.conv1_2.conv.weight
[X] Importing initializer: model.backbone.conv1.conv1_2.norm.weight
[X] Importing initializer: model.backbone.conv1.conv1_2.norm.bias
[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_mean
[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_var
[X] Importing initializer: model.backbone.conv1.conv1_3.conv.weight
[X] Importing initializer: model.backbone.conv1.conv1_3.norm.weight
[X] Importing initializer: model.backbone.conv1.conv1_3.norm.bias
[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_mean
[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.bias
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var
[X] Importing initializer: model.decoder.anchors
[X] Importing initializer: model.decoder.input_proj.0.conv.weight
[X] Importing initializer: model.decoder.input_proj.0.norm.weight
[X] Importing initializer: model.decoder.input_proj.0.norm.bias
[X] Importing initializer: model.decoder.input_proj.0.norm.running_mean
[X] Importing initializer: model.decoder.input_proj.0.norm.running_var
[X] Importing initializer: model.decoder.input_proj.1.conv.weight
[X] Importing initializer: model.decoder.input_proj.1.norm.weight
[X] Importing initializer: model.decoder.input_proj.1.norm.bias
[X] Importing initializer: model.decoder.input_proj.1.norm.running_mean
[X] Importing initializer: model.decoder.input_proj.1.norm.running_var
[X] Importing initializer: model.decoder.input_proj.2.conv.weight
[X] Importing initializer: model.decoder.input_proj.2.norm.weight
[X] Importing initializer: model.decoder.input_proj.2.norm.bias
[X] Importing initializer: model.decoder.input_proj.2.norm.running_mean
[X] Importing initializer: model.decoder.input_proj.2.norm.running_var
[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.0.norm1.weight
[X] Importing initializer: model.decoder.decoder.layers.0.norm1.bias
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.0.norm2.weight
[X] Importing initializer: model.decoder.decoder.layers.0.norm2.bias
[X] Importing initializer: model.decoder.decoder.layers.0.linear1.weight
[X] Importing initializer: model.decoder.decoder.layers.0.linear1.bias
[X] Importing initializer: model.decoder.decoder.layers.0.linear2.weight
[X] Importing initializer: model.decoder.decoder.layers.0.linear2.bias
[X] Importing initializer: model.decoder.decoder.layers.0.norm3.weight
[X] Importing initializer: model.decoder.decoder.layers.0.norm3.bias
[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.1.norm1.weight
[X] Importing initializer: model.decoder.decoder.layers.1.norm1.bias
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.1.norm2.weight
[X] Importing initializer: model.decoder.decoder.layers.1.norm2.bias
[X] Importing initializer: model.decoder.decoder.layers.1.linear1.weight
[X] Importing initializer: model.decoder.decoder.layers.1.linear1.bias
[X] Importing initializer: model.decoder.decoder.layers.1.linear2.weight
[X] Importing initializer: model.decoder.decoder.layers.1.linear2.bias
[X] Importing initializer: model.decoder.decoder.layers.1.norm3.weight
[X] Importing initializer: model.decoder.decoder.layers.1.norm3.bias
[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.2.norm1.weight
[X] Importing initializer: model.decoder.decoder.layers.2.norm1.bias
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.2.norm2.weight
[X] Importing initializer: model.decoder.decoder.layers.2.norm2.bias
[X] Importing initializer: model.decoder.decoder.layers.2.linear1.weight
[X] Importing initializer: model.decoder.decoder.layers.2.linear1.bias
[X] Importing initializer: model.decoder.decoder.layers.2.linear2.weight
[X] Importing initializer: model.decoder.decoder.layers.2.linear2.bias
[X] Importing initializer: model.decoder.decoder.layers.2.norm3.weight
[X] Importing initializer: model.decoder.decoder.layers.2.norm3.bias
[X] Importing initializer: model.decoder.query_pos_head.layers.0.weight
[X] Importing initializer: model.decoder.query_pos_head.layers.0.bias
[X] Importing initializer: model.decoder.query_pos_head.layers.1.weight
[X] Importing initializer: model.decoder.query_pos_head.layers.1.bias
[X] Importing initializer: model.decoder.enc_output.proj.weight
[X] Importing initializer: model.decoder.enc_output.proj.bias
[X] Importing initializer: model.decoder.enc_output.norm.weight
[X] Importing initializer: model.decoder.enc_output.norm.bias
[X] Importing initializer: model.decoder.enc_score_head.weight
[X] Importing initializer: model.decoder.enc_score_head.bias
[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.weight
[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.bias
[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.weight
[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.bias
[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.weight
[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.bias
[X] Importing initializer: model.decoder.dec_score_head.2.weight
[X] Importing initializer: model.decoder.dec_score_head.2.bias
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.weight
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.bias
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.weight
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.bias
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.weight
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.bias
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.weight
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.bias
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.weight
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.bias
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.weight
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.bias
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.weight
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.bias
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.weight
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.bias
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.weight
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.bias
[X] Importing initializer: model.encoder.input_proj.0.conv.weight
[X] Importing initializer: model.encoder.input_proj.0.norm.weight
[X] Importing initializer: model.encoder.input_proj.0.norm.bias
[X] Importing initializer: model.encoder.input_proj.0.norm.running_mean
[X] Importing initializer: model.encoder.input_proj.0.norm.running_var
[X] Importing initializer: model.encoder.input_proj.1.conv.weight
[X] Importing initializer: model.encoder.input_proj.1.norm.weight
[X] Importing initializer: model.encoder.input_proj.1.norm.bias
[X] Importing initializer: model.encoder.input_proj.1.norm.running_mean
[X] Importing initializer: model.encoder.input_proj.1.norm.running_var
[X] Importing initializer: model.encoder.input_proj.2.conv.weight
[X] Importing initializer: model.encoder.input_proj.2.norm.weight
[X] Importing initializer: model.encoder.input_proj.2.norm.bias
[X] Importing initializer: model.encoder.input_proj.2.norm.running_mean
[X] Importing initializer: model.encoder.input_proj.2.norm.running_var
[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias
[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.bias
[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.bias
[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.bias
[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.bias
[X] Importing initializer: model.encoder.lateral_convs.0.conv.weight
[X] Importing initializer: model.encoder.lateral_convs.0.norm.weight
[X] Importing initializer: model.encoder.lateral_convs.0.norm.bias
[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_mean
[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_var
[X] Importing initializer: model.encoder.lateral_convs.1.conv.weight
[X] Importing initializer: model.encoder.lateral_convs.1.norm.weight
[X] Importing initializer: model.encoder.lateral_convs.1.norm.bias
[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_mean
[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_var
[X] Importing initializer: model.encoder.downsample_convs.0.conv.weight
[X] Importing initializer: model.encoder.downsample_convs.0.norm.weight
[X] Importing initializer: model.encoder.downsample_convs.0.norm.bias
[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_mean
[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_var
[X] Importing initializer: model.encoder.downsample_convs.1.conv.weight
[X] Importing initializer: model.encoder.downsample_convs.1.norm.weight
[X] Importing initializer: model.encoder.downsample_convs.1.norm.bias
[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_mean
[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_var
[X] Importing initializer: onnx::Add_3614
[X] Importing initializer: onnx::Add_3616
[X] Importing initializer: onnx::Add_3618
[X] Importing initializer: onnx::MatMul_3619
[X] Importing initializer: onnx::MatMul_3620
[X] Importing initializer: onnx::MatMul_3621
[X] Importing initializer: onnx::Mul_3692
[X] Importing initializer: onnx::Add_3731
[X] Importing initializer: onnx::Add_3733
[X] Importing initializer: onnx::Add_3735
[X] Importing initializer: onnx::MatMul_3736
[X] Importing initializer: onnx::MatMul_3737
[X] Importing initializer: onnx::MatMul_3738
[X] Importing initializer: onnx::Mul_3755
[X] Importing initializer: onnx::Add_3803
[X] Importing initializer: onnx::Add_3805
[X] Importing initializer: onnx::Add_3807
[X] Importing initializer: onnx::MatMul_3808
[X] Importing initializer: onnx::MatMul_3809
[X] Importing initializer: onnx::MatMul_3810
[X] Importing initializer: onnx::Add_3875
[X] Importing initializer: onnx::Add_3877
[X] Importing initializer: onnx::Add_3879
[X] Importing initializer: onnx::MatMul_3880
[X] Importing initializer: onnx::MatMul_3881
[X] Importing initializer: onnx::MatMul_3882
[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/Constant_2_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/Constant_output_0
[X] Importing initializer: onnx::Unsqueeze_1255
[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/Constant_7_output_0
[X] Importing initializer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/Constant_9_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/Constant_18_output_0
[X] Importing initializer: /model/decoder/Constant_21_output_0
[X] Importing initializer: /model/decoder/decoder/Constant_output_0
[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0
[X] Importing initializer: onnx::Split_2305
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/Constant_3_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /postprocessor/Constant_output_0
[X] Importing initializer: onnx::Tile_3498
[X] Importing initializer: /postprocessor/Constant_14_output_0
[X] Importing initializer: _v_4326
[X] Importing initializer: _v_1997
[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0
[X] Importing initializer: /model/encoder/Concat_1_output_0
[X] Importing initializer: /model/decoder/Concat_5_output_0
[X] Importing initializer: /model/decoder/Concat_7_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0
[X] Importing initializer: _v_1846
[X] Importing initializer: _v_1848
[X] Importing initializer: _v_1850
[X] Importing initializer: _v_1749
[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0
[X] Importing initializer: _v_1663
[X] Importing initializer: _v_1665
[X] Importing initializer: _v_1669
[X] Importing initializer: _v_1675
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: images
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [images -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight for ONNX node: tmp_weight
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_0 for ONNX node: tmp_weight_0
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.conv1.conv1_1.conv.weight
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_1.conv.weight -> (32, 3, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], 
[X] Registering layer: model.backbone.conv1.conv1_1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1 for ONNX node: tmp_weight_1
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], 
[X] Registering layer: tmp_weight_2 for ONNX node: tmp_weight_2
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv]
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/Conv for ONNX node: /model/backbone/conv1/conv1_1/conv/Conv
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0
[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/Conv_output_0
[X] Searching for input: model.backbone.conv1.conv1_1.norm.weight
[X] Searching for input: model.backbone.conv1.conv1_1.norm.bias
[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_mean
[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_var
[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_1.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_var -> (32)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_1/norm/BatchNormalization
[X] Registering tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu]
[X] Parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu]
[X] Searching for input: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Registering layer: /model/backbone/conv1/conv1_1/act/Relu for ONNX node: /model/backbone/conv1/conv1_1/act/Relu
[X] Registering tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0
[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_1/act/Relu_output_0
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_3 for ONNX node: tmp_weight_3
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_4 for ONNX node: tmp_weight_4
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.conv1.conv1_2.conv.weight
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_2.conv.weight -> (32, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], 
[X] Registering layer: model.backbone.conv1.conv1_2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_5 for ONNX node: tmp_weight_5
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], 
[X] Registering layer: tmp_weight_6 for ONNX node: tmp_weight_6
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv]
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/Conv for ONNX node: /model/backbone/conv1/conv1_2/conv/Conv
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0
[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/Conv_output_0
[X] Searching for input: model.backbone.conv1.conv1_2.norm.weight
[X] Searching for input: model.backbone.conv1.conv1_2.norm.bias
[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_mean
[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_var
[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_2.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_var -> (32)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_2/norm/BatchNormalization
[X] Registering tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu]
[X] Parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu]
[X] Searching for input: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Registering layer: /model/backbone/conv1/conv1_2/act/Relu for ONNX node: /model/backbone/conv1/conv1_2/act/Relu
[X] Registering tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0
[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_2/act/Relu_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_7 for ONNX node: tmp_weight_7
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_8 for ONNX node: tmp_weight_8
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.conv1.conv1_3.conv.weight
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_3.conv.weight -> (64, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.conv1.conv1_3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_9 for ONNX node: tmp_weight_9
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_10 for ONNX node: tmp_weight_10
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv]
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/Conv for ONNX node: /model/backbone/conv1/conv1_3/conv/Conv
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0
[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/Conv_output_0
[X] Searching for input: model.backbone.conv1.conv1_3.norm.weight
[X] Searching for input: model.backbone.conv1.conv1_3.norm.bias
[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_mean
[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_var
[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_3.norm.weight -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.bias -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_mean -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_3/norm/BatchNormalization
[X] Registering tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu]
[X] Parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu]
[X] Searching for input: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Registering layer: /model/backbone/conv1/conv1_3/act/Relu for ONNX node: /model/backbone/conv1/conv1_3/act/Relu
[X] Registering tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0
[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/MaxPool [MaxPool]
[X] Parsing node: /model/backbone/MaxPool [MaxPool]
[X] Searching for input: /model/backbone/conv1/conv1_3/act/Relu_output_0
[X] /model/backbone/MaxPool [MaxPool] inputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Registering layer: /model/backbone/MaxPool for ONNX node: /model/backbone/MaxPool
[X] Registering tensor: /model/backbone/MaxPool_output_0 for ONNX tensor: /model/backbone/MaxPool_output_0
[X] /model/backbone/MaxPool [MaxPool] outputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/MaxPool_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_11 for ONNX node: tmp_weight_11
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_12 for ONNX node: tmp_weight_12
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_13 for ONNX node: tmp_weight_13
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_14 for ONNX node: tmp_weight_14
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var
[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_15 for ONNX node: tmp_weight_15
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_16 for ONNX node: tmp_weight_16
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_17 for ONNX node: tmp_weight_17
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_18 for ONNX node: tmp_weight_18
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var
[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.short.conv.weight -> (64, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.0.short.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_19 for ONNX node: tmp_weight_19
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_20 for ONNX node: tmp_weight_20
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_var
[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/Add for ONNX node: /model/backbone/res_layers.0/blocks.0/Add
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0
[X] /model/backbone/res_layers.0/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/Add_output_0
[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/act/Relu
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_21 for ONNX node: tmp_weight_21
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_22 for ONNX node: tmp_weight_22
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_23 for ONNX node: tmp_weight_23
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_24 for ONNX node: tmp_weight_24
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var
[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_25 for ONNX node: tmp_weight_25
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_26 for ONNX node: tmp_weight_26
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_27 for ONNX node: tmp_weight_27
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_28 for ONNX node: tmp_weight_28
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var
[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/Add for ONNX node: /model/backbone/res_layers.0/blocks.1/Add
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0
[X] /model/backbone/res_layers.0/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/Add_output_0
[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/act/Relu
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_29 for ONNX node: tmp_weight_29
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_30 for ONNX node: tmp_weight_30
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2a.conv.weight -> (128, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_31 for ONNX node: tmp_weight_31
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_32 for ONNX node: tmp_weight_32
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var
[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_33 for ONNX node: tmp_weight_33
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_34 for ONNX node: tmp_weight_34
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_35 for ONNX node: tmp_weight_35
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_36 for ONNX node: tmp_weight_36
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var
[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_37 for ONNX node: tmp_weight_37
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_38 for ONNX node: tmp_weight_38
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.short.conv.conv.weight -> (128, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_39 for ONNX node: tmp_weight_39
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_40 for ONNX node: tmp_weight_40
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var
[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/Add for ONNX node: /model/backbone/res_layers.1/blocks.0/Add
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0
[X] /model/backbone/res_layers.1/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/Add_output_0
[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/act/Relu
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_41 for ONNX node: tmp_weight_41
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_42 for ONNX node: tmp_weight_42
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2a.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_43 for ONNX node: tmp_weight_43
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_44 for ONNX node: tmp_weight_44
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var
[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_45 for ONNX node: tmp_weight_45
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_46 for ONNX node: tmp_weight_46
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_47 for ONNX node: tmp_weight_47
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_48 for ONNX node: tmp_weight_48
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var
[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/Add for ONNX node: /model/backbone/res_layers.1/blocks.1/Add
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0
[X] /model/backbone/res_layers.1/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/Add_output_0
[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/act/Relu
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_49 for ONNX node: tmp_weight_49
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_50 for ONNX node: tmp_weight_50
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2a.conv.weight -> (256, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_51 for ONNX node: tmp_weight_51
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_52 for ONNX node: tmp_weight_52
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var
[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_53 for ONNX node: tmp_weight_53
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_54 for ONNX node: tmp_weight_54
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_55 for ONNX node: tmp_weight_55
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_56 for ONNX node: tmp_weight_56
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var
[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_57 for ONNX node: tmp_weight_57
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_58 for ONNX node: tmp_weight_58
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.short.conv.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_59 for ONNX node: tmp_weight_59
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_60 for ONNX node: tmp_weight_60
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var
[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/Add for ONNX node: /model/backbone/res_layers.2/blocks.0/Add
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0
[X] /model/backbone/res_layers.2/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/Add_output_0
[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/act/Relu
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_61 for ONNX node: tmp_weight_61
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_62 for ONNX node: tmp_weight_62
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2a.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_63 for ONNX node: tmp_weight_63
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_64 for ONNX node: tmp_weight_64
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var
[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_65 for ONNX node: tmp_weight_65
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_66 for ONNX node: tmp_weight_66
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_67 for ONNX node: tmp_weight_67
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_68 for ONNX node: tmp_weight_68
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var
[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/Add for ONNX node: /model/backbone/res_layers.2/blocks.1/Add
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0
[X] /model/backbone/res_layers.2/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/Add_output_0
[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/act/Relu
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_69 for ONNX node: tmp_weight_69
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_70 for ONNX node: tmp_weight_70
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2a.conv.weight -> (512, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_71 for ONNX node: tmp_weight_71
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_72 for ONNX node: tmp_weight_72
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var
[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_73 for ONNX node: tmp_weight_73
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_74 for ONNX node: tmp_weight_74
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_75 for ONNX node: tmp_weight_75
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_76 for ONNX node: tmp_weight_76
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var
[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_77 for ONNX node: tmp_weight_77
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_78 for ONNX node: tmp_weight_78
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.short.conv.conv.weight -> (512, 256, 1, 1)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_79 for ONNX node: tmp_weight_79
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_80 for ONNX node: tmp_weight_80
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var
[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/Add for ONNX node: /model/backbone/res_layers.3/blocks.0/Add
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0
[X] /model/backbone/res_layers.3/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/Add_output_0
[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/act/Relu
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_81 for ONNX node: tmp_weight_81
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_82 for ONNX node: tmp_weight_82
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2a.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_83 for ONNX node: tmp_weight_83
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_84 for ONNX node: tmp_weight_84
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var
[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_85 for ONNX node: tmp_weight_85
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_86 for ONNX node: tmp_weight_86
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_87 for ONNX node: tmp_weight_87
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_88 for ONNX node: tmp_weight_88
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var
[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/Add for ONNX node: /model/backbone/res_layers.3/blocks.1/Add
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0
[X] /model/backbone/res_layers.3/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/Add_output_0
[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/act/Relu
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.input_proj.0.conv.weight
[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.0.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.input_proj.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_89 for ONNX node: tmp_weight_89
[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_90 for ONNX node: tmp_weight_90
[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/input_proj.0/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.0/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/input_proj.0/conv/Conv for ONNX node: /model/encoder/input_proj.0/conv/Conv
[X] Registering tensor: /model/encoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/Conv_output_0
[X] /model/encoder/input_proj.0/conv/Conv [Conv] outputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/input_proj.0/conv/Conv_output_0
[X] Searching for input: model.encoder.input_proj.0.norm.weight
[X] Searching for input: model.encoder.input_proj.0.norm.bias
[X] Searching for input: model.encoder.input_proj.0.norm.running_mean
[X] Searching for input: model.encoder.input_proj.0.norm.running_var
[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.0/norm/BatchNormalization
[X] Registering tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0
[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.input_proj.1.conv.weight
[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.input_proj.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_91 for ONNX node: tmp_weight_91
[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_92 for ONNX node: tmp_weight_92
[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/input_proj.1/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.1/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/input_proj.1/conv/Conv for ONNX node: /model/encoder/input_proj.1/conv/Conv
[X] Registering tensor: /model/encoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/Conv_output_0
[X] /model/encoder/input_proj.1/conv/Conv [Conv] outputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/input_proj.1/conv/Conv_output_0
[X] Searching for input: model.encoder.input_proj.1.norm.weight
[X] Searching for input: model.encoder.input_proj.1.norm.bias
[X] Searching for input: model.encoder.input_proj.1.norm.running_mean
[X] Searching for input: model.encoder.input_proj.1.norm.running_var
[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0
[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0
[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_93 for ONNX node: tmp_weight_93
[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_94 for ONNX node: tmp_weight_94
[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.input_proj.2.conv.weight
[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.2.conv.weight -> (256, 512, 1, 1)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.input_proj.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_95 for ONNX node: tmp_weight_95
[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_96 for ONNX node: tmp_weight_96
[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/input_proj.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/Conv [Conv] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/input_proj.2/conv/Conv for ONNX node: /model/encoder/input_proj.2/conv/Conv
[X] Registering tensor: /model/encoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/Conv_output_0
[X] /model/encoder/input_proj.2/conv/Conv [Conv] outputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/input_proj.2/conv/Conv_output_0
[X] Searching for input: model.encoder.input_proj.2.norm.weight
[X] Searching for input: model.encoder.input_proj.2.norm.bias
[X] Searching for input: model.encoder.input_proj.2.norm.running_mean
[X] Searching for input: model.encoder.input_proj.2.norm.running_var
[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0
[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Reshape [Reshape]
[X] Parsing node: /model/encoder/Reshape [Reshape]
[X] Searching for input: /model/encoder/input_proj.2/norm/BatchNormalization_output_0
[X] Searching for input: _v_4326
[X] /model/encoder/Reshape [Reshape] inputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle required by ONNX-TRT
[X] Registering layer: /model/encoder/Reshape for ONNX node: /model/encoder/Reshape
[X] Registering tensor: /model/encoder/Reshape_output_0 for ONNX tensor: /model/encoder/Reshape_output_0
[X] /model/encoder/Reshape [Reshape] outputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Transpose [Transpose]
[X] Parsing node: /model/encoder/Transpose [Transpose]
[X] Searching for input: /model/encoder/Reshape_output_0
[X] /model/encoder/Transpose [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Registering layer: /model/encoder/Transpose for ONNX node: /model/encoder/Transpose
[X] Registering tensor: /model/encoder/Transpose_output_0 for ONNX tensor: /model/encoder/Transpose_output_0
[X] /model/encoder/Transpose [Transpose] outputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/Add [Add]
[X] Searching for input: /model/encoder/Transpose_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/Add [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/Constant_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/Constant_output_0 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/Add for ONNX node: /model/encoder/encoder.0/layers.0/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_output_0
[X] /model/encoder/encoder.0/layers.0/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/encoder/Reshape_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3619
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3619 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3619 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_97 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add]
[X] Searching for input: onnx::Add_3614
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3614 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3614 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_98 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_99 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3620
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3620 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3620 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_100 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_101 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add]
[X] Searching for input: onnx::Add_3616
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3616 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3616 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_102 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_103 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0
[X] Searching for input: onnx::MatMul_3621
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3621 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3621 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_104 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_105 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add]
[X] Searching for input: onnx::Add_3618
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3618 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3618 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_106 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_107 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_2
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_108 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_109 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_110 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_111 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_112 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_113 required by ONNX-TRT
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_114 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0
[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight
[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias
[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight required by ONNX-TRT
[X] Using opA: 0 opB: 1
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Gemm
[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_115 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_116 required by ONNX-TRT
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_117 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add]
[X] Searching for input: /model/encoder/Transpose_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0
[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/Add_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0
[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_1_output_0
[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.weight
[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.bias
[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.bias -> (256)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.weight required by ONNX-TRT
[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_120 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_121 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_122 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_123 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization
[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0
[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_124 for ONNX node: tmp_weight_124
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_125 for ONNX node: tmp_weight_125
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.weight
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_126 for ONNX node: tmp_weight_126
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: tmp_weight_127 for ONNX node: tmp_weight_127
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Transpose
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_128 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_129 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear1/MatMul
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add]
[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.bias
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_130 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_131 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_132 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_133 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Div for ONNX node: /model/encoder/encoder.0/layers.0/activation/Div
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] outputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Div_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] inputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Erf for ONNX node: /model/encoder/encoder.0/layers.0/activation/Erf
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] outputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Erf_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] inputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_134 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_135 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Add for ONNX node: /model/encoder/encoder.0/layers.0/activation/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Add_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_136 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_137 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_138 for ONNX node: tmp_weight_138
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_139 for ONNX node: tmp_weight_139
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.weight
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_140 for ONNX node: tmp_weight_140
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_141 for ONNX node: tmp_weight_141
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Transpose
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_142 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_143 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear2/MatMul
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add]
[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.bias
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear2.bias -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_144 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_145 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add]
[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Add_output_0
[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/Add_2
[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0
[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_2_output_0
[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.weight
[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.bias
[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.bias -> (256)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.weight required by ONNX-TRT
[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_148 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_149 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_150 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_151 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization
[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0
[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Transpose_1 [Transpose]
[X] Parsing node: /model/encoder/Transpose_1 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0
[X] /model/encoder/Transpose_1 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/Transpose_1 for ONNX node: /model/encoder/Transpose_1
[X] Registering tensor: /model/encoder/Transpose_1_output_0 for ONNX tensor: /model/encoder/Transpose_1_output_0
[X] /model/encoder/Transpose_1 [Transpose] outputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Reshape_1 [Reshape]
[X] Parsing node: /model/encoder/Reshape_1 [Reshape]
[X] Searching for input: /model/encoder/Transpose_1_output_0
[X] Searching for input: /model/encoder/Concat_1_output_0
[X] /model/encoder/Reshape_1 [Reshape] inputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], [/model/encoder/Concat_1_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_152 required by ONNX-TRT
[X] Registering layer: /model/encoder/Reshape_1 for ONNX node: /model/encoder/Reshape_1
[X] Registering tensor: /model/encoder/Reshape_1_output_0 for ONNX tensor: /model/encoder/Reshape_1_output_0
[X] /model/encoder/Reshape_1 [Reshape] outputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Reshape_1_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_153 for ONNX node: tmp_weight_153
[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_154 for ONNX node: tmp_weight_154
[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.lateral_convs.0.conv.weight
[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.lateral_convs.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_155 for ONNX node: tmp_weight_155
[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_156 for ONNX node: tmp_weight_156
[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/lateral_convs.0/conv/Conv for ONNX node: /model/encoder/lateral_convs.0/conv/Conv
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0
[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/lateral_convs.0/conv/Conv_output_0
[X] Searching for input: model.encoder.lateral_convs.0.norm.weight
[X] Searching for input: model.encoder.lateral_convs.0.norm.bias
[X] Searching for input: model.encoder.lateral_convs.0.norm.running_mean
[X] Searching for input: model.encoder.lateral_convs.0.norm.running_var
[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.lateral_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/lateral_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.0/norm/BatchNormalization
[X] Registering tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0
[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0
[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/lateral_convs.0/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.0/act/Sigmoid
[X] Registering tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0
[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/act/Sigmoid_output_0
[X] /model/encoder/lateral_convs.0/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/lateral_convs.0/act/Mul for ONNX node: /model/encoder/lateral_convs.0/act/Mul
[X] Registering tensor: /model/encoder/lateral_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Mul_output_0
[X] /model/encoder/lateral_convs.0/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Resize [Resize]
[X] Parsing node: /model/encoder/Resize [Resize]
[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0
[X] Searching for input: /model/encoder/Constant_9_output_0
[X] /model/encoder/Resize [Resize] inputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], 
[X] Registering layer: /model/encoder/Resize for ONNX node: /model/encoder/Resize
[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest
[X] Registering tensor: /model/encoder/Resize_output_0 for ONNX tensor: /model/encoder/Resize_output_0
[X] /model/encoder/Resize [Resize] outputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Concat_2 [Concat]
[X] Parsing node: /model/encoder/Concat_2 [Concat]
[X] Searching for input: /model/encoder/Resize_output_0
[X] Searching for input: /model/encoder/input_proj.1/norm/BatchNormalization_output_0
[X] /model/encoder/Concat_2 [Concat] inputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/Concat_2 for ONNX node: /model/encoder/Concat_2
[X] Registering tensor: /model/encoder/Concat_2_output_0 for ONNX tensor: /model/encoder/Concat_2_output_0
[X] /model/encoder/Concat_2 [Concat] outputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Concat_2_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_157 for ONNX node: tmp_weight_157
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_158 for ONNX node: tmp_weight_158
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.conv1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_159 for ONNX node: tmp_weight_159
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_160 for ONNX node: tmp_weight_160
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_var
[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_161 for ONNX node: tmp_weight_161
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_162 for ONNX node: tmp_weight_162
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_163 for ONNX node: tmp_weight_163
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_164 for ONNX node: tmp_weight_164
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_165 for ONNX node: tmp_weight_165
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_166 for ONNX node: tmp_weight_166
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_167 for ONNX node: tmp_weight_167
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_168 for ONNX node: tmp_weight_168
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_169 for ONNX node: tmp_weight_169
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_170 for ONNX node: tmp_weight_170
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_171 for ONNX node: tmp_weight_171
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_172 for ONNX node: tmp_weight_172
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.conv2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_173 for ONNX node: tmp_weight_173
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_174 for ONNX node: tmp_weight_174
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_var
[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/Add [Add]
[X] Parsing node: /model/encoder/fpn_blocks.0/Add [Add]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/Add [Add] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/Add for ONNX node: /model/encoder/fpn_blocks.0/Add
[X] Registering tensor: /model/encoder/fpn_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/Add_output_0
[X] /model/encoder/fpn_blocks.0/Add [Add] outputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/Add_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_175 for ONNX node: tmp_weight_175
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_176 for ONNX node: tmp_weight_176
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.conv3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_177 for ONNX node: tmp_weight_177
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_178 for ONNX node: tmp_weight_178
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_var
[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_179 for ONNX node: tmp_weight_179
[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_180 for ONNX node: tmp_weight_180
[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.lateral_convs.1.conv.weight
[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.lateral_convs.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_181 for ONNX node: tmp_weight_181
[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_182 for ONNX node: tmp_weight_182
[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/lateral_convs.1/conv/Conv for ONNX node: /model/encoder/lateral_convs.1/conv/Conv
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0
[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/lateral_convs.1/conv/Conv_output_0
[X] Searching for input: model.encoder.lateral_convs.1.norm.weight
[X] Searching for input: model.encoder.lateral_convs.1.norm.bias
[X] Searching for input: model.encoder.lateral_convs.1.norm.running_mean
[X] Searching for input: model.encoder.lateral_convs.1.norm.running_var
[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.lateral_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/lateral_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0
[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0
[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/lateral_convs.1/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.1/act/Sigmoid
[X] Registering tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0
[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/act/Sigmoid_output_0
[X] /model/encoder/lateral_convs.1/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/lateral_convs.1/act/Mul for ONNX node: /model/encoder/lateral_convs.1/act/Mul
[X] Registering tensor: /model/encoder/lateral_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Mul_output_0
[X] /model/encoder/lateral_convs.1/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Resize_1 [Resize]
[X] Parsing node: /model/encoder/Resize_1 [Resize]
[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0
[X] Searching for input: /model/encoder/Constant_9_output_0
[X] /model/encoder/Resize_1 [Resize] inputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], 
[X] Registering layer: /model/encoder/Resize_1 for ONNX node: /model/encoder/Resize_1
[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest
[X] Registering tensor: /model/encoder/Resize_1_output_0 for ONNX tensor: /model/encoder/Resize_1_output_0
[X] /model/encoder/Resize_1 [Resize] outputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Concat_3 [Concat]
[X] Parsing node: /model/encoder/Concat_3 [Concat]
[X] Searching for input: /model/encoder/Resize_1_output_0
[X] Searching for input: /model/encoder/input_proj.0/norm/BatchNormalization_output_0
[X] /model/encoder/Concat_3 [Concat] inputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/Concat_3 for ONNX node: /model/encoder/Concat_3
[X] Registering tensor: /model/encoder/Concat_3_output_0 for ONNX tensor: /model/encoder/Concat_3_output_0
[X] /model/encoder/Concat_3 [Concat] outputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Concat_3_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_183 for ONNX node: tmp_weight_183
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_184 for ONNX node: tmp_weight_184
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.conv1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_185 for ONNX node: tmp_weight_185
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_186 for ONNX node: tmp_weight_186
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_var
[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_187 for ONNX node: tmp_weight_187
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_188 for ONNX node: tmp_weight_188
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_189 for ONNX node: tmp_weight_189
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_190 for ONNX node: tmp_weight_190
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_191 for ONNX node: tmp_weight_191
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_192 for ONNX node: tmp_weight_192
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_193 for ONNX node: tmp_weight_193
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_194 for ONNX node: tmp_weight_194
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_195 for ONNX node: tmp_weight_195
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_196 for ONNX node: tmp_weight_196
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_197 for ONNX node: tmp_weight_197
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_198 for ONNX node: tmp_weight_198
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.conv2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_199 for ONNX node: tmp_weight_199
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_200 for ONNX node: tmp_weight_200
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_var
[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/Add [Add]
[X] Parsing node: /model/encoder/fpn_blocks.1/Add [Add]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/Add [Add] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/Add for ONNX node: /model/encoder/fpn_blocks.1/Add
[X] Registering tensor: /model/encoder/fpn_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/Add_output_0
[X] /model/encoder/fpn_blocks.1/Add [Add] outputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/Add_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_201 for ONNX node: tmp_weight_201
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_202 for ONNX node: tmp_weight_202
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.conv3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_203 for ONNX node: tmp_weight_203
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_204 for ONNX node: tmp_weight_204
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_var
[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_205 for ONNX node: tmp_weight_205
[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_206 for ONNX node: tmp_weight_206
[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.downsample_convs.0.conv.weight
[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.0.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.downsample_convs.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_207 for ONNX node: tmp_weight_207
[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_208 for ONNX node: tmp_weight_208
[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/downsample_convs.0/conv/Conv for ONNX node: /model/encoder/downsample_convs.0/conv/Conv
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0
[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/Conv_output_0
[X] Searching for input: model.encoder.downsample_convs.0.norm.weight
[X] Searching for input: model.encoder.downsample_convs.0.norm.bias
[X] Searching for input: model.encoder.downsample_convs.0.norm.running_mean
[X] Searching for input: model.encoder.downsample_convs.0.norm.running_var
[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.downsample_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/downsample_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.0/norm/BatchNormalization
[X] Registering tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0
[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0
[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/downsample_convs.0/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.0/act/Sigmoid
[X] Registering tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0
[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/act/Sigmoid_output_0
[X] /model/encoder/downsample_convs.0/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/downsample_convs.0/act/Mul for ONNX node: /model/encoder/downsample_convs.0/act/Mul
[X] Registering tensor: /model/encoder/downsample_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Mul_output_0
[X] /model/encoder/downsample_convs.0/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Concat_4 [Concat]
[X] Parsing node: /model/encoder/Concat_4 [Concat]
[X] Searching for input: /model/encoder/downsample_convs.0/act/Mul_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0
[X] /model/encoder/Concat_4 [Concat] inputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/Concat_4 for ONNX node: /model/encoder/Concat_4
[X] Registering tensor: /model/encoder/Concat_4_output_0 for ONNX tensor: /model/encoder/Concat_4_output_0
[X] /model/encoder/Concat_4 [Concat] outputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Concat_4_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_209 for ONNX node: tmp_weight_209
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_210 for ONNX node: tmp_weight_210
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.conv1.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.conv1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_211 for ONNX node: tmp_weight_211
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_212 for ONNX node: tmp_weight_212
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.weight
[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.bias
[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_var
[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_213 for ONNX node: tmp_weight_213
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_214 for ONNX node: tmp_weight_214
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_215 for ONNX node: tmp_weight_215
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_216 for ONNX node: tmp_weight_216
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_217 for ONNX node: tmp_weight_217
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_218 for ONNX node: tmp_weight_218
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_219 for ONNX node: tmp_weight_219
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_220 for ONNX node: tmp_weight_220
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_221 for ONNX node: tmp_weight_221
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_222 for ONNX node: tmp_weight_222
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_223 for ONNX node: tmp_weight_223
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_224 for ONNX node: tmp_weight_224
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.conv2.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.conv2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_225 for ONNX node: tmp_weight_225
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_226 for ONNX node: tmp_weight_226
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.weight
[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.bias
[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_var
[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/Add [Add]
[X] Parsing node: /model/encoder/pan_blocks.0/Add [Add]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/Add [Add] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/Add for ONNX node: /model/encoder/pan_blocks.0/Add
[X] Registering tensor: /model/encoder/pan_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/Add_output_0
[X] /model/encoder/pan_blocks.0/Add [Add] outputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/Add_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_227 for ONNX node: tmp_weight_227
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_228 for ONNX node: tmp_weight_228
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.conv3.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.conv3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_229 for ONNX node: tmp_weight_229
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_230 for ONNX node: tmp_weight_230
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.weight
[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.bias
[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_var
[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_231 for ONNX node: tmp_weight_231
[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_232 for ONNX node: tmp_weight_232
[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.downsample_convs.1.conv.weight
[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.1.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.downsample_convs.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_233 for ONNX node: tmp_weight_233
[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_234 for ONNX node: tmp_weight_234
[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/downsample_convs.1/conv/Conv for ONNX node: /model/encoder/downsample_convs.1/conv/Conv
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0
[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/Conv_output_0
[X] Searching for input: model.encoder.downsample_convs.1.norm.weight
[X] Searching for input: model.encoder.downsample_convs.1.norm.bias
[X] Searching for input: model.encoder.downsample_convs.1.norm.running_mean
[X] Searching for input: model.encoder.downsample_convs.1.norm.running_var
[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.downsample_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/downsample_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0
[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0
[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/downsample_convs.1/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.1/act/Sigmoid
[X] Registering tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0
[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/act/Sigmoid_output_0
[X] /model/encoder/downsample_convs.1/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/downsample_convs.1/act/Mul for ONNX node: /model/encoder/downsample_convs.1/act/Mul
[X] Registering tensor: /model/encoder/downsample_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Mul_output_0
[X] /model/encoder/downsample_convs.1/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Concat_5 [Concat]
[X] Parsing node: /model/encoder/Concat_5 [Concat]
[X] Searching for input: /model/encoder/downsample_convs.1/act/Mul_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0
[X] /model/encoder/Concat_5 [Concat] inputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/Concat_5 for ONNX node: /model/encoder/Concat_5
[X] Registering tensor: /model/encoder/Concat_5_output_0 for ONNX tensor: /model/encoder/Concat_5_output_0
[X] /model/encoder/Concat_5 [Concat] outputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Concat_5_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_235 for ONNX node: tmp_weight_235
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_236 for ONNX node: tmp_weight_236
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.conv1.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.conv1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_237 for ONNX node: tmp_weight_237
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_238 for ONNX node: tmp_weight_238
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.weight
[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.bias
[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_var
[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_239 for ONNX node: tmp_weight_239
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_240 for ONNX node: tmp_weight_240
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_241 for ONNX node: tmp_weight_241
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_242 for ONNX node: tmp_weight_242
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_243 for ONNX node: tmp_weight_243
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_244 for ONNX node: tmp_weight_244
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_245 for ONNX node: tmp_weight_245
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_246 for ONNX node: tmp_weight_246
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_247 for ONNX node: tmp_weight_247
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_248 for ONNX node: tmp_weight_248
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_249 for ONNX node: tmp_weight_249
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_250 for ONNX node: tmp_weight_250
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.conv2.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.conv2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_251 for ONNX node: tmp_weight_251
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_252 for ONNX node: tmp_weight_252
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.weight
[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.bias
[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_var
[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/Add [Add]
[X] Parsing node: /model/encoder/pan_blocks.1/Add [Add]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/Add [Add] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/Add for ONNX node: /model/encoder/pan_blocks.1/Add
[X] Registering tensor: /model/encoder/pan_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/Add_output_0
[X] /model/encoder/pan_blocks.1/Add [Add] outputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/Add_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_253 for ONNX node: tmp_weight_253
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_254 for ONNX node: tmp_weight_254
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.conv3.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.conv3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_255 for ONNX node: tmp_weight_255
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_256 for ONNX node: tmp_weight_256
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.weight
[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.bias
[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_var
[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.input_proj.0.conv.weight
[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.input_proj.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_257 for ONNX node: tmp_weight_257
[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_258 for ONNX node: tmp_weight_258
[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.0/conv/Conv [Conv]
[X] Parsing node: /model/decoder/input_proj.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/decoder/input_proj.0/conv/Conv for ONNX node: /model/decoder/input_proj.0/conv/Conv
[X] Registering tensor: /model/decoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/Conv_output_0
[X] /model/decoder/input_proj.0/conv/Conv [Conv] outputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/decoder/input_proj.0/conv/Conv_output_0
[X] Searching for input: model.decoder.input_proj.0.norm.weight
[X] Searching for input: model.decoder.input_proj.0.norm.bias
[X] Searching for input: model.decoder.input_proj.0.norm.running_mean
[X] Searching for input: model.decoder.input_proj.0.norm.running_var
[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.decoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/decoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.0/norm/BatchNormalization
[X] Registering tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0
[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.input_proj.1.conv.weight
[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.input_proj.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_259 for ONNX node: tmp_weight_259
[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_260 for ONNX node: tmp_weight_260
[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.1/conv/Conv [Conv]
[X] Parsing node: /model/decoder/input_proj.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/decoder/input_proj.1/conv/Conv for ONNX node: /model/decoder/input_proj.1/conv/Conv
[X] Registering tensor: /model/decoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/Conv_output_0
[X] /model/decoder/input_proj.1/conv/Conv [Conv] outputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/decoder/input_proj.1/conv/Conv_output_0
[X] Searching for input: model.decoder.input_proj.1.norm.weight
[X] Searching for input: model.decoder.input_proj.1.norm.bias
[X] Searching for input: model.decoder.input_proj.1.norm.running_mean
[X] Searching for input: model.decoder.input_proj.1.norm.running_var
[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.decoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/decoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.1/norm/BatchNormalization
[X] Registering tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0
[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0
[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_261 for ONNX node: tmp_weight_261
[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_262 for ONNX node: tmp_weight_262
[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.input_proj.2.conv.weight
[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.2.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.input_proj.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_263 for ONNX node: tmp_weight_263
[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_264 for ONNX node: tmp_weight_264
[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/Conv [Conv]
[X] Parsing node: /model/decoder/input_proj.2/conv/Conv [Conv]
[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/Conv [Conv] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/decoder/input_proj.2/conv/Conv for ONNX node: /model/decoder/input_proj.2/conv/Conv
[X] Registering tensor: /model/decoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/Conv_output_0
[X] /model/decoder/input_proj.2/conv/Conv [Conv] outputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/decoder/input_proj.2/conv/Conv_output_0
[X] Searching for input: model.decoder.input_proj.2.norm.weight
[X] Searching for input: model.decoder.input_proj.2.norm.bias
[X] Searching for input: model.decoder.input_proj.2.norm.running_mean
[X] Searching for input: model.decoder.input_proj.2.norm.running_var
[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.decoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/decoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.2/norm/BatchNormalization
[X] Registering tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0
[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Reshape [Reshape]
[X] Parsing node: /model/decoder/Reshape [Reshape]
[X] Searching for input: /model/decoder/input_proj.0/norm/BatchNormalization_output_0
[X] Searching for input: _v_4326
[X] /model/decoder/Reshape [Reshape] inputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [_v_4326 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_265 required by ONNX-TRT
[X] Registering layer: /model/decoder/Reshape for ONNX node: /model/decoder/Reshape
[X] Registering tensor: /model/decoder/Reshape_output_0 for ONNX tensor: /model/decoder/Reshape_output_0
[X] /model/decoder/Reshape [Reshape] outputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Transpose [Transpose]
[X] Parsing node: /model/decoder/Transpose [Transpose]
[X] Searching for input: /model/decoder/Reshape_output_0
[X] /model/decoder/Transpose [Transpose] inputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], 
[X] Registering layer: /model/decoder/Transpose for ONNX node: /model/decoder/Transpose
[X] Registering tensor: /model/decoder/Transpose_output_0 for ONNX tensor: /model/decoder/Transpose_output_0
[X] /model/decoder/Transpose [Transpose] outputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/input_proj.1/norm/BatchNormalization_output_0
[X] Searching for input: _v_4326
[X] /model/decoder/Reshape_1 [Reshape] inputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [_v_4326 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_266 required by ONNX-TRT
[X] Registering layer: /model/decoder/Reshape_1 for ONNX node: /model/decoder/Reshape_1
[X] Registering tensor: /model/decoder/Reshape_1_output_0 for ONNX tensor: /model/decoder/Reshape_1_output_0
[X] /model/decoder/Reshape_1 [Reshape] outputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/Reshape_1_output_0
[X] /model/decoder/Transpose_1 [Transpose] inputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], 
[X] Registering layer: /model/decoder/Transpose_1 for ONNX node: /model/decoder/Transpose_1
[X] Registering tensor: /model/decoder/Transpose_1_output_0 for ONNX tensor: /model/decoder/Transpose_1_output_0
[X] /model/decoder/Transpose_1 [Transpose] outputs: [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/input_proj.2/norm/BatchNormalization_output_0
[X] Searching for input: _v_4326
[X] /model/decoder/Reshape_2 [Reshape] inputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_267 required by ONNX-TRT
[X] Registering layer: /model/decoder/Reshape_2 for ONNX node: /model/decoder/Reshape_2
[X] Registering tensor: /model/decoder/Reshape_2_output_0 for ONNX tensor: /model/decoder/Reshape_2_output_0
[X] /model/decoder/Reshape_2 [Reshape] outputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/Reshape_2_output_0
[X] /model/decoder/Transpose_2 [Transpose] inputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Registering layer: /model/decoder/Transpose_2 for ONNX node: /model/decoder/Transpose_2
[X] Registering tensor: /model/decoder/Transpose_2_output_0 for ONNX tensor: /model/decoder/Transpose_2_output_0
[X] /model/decoder/Transpose_2 [Transpose] outputs: [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Concat_3 [Concat]
[X] Parsing node: /model/decoder/Concat_3 [Concat]
[X] Searching for input: /model/decoder/Transpose_output_0
[X] Searching for input: /model/decoder/Transpose_1_output_0
[X] Searching for input: /model/decoder/Transpose_2_output_0
[X] /model/decoder/Concat_3 [Concat] inputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/Concat_3 for ONNX node: /model/decoder/Concat_3
[X] Registering tensor: /model/decoder/Concat_3_output_0 for ONNX tensor: /model/decoder/Concat_3_output_0
[X] /model/decoder/Concat_3 [Concat] outputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Mul [Mul]
[X] Parsing node: /model/decoder/Mul [Mul]
[X] Searching for input: onnx::Mul_3692
[X] Searching for input: /model/decoder/Concat_3_output_0
[X] /model/decoder/Mul [Mul] inputs: [onnx::Mul_3692 -> (1, 8400, 1)[FLOAT]], [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: onnx::Mul_3692 required by ONNX-TRT
[X] Registering layer: /model/decoder/Mul for ONNX node: /model/decoder/Mul
[X] Registering tensor: /model/decoder/Mul_output_0 for ONNX tensor: /model/decoder/Mul_output_0
[X] /model/decoder/Mul [Mul] outputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/Mul_output_0
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_268 for ONNX node: tmp_weight_268
[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_269 for ONNX node: tmp_weight_269
[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_output.proj.weight
[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_output.proj.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.enc_output.proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_270 for ONNX node: tmp_weight_270
[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_271 for ONNX node: tmp_weight_271
[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_output/proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_output/proj/Transpose [Transpose] inputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_output/proj/Transpose for ONNX node: /model/decoder/enc_output/proj/Transpose
[X] Registering tensor: /model/decoder/enc_output/proj/Transpose_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Transpose_output_0
[X] /model/decoder/enc_output/proj/Transpose [Transpose] outputs: [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_output/proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_output/proj/Transpose_output_0
[X] /model/decoder/enc_output/proj/MatMul [MatMul] inputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_272 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_273 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_output/proj/MatMul for ONNX node: /model/decoder/enc_output/proj/MatMul
[X] Registering tensor: /model/decoder/enc_output/proj/MatMul_output_0 for ONNX tensor: /model/decoder/enc_output/proj/MatMul_output_0
[X] /model/decoder/enc_output/proj/MatMul [MatMul] outputs: [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/Add [Add]
[X] Parsing node: /model/decoder/enc_output/proj/Add [Add]
[X] Searching for input: model.decoder.enc_output.proj.bias
[X] Searching for input: /model/decoder/enc_output/proj/MatMul_output_0
[X] /model/decoder/enc_output/proj/Add [Add] inputs: [model.decoder.enc_output.proj.bias -> (256)[FLOAT]], [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.enc_output.proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_274 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_275 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_output/proj/Add for ONNX node: /model/decoder/enc_output/proj/Add
[X] Registering tensor: /model/decoder/enc_output/proj/Add_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Add_output_0
[X] /model/decoder/enc_output/proj/Add [Add] outputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/enc_output/proj/Add_output_0
[X] Searching for input: model.decoder.enc_output.norm.weight
[X] Searching for input: model.decoder.enc_output.norm.bias
[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] inputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [model.decoder.enc_output.norm.weight -> (256)[FLOAT]], [model.decoder.enc_output.norm.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.enc_output.norm.weight required by ONNX-TRT
[X] Registering layer: model.decoder.enc_output.norm.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_278 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_279 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_280 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_281 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_output/norm/LayerNormalization for ONNX node: /model/decoder/enc_output/norm/LayerNormalization
[X] Registering tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0 for ONNX tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0
[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] outputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_282 for ONNX node: tmp_weight_282
[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_283 for ONNX node: tmp_weight_283
[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_score_head.weight
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_score_head.weight -> (80, 256)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], 
[X] Registering layer: model.decoder.enc_score_head.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_284 for ONNX node: tmp_weight_284
[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], 
[X] Registering layer: tmp_weight_285 for ONNX node: tmp_weight_285
[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_score_head/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_score_head/Transpose [Transpose] inputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_score_head/Transpose for ONNX node: /model/decoder/enc_score_head/Transpose
[X] Registering tensor: /model/decoder/enc_score_head/Transpose_output_0 for ONNX tensor: /model/decoder/enc_score_head/Transpose_output_0
[X] /model/decoder/enc_score_head/Transpose [Transpose] outputs: [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_score_head/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_score_head/Transpose_output_0
[X] /model/decoder/enc_score_head/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_286 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_287 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_score_head/MatMul for ONNX node: /model/decoder/enc_score_head/MatMul
[X] Registering tensor: /model/decoder/enc_score_head/MatMul_output_0 for ONNX tensor: /model/decoder/enc_score_head/MatMul_output_0
[X] /model/decoder/enc_score_head/MatMul [MatMul] outputs: [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/Add [Add]
[X] Parsing node: /model/decoder/enc_score_head/Add [Add]
[X] Searching for input: model.decoder.enc_score_head.bias
[X] Searching for input: /model/decoder/enc_score_head/MatMul_output_0
[X] /model/decoder/enc_score_head/Add [Add] inputs: [model.decoder.enc_score_head.bias -> (80)[FLOAT]], [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], 
[X] Registering layer: model.decoder.enc_score_head.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_288 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_289 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_score_head/Add for ONNX node: /model/decoder/enc_score_head/Add
[X] Registering tensor: /model/decoder/enc_score_head/Add_output_0 for ONNX tensor: /model/decoder/enc_score_head/Add_output_0
[X] /model/decoder/enc_score_head/Add [Add] outputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_bbox_head.layers.0.weight
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_290 for ONNX node: tmp_weight_290
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_291 for ONNX node: tmp_weight_291
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.0/Transpose
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_292 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_293 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.0/MatMul
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add]
[X] Searching for input: model.decoder.enc_bbox_head.layers.0.bias
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.0.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_294 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_295 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Add for ONNX node: /model/decoder/enc_bbox_head/layers.0/Add
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0
[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu]
[X] Parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Add_output_0
[X] /model/decoder/enc_bbox_head/act/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/act/Relu for ONNX node: /model/decoder/enc_bbox_head/act/Relu
[X] Registering tensor: /model/decoder/enc_bbox_head/act/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act/Relu_output_0
[X] /model/decoder/enc_bbox_head/act/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/act/Relu_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_296 for ONNX node: tmp_weight_296
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_297 for ONNX node: tmp_weight_297
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_bbox_head.layers.1.weight
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_298 for ONNX node: tmp_weight_298
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_299 for ONNX node: tmp_weight_299
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.1/Transpose
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_300 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_301 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.1/MatMul
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add]
[X] Searching for input: model.decoder.enc_bbox_head.layers.1.bias
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_302 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_303 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Add for ONNX node: /model/decoder/enc_bbox_head/layers.1/Add
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0
[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Add_output_0
[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/act_1/Relu for ONNX node: /model/decoder/enc_bbox_head/act_1/Relu
[X] Registering tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0
[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/act_1/Relu_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_304 for ONNX node: tmp_weight_304
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_305 for ONNX node: tmp_weight_305
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_bbox_head.layers.2.weight
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_306 for ONNX node: tmp_weight_306
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: tmp_weight_307 for ONNX node: tmp_weight_307
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.2/Transpose
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_308 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_309 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.2/MatMul
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add]
[X] Searching for input: model.decoder.enc_bbox_head.layers.2.bias
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.2.bias -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_310 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_311 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Add for ONNX node: /model/decoder/enc_bbox_head/layers.2/Add
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0
[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Add [Add]
[X] Parsing node: /model/decoder/Add [Add]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Add_output_0
[X] Searching for input: model.decoder.anchors
[X] /model/decoder/Add [Add] inputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], [model.decoder.anchors -> (1, 8400, 4)[FLOAT]], 
[X] Registering layer: model.decoder.anchors required by ONNX-TRT
[X] Registering layer: /model/decoder/Add for ONNX node: /model/decoder/Add
[X] Registering tensor: /model/decoder/Add_output_0 for ONNX tensor: /model/decoder/Add_output_0
[X] /model/decoder/Add [Add] outputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/ReduceMax [ReduceMax]
[X] Parsing node: /model/decoder/ReduceMax [ReduceMax]
[X] Searching for input: /model/decoder/enc_score_head/Add_output_0
[X] /model/decoder/ReduceMax [ReduceMax] inputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], 
[X] Registering layer: /model/decoder/ReduceMax for ONNX node: /model/decoder/ReduceMax
[X] Registering tensor: /model/decoder/ReduceMax_output_0 for ONNX tensor: /model/decoder/ReduceMax_output_0
[X] /model/decoder/ReduceMax [ReduceMax] outputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/TopK [TopK]
[X] Parsing node: /model/decoder/TopK [TopK]
[X] Searching for input: /model/decoder/ReduceMax_output_0
[X] Searching for input: /model/decoder/Constant_18_output_0
[X] /model/decoder/TopK [TopK] inputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/Constant_18_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_convertToScalar required by ONNX-TRT
[X] Registering layer: /model/decoder/TopK for ONNX node: /model/decoder/TopK
[X] Registering layer: ONNXTRT_castHelper required by ONNX-TRT
[X] Registering tensor: /model/decoder/TopK_output_0 for ONNX tensor: /model/decoder/TopK_output_0
[X] Registering tensor: /model/decoder/TopK_output_1 for ONNX tensor: /model/decoder/TopK_output_1
[X] /model/decoder/TopK [TopK] outputs: [/model/decoder/TopK_output_0 -> (1, 300)[FLOAT]], [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], 
[X] Static check for parsing node: /model/decoder/Unsqueeze [Unsqueeze]
[X] Parsing node: /model/decoder/Unsqueeze [Unsqueeze]
[X] Searching for input: /model/decoder/TopK_output_1
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/Unsqueeze [Unsqueeze] inputs: [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/encoder/Constant_7_output_0 required by ONNX-TRT
[X] Registering layer: /model/decoder/Unsqueeze for ONNX node: /model/decoder/Unsqueeze
[X] Registering tensor: /model/decoder/Unsqueeze_output_0 for ONNX tensor: /model/decoder/Unsqueeze_output_0
[X] /model/decoder/Unsqueeze [Unsqueeze] outputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], 
[X] Static check for parsing node: /model/decoder/Tile [Tile]
[X] Parsing node: /model/decoder/Tile [Tile]
[X] Searching for input: /model/decoder/Unsqueeze_output_0
[X] Searching for input: /model/decoder/Concat_5_output_0
[X] /model/decoder/Tile [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice required by ONNX-TRT
[X] Registering layer: /model/decoder/Tile for ONNX node: /model/decoder/Tile
[X] Registering tensor: /model/decoder/Tile_output_0 for ONNX tensor: /model/decoder/Tile_output_0
[X] /model/decoder/Tile [Tile] outputs: [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], 
[X] Static check for parsing node: /model/decoder/GatherElements [GatherElements]
[X] Parsing node: /model/decoder/GatherElements [GatherElements]
[X] Searching for input: /model/decoder/Add_output_0
[X] Searching for input: /model/decoder/Tile_output_0
[X] /model/decoder/GatherElements [GatherElements] inputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], 
[X] Using Gather axis: 1
[X] Registering layer: ONNXTRT_castHelper_312 required by ONNX-TRT
[X] Registering layer: /model/decoder/GatherElements for ONNX node: /model/decoder/GatherElements
[X] Registering tensor: /model/decoder/GatherElements_output_0 for ONNX tensor: /model/decoder/GatherElements_output_0
[X] /model/decoder/GatherElements [GatherElements] outputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Tile_1 [Tile]
[X] Parsing node: /model/decoder/Tile_1 [Tile]
[X] Searching for input: /model/decoder/Unsqueeze_output_0
[X] Searching for input: /model/decoder/Concat_7_output_0
[X] /model/decoder/Tile_1 [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_7_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_313 required by ONNX-TRT
[X] Registering layer: /model/decoder/Tile_1 for ONNX node: /model/decoder/Tile_1
[X] Registering tensor: /model/decoder/Tile_1_output_0 for ONNX tensor: /model/decoder/Tile_1_output_0
[X] /model/decoder/Tile_1 [Tile] outputs: [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], 
[X] Static check for parsing node: /model/decoder/GatherElements_1 [GatherElements]
[X] Parsing node: /model/decoder/GatherElements_1 [GatherElements]
[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0
[X] Searching for input: /model/decoder/Tile_1_output_0
[X] /model/decoder/GatherElements_1 [GatherElements] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], 
[X] Using Gather axis: 1
[X] Registering layer: ONNXTRT_castHelper_314 required by ONNX-TRT
[X] Registering layer: /model/decoder/GatherElements_1 for ONNX node: /model/decoder/GatherElements_1
[X] Registering tensor: /model/decoder/GatherElements_1_output_0 for ONNX tensor: /model/decoder/GatherElements_1_output_0
[X] /model/decoder/GatherElements_1 [GatherElements] outputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sigmoid [Sigmoid]
[X] Parsing node: /model/decoder/decoder/Sigmoid [Sigmoid]
[X] Searching for input: /model/decoder/GatherElements_output_0
[X] /model/decoder/decoder/Sigmoid [Sigmoid] inputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Sigmoid for ONNX node: /model/decoder/decoder/Sigmoid
[X] Registering tensor: /model/decoder/decoder/Sigmoid_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_output_0
[X] /model/decoder/decoder/Sigmoid [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_315 for ONNX node: tmp_weight_315
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_316 for ONNX node: tmp_weight_316
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.query_pos_head.layers.0.weight
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.0.weight -> (512, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.decoder.query_pos_head.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_317 for ONNX node: tmp_weight_317
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_318 for ONNX node: tmp_weight_318
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Transpose
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_319 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_320 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.0.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: model.decoder.query_pos_head.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_321 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_322 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0
[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/act/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act/Relu
[X] Registering tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0
[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/act/Relu_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_323 for ONNX node: tmp_weight_323
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_324 for ONNX node: tmp_weight_324
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.query_pos_head.layers.1.weight
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.1.weight -> (256, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.query_pos_head.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_325 for ONNX node: tmp_weight_325
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_326 for ONNX node: tmp_weight_326
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Transpose
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_327 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_328 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.1.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.query_pos_head.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_329 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_330 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add [Add]
[X] Searching for input: /model/decoder/GatherElements_1_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0
[X] /model/decoder/decoder/layers.0/Add [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add for ONNX node: /model/decoder/decoder/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_output_0
[X] /model/decoder/decoder/layers.0/Add [Add] outputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/GatherElements_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3736
[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3736 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3736 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_331 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_332 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul
[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add]
[X] Searching for input: onnx::Add_3731
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3731 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3731 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_333 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_334 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3737
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3737 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3737 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_335 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_336 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_1
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add]
[X] Searching for input: onnx::Add_3733
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3733 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3733 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_337 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_338 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0
[X] Searching for input: onnx::MatMul_3738
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3738 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3738 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_339 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_340 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_2
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add]
[X] Searching for input: onnx::Add_3735
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3735 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3735 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_341 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_342 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_343 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_344 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_345 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_346 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_347 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_4
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_3
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/self_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_348 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_4
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_5
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0
[X] Searching for input: _v_1846
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_349 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_3
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0
[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.weight
[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.bias
[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.weight required by ONNX-TRT
[X] Using opA: 0 opB: 1
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.0/self_attn/Gemm
[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_350 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_351 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_352 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_6
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add_1 [Add]
[X] Searching for input: /model/decoder/GatherElements_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.0/Add_1 [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add_1 for ONNX node: /model/decoder/decoder/layers.0/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_1_output_0
[X] /model/decoder/decoder/layers.0/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_1_output_0
[X] Searching for input: model.decoder.decoder.layers.0.norm1.weight
[X] Searching for input: model.decoder.decoder.layers.0.norm1.bias
[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.norm1.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.0.norm1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_355 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_356 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_357 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_358 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm1/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add_2 [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0
[X] /model/decoder/decoder/layers.0/Add_2 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add_2 for ONNX node: /model/decoder/decoder/layers.0/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_2_output_0
[X] /model/decoder/decoder/layers.0/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/Concat_3_output_0
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_359 for ONNX node: tmp_weight_359
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_360 for ONNX node: tmp_weight_360
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_361 for ONNX node: tmp_weight_361
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_362 for ONNX node: tmp_weight_362
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_363 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_364 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_365 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_366 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0
[X] Searching for input: _v_1848
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_367 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_368 for ONNX node: tmp_weight_368
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_369 for ONNX node: tmp_weight_369
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_370 for ONNX node: tmp_weight_370
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: tmp_weight_371 for ONNX node: tmp_weight_371
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_372 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_373 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_374 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_375 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0_376 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[X] Searching for input: _v_1663
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_377 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_378 for ONNX node: tmp_weight_378
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: tmp_weight_379 for ONNX node: tmp_weight_379
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_380 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_381 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_382 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_383 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0_384 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
[X] Searching for input: _v_1665
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_385 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_386 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0
[X] Searching for input: onnx::Mul_3755
[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], 
[X] Registering layer: onnx::Mul_3755 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_387 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_388 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0
[X] Searching for input: _v_1997
[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], 
[X] Registering layer: _v_1997 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/Constant_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_389 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_390 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_393 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_394 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_396 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_397 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_398 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_400 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_401 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_402 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_404 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_405 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_407 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_408 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_409 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_410 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_412 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_413 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_414 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_415 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_417 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_418 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_419 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_420 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_421 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_2
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: onnx::Unsqueeze_1255 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_422 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_423 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_425 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_426 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_428 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_429 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_431 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_432 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_433 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_435 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_436 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_437 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_439 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_440 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_442 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_443 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_444 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_445 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_447 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_448 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_449 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_450 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_452 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_453 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_454 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0
[X] Searching for input: _v_1749
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_455 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_456 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_457 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_459 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_460 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_462 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_463 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_465 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_466 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_467 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_469 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_470 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_471 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_473 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_474 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_476 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_477 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_478 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_479 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_481 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_482 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_483 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_484 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_486 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_487 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_488 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_4
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_489 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_490 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_492 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_493 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_495 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_496 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_498 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_499 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_500 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_502 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_503 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_504 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_506 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_507 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_509 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_510 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_511 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_512 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_514 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_515 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_516 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_517 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_519 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_520 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_521 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_5
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_522 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_523 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_525 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_526 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_528 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_529 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_531 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_532 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_533 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_535 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_536 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_537 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_539 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_540 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_542 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_543 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_544 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_545 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_547 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_548 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_549 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_550 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_552 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_553 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_554 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_6
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_555 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_556 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_6
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_557 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_558 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Sub
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0
[X] Searching for input: _v_1850
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_559 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0
[X] Searching for input: onnx::Split_2305
[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_560 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_561 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_562 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_563 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_564 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_565 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_566 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_1
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_567 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_568 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Concat_10
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_8
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_569 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_570 for ONNX node: tmp_weight_570
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_571 for ONNX node: tmp_weight_571
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_572 for ONNX node: tmp_weight_572
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_573 for ONNX node: tmp_weight_573
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_574 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_575 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_576 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_577 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_3 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add_3 [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.0/Add_3 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add_3 for ONNX node: /model/decoder/decoder/layers.0/Add_3
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_3_output_0
[X] /model/decoder/decoder/layers.0/Add_3 [Add] outputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_3_output_0
[X] Searching for input: model.decoder.decoder.layers.0.norm2.weight
[X] Searching for input: model.decoder.decoder.layers.0.norm2.bias
[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.norm2.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.0.norm2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_580 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_581 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_582 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_583 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm2/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_584 for ONNX node: tmp_weight_584
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_585 for ONNX node: tmp_weight_585
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.linear1.weight
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.linear1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_586 for ONNX node: tmp_weight_586
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: tmp_weight_587 for ONNX node: tmp_weight_587
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear1/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_588 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_589 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear1/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.linear1.bias
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.0/linear1/Add [Add] inputs: [model.decoder.decoder.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.linear1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_590 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_591 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Add for ONNX node: /model/decoder/decoder/layers.0/linear1/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0
[X] /model/decoder/decoder/layers.0/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Add_output_0
[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/activation/Relu for ONNX node: /model/decoder/decoder/layers.0/activation/Relu
[X] Registering tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0
[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/activation/Relu_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_592 for ONNX node: tmp_weight_592
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_593 for ONNX node: tmp_weight_593
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.linear2.weight
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.linear2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_594 for ONNX node: tmp_weight_594
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_595 for ONNX node: tmp_weight_595
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear2/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_596 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_597 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear2/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.linear2.bias
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.0/linear2/Add [Add] inputs: [model.decoder.decoder.layers.0.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.linear2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_598 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_599 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Add for ONNX node: /model/decoder/decoder/layers.0/linear2/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0
[X] /model/decoder/decoder/layers.0/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_4 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add_4 [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Add_output_0
[X] /model/decoder/decoder/layers.0/Add_4 [Add] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add_4 for ONNX node: /model/decoder/decoder/layers.0/Add_4
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_4_output_0
[X] /model/decoder/decoder/layers.0/Add_4 [Add] outputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_4_output_0
[X] Searching for input: model.decoder.decoder.layers.0.norm3.weight
[X] Searching for input: model.decoder.decoder.layers.0.norm3.bias
[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.norm3.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.0.norm3.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_602 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_603 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_604 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_605 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm3/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_606 for ONNX node: tmp_weight_606
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_607 for ONNX node: tmp_weight_607
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_608 for ONNX node: tmp_weight_608
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_609 for ONNX node: tmp_weight_609
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_610 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_611 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_612 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_613 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_614 for ONNX node: tmp_weight_614
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_615 for ONNX node: tmp_weight_615
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_616 for ONNX node: tmp_weight_616
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_617 for ONNX node: tmp_weight_617
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_618 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_619 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_620 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_621 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_622 for ONNX node: tmp_weight_622
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_623 for ONNX node: tmp_weight_623
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_624 for ONNX node: tmp_weight_624
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: tmp_weight_625 for ONNX node: tmp_weight_625
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_626 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_627 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_628 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_629 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip [Clip]
[X] Parsing node: /model/decoder/decoder/Clip [Clip]
[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0
[X] Searching for input: /model/decoder/decoder/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/Clip [Clip] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Clip for ONNX node: /model/decoder/decoder/Clip
[X] Registering tensor: /model/decoder/decoder/Clip_output_0 for ONNX tensor: /model/decoder/decoder/Clip_output_0
[X] /model/decoder/decoder/Clip [Clip] outputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_1 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_1 [Clip]
[X] Searching for input: /model/decoder/decoder/Clip_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_1 [Clip] inputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: /model/decoder/decoder/Constant_3_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_631 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_632 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_633 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_634 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_1_output_0 for ONNX tensor: /model/decoder/decoder/Clip_1_output_0
[X] /model/decoder/decoder/Clip_1 [Clip] outputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sub [Sub]
[X] Parsing node: /model/decoder/decoder/Sub [Sub]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/Clip_output_0
[X] /model/decoder/decoder/Sub [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_635 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_636 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/Sub for ONNX node: /model/decoder/decoder/Sub
[X] Registering tensor: /model/decoder/decoder/Sub_output_0 for ONNX tensor: /model/decoder/decoder/Sub_output_0
[X] /model/decoder/decoder/Sub [Sub] outputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_2 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_2 [Clip]
[X] Searching for input: /model/decoder/decoder/Sub_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_2 [Clip] inputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_638 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_639 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_640 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_641 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_2_output_0 for ONNX tensor: /model/decoder/decoder/Clip_2_output_0
[X] /model/decoder/decoder/Clip_2 [Clip] outputs: [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Div [Div]
[X] Parsing node: /model/decoder/decoder/Div [Div]
[X] Searching for input: /model/decoder/decoder/Clip_1_output_0
[X] Searching for input: /model/decoder/decoder/Clip_2_output_0
[X] /model/decoder/decoder/Div [Div] inputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Div for ONNX node: /model/decoder/decoder/Div
[X] Registering tensor: /model/decoder/decoder/Div_output_0 for ONNX tensor: /model/decoder/decoder/Div_output_0
[X] /model/decoder/decoder/Div [Div] outputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Log [Log]
[X] Parsing node: /model/decoder/decoder/Log [Log]
[X] Searching for input: /model/decoder/decoder/Div_output_0
[X] /model/decoder/decoder/Log [Log] inputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Log for ONNX node: /model/decoder/decoder/Log
[X] Registering tensor: /model/decoder/decoder/Log_output_0 for ONNX tensor: /model/decoder/decoder/Log_output_0
[X] /model/decoder/decoder/Log [Log] outputs: [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Add [Add]
[X] Parsing node: /model/decoder/decoder/Add [Add]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0
[X] Searching for input: /model/decoder/decoder/Log_output_0
[X] /model/decoder/decoder/Add [Add] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Add for ONNX node: /model/decoder/decoder/Add
[X] Registering tensor: /model/decoder/decoder/Add_output_0 for ONNX tensor: /model/decoder/decoder/Add_output_0
[X] /model/decoder/decoder/Add [Add] outputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid]
[X] Parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid]
[X] Searching for input: /model/decoder/decoder/Add_output_0
[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] inputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Sigmoid_1 for ONNX node: /model/decoder/decoder/Sigmoid_1
[X] Registering tensor: /model/decoder/decoder/Sigmoid_1_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_1_output_0
[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_642 for ONNX node: tmp_weight_642
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_643 for ONNX node: tmp_weight_643
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_644 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_645 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.0.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_646 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_647 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0
[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/act_1/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_1/Relu
[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0
[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_648 for ONNX node: tmp_weight_648
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_649 for ONNX node: tmp_weight_649
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_650 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_651 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.1.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_652 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_653 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0
[X] /model/decoder/decoder/layers.1/Add [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add for ONNX node: /model/decoder/decoder/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_output_0
[X] /model/decoder/decoder/layers.1/Add [Add] outputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3808
[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3808 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3808 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_654 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_655 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul
[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add]
[X] Searching for input: onnx::Add_3803
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] inputs: [onnx::Add_3803 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3803 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_656 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_657 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3809
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3809 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3809 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_658 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_659 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_1
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add]
[X] Searching for input: onnx::Add_3805
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] inputs: [onnx::Add_3805 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3805 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_660 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_661 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0
[X] Searching for input: onnx::MatMul_3810
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3810 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3810 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_662 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_663 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_2
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add]
[X] Searching for input: onnx::Add_3807
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] inputs: [onnx::Add_3807 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3807 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_664 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_665 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_output_0
[X] Searching for input: _v_1669
[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_666 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0
[X] Searching for input: _v_1669
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_667 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0
[X] Searching for input: _v_1669
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_668 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_669 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_670 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_4
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_3
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/self_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_671 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_4
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_5
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0
[X] Searching for input: _v_1846
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_672 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_3
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0
[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.weight
[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.bias
[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.weight required by ONNX-TRT
[X] Using opA: 0 opB: 1
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.1/self_attn/Gemm
[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_673 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_674 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0
[X] Searching for input: _v_1675
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1675 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_675 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_6
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add_1 [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.1/Add_1 [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add_1 for ONNX node: /model/decoder/decoder/layers.1/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_1_output_0
[X] /model/decoder/decoder/layers.1/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_1_output_0
[X] Searching for input: model.decoder.decoder.layers.1.norm1.weight
[X] Searching for input: model.decoder.decoder.layers.1.norm1.bias
[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.norm1.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.1.norm1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_678 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_679 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_680 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_681 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm1/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add_2 [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0
[X] /model/decoder/decoder/layers.1/Add_2 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add_2 for ONNX node: /model/decoder/decoder/layers.1/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_2_output_0
[X] /model/decoder/decoder/layers.1/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_682 for ONNX node: tmp_weight_682
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_683 for ONNX node: tmp_weight_683
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_684 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_685 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_686 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_687 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0
[X] Searching for input: _v_1848
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_688 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_689 for ONNX node: tmp_weight_689
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_690 for ONNX node: tmp_weight_690
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_691 for ONNX node: tmp_weight_691
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: tmp_weight_692 for ONNX node: tmp_weight_692
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_693 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_694 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_695 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_696 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0_697 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[X] Searching for input: _v_1663
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_698 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_699 for ONNX node: tmp_weight_699
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: tmp_weight_700 for ONNX node: tmp_weight_700
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_701 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_702 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_703 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_704 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0_705 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
[X] Searching for input: _v_1665
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_706 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_707 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0
[X] Searching for input: onnx::Mul_3755
[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_708 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_709 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0
[X] Searching for input: _v_1997
[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_710 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_711 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_713 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_714 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_716 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_717 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_719 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_720 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_721 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_723 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_724 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_725 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_727 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_728 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_730 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_731 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_732 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_733 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_735 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_736 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_737 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_738 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_740 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_741 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_742 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_743 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_744 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_2
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_745 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_746 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_748 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_749 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_751 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_752 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_754 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_755 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_756 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_758 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_759 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_760 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_762 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_763 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_765 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_766 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_767 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_768 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_770 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_771 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_772 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_773 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_775 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_776 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_777 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0
[X] Searching for input: _v_1749
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_778 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_779 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_780 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_782 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_783 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_785 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_786 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_788 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_789 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_790 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_792 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_793 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_794 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_796 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_797 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_799 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_800 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_801 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_802 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_804 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_805 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_806 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_807 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_809 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_810 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_811 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_4
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_812 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_813 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_815 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_816 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_818 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_819 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_821 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_822 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_823 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_825 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_826 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_827 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_829 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_830 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_832 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_833 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_834 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_835 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_837 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_838 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_839 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_840 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_842 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_843 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_844 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_5
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_845 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_846 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_848 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_849 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_851 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_852 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_854 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_855 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_856 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_858 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_859 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_860 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_862 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_863 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_865 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_866 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_867 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_868 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_870 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_871 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_872 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_873 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_875 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_876 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_877 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_6
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Add_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_878 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_879 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_3
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_880 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_881 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Sub
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0
[X] Searching for input: _v_1850
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_882 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0
[X] Searching for input: onnx::Split_2305
[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_883 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_884 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_885 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_886 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_887 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_888 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_889 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_1
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_890 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_891 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Concat_10
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_5
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_892 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_893 for ONNX node: tmp_weight_893
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_894 for ONNX node: tmp_weight_894
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_895 for ONNX node: tmp_weight_895
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_896 for ONNX node: tmp_weight_896
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_897 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_898 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_899 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_900 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_3 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add_3 [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.1/Add_3 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add_3 for ONNX node: /model/decoder/decoder/layers.1/Add_3
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_3_output_0
[X] /model/decoder/decoder/layers.1/Add_3 [Add] outputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_3_output_0
[X] Searching for input: model.decoder.decoder.layers.1.norm2.weight
[X] Searching for input: model.decoder.decoder.layers.1.norm2.bias
[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.norm2.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.1.norm2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_903 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_904 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_905 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_906 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm2/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_907 for ONNX node: tmp_weight_907
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_908 for ONNX node: tmp_weight_908
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.linear1.weight
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.linear1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_909 for ONNX node: tmp_weight_909
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: tmp_weight_910 for ONNX node: tmp_weight_910
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear1/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_911 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_912 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear1/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.linear1.bias
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.1/linear1/Add [Add] inputs: [model.decoder.decoder.layers.1.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.linear1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_913 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_914 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Add for ONNX node: /model/decoder/decoder/layers.1/linear1/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0
[X] /model/decoder/decoder/layers.1/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Add_output_0
[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/activation/Relu for ONNX node: /model/decoder/decoder/layers.1/activation/Relu
[X] Registering tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0
[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/activation/Relu_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_915 for ONNX node: tmp_weight_915
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_916 for ONNX node: tmp_weight_916
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.linear2.weight
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.linear2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_917 for ONNX node: tmp_weight_917
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_918 for ONNX node: tmp_weight_918
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear2/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_919 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_920 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear2/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.linear2.bias
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.1/linear2/Add [Add] inputs: [model.decoder.decoder.layers.1.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.linear2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_921 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_922 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Add for ONNX node: /model/decoder/decoder/layers.1/linear2/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0
[X] /model/decoder/decoder/layers.1/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_4 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add_4 [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Add_output_0
[X] /model/decoder/decoder/layers.1/Add_4 [Add] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add_4 for ONNX node: /model/decoder/decoder/layers.1/Add_4
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_4_output_0
[X] /model/decoder/decoder/layers.1/Add_4 [Add] outputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_4_output_0
[X] Searching for input: model.decoder.decoder.layers.1.norm3.weight
[X] Searching for input: model.decoder.decoder.layers.1.norm3.bias
[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.norm3.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.1.norm3.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_925 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_926 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_927 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_928 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm3/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_929 for ONNX node: tmp_weight_929
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_930 for ONNX node: tmp_weight_930
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_931 for ONNX node: tmp_weight_931
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_932 for ONNX node: tmp_weight_932
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_933 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_934 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_935 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_936 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_937 for ONNX node: tmp_weight_937
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_938 for ONNX node: tmp_weight_938
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_939 for ONNX node: tmp_weight_939
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_940 for ONNX node: tmp_weight_940
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_941 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_942 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_943 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_944 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_945 for ONNX node: tmp_weight_945
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_946 for ONNX node: tmp_weight_946
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_947 for ONNX node: tmp_weight_947
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: tmp_weight_948 for ONNX node: tmp_weight_948
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_949 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_950 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_951 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_952 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_3 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_3 [Clip]
[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0
[X] Searching for input: /model/decoder/decoder/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/Clip_3 [Clip] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Clip_3 for ONNX node: /model/decoder/decoder/Clip_3
[X] Registering tensor: /model/decoder/decoder/Clip_3_output_0 for ONNX tensor: /model/decoder/decoder/Clip_3_output_0
[X] /model/decoder/decoder/Clip_3 [Clip] outputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_4 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_4 [Clip]
[X] Searching for input: /model/decoder/decoder/Clip_3_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_4 [Clip] inputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_954 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_955 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_956 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_957 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_4_output_0 for ONNX tensor: /model/decoder/decoder/Clip_4_output_0
[X] /model/decoder/decoder/Clip_4 [Clip] outputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sub_1 [Sub]
[X] Parsing node: /model/decoder/decoder/Sub_1 [Sub]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/Clip_3_output_0
[X] /model/decoder/decoder/Sub_1 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_958 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_959 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/Sub_1 for ONNX node: /model/decoder/decoder/Sub_1
[X] Registering tensor: /model/decoder/decoder/Sub_1_output_0 for ONNX tensor: /model/decoder/decoder/Sub_1_output_0
[X] /model/decoder/decoder/Sub_1 [Sub] outputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_5 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_5 [Clip]
[X] Searching for input: /model/decoder/decoder/Sub_1_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_5 [Clip] inputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_961 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_962 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_963 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_964 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_5_output_0 for ONNX tensor: /model/decoder/decoder/Clip_5_output_0
[X] /model/decoder/decoder/Clip_5 [Clip] outputs: [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Div_1 [Div]
[X] Parsing node: /model/decoder/decoder/Div_1 [Div]
[X] Searching for input: /model/decoder/decoder/Clip_4_output_0
[X] Searching for input: /model/decoder/decoder/Clip_5_output_0
[X] /model/decoder/decoder/Div_1 [Div] inputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Div_1 for ONNX node: /model/decoder/decoder/Div_1
[X] Registering tensor: /model/decoder/decoder/Div_1_output_0 for ONNX tensor: /model/decoder/decoder/Div_1_output_0
[X] /model/decoder/decoder/Div_1 [Div] outputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Log_1 [Log]
[X] Parsing node: /model/decoder/decoder/Log_1 [Log]
[X] Searching for input: /model/decoder/decoder/Div_1_output_0
[X] /model/decoder/decoder/Log_1 [Log] inputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Log_1 for ONNX node: /model/decoder/decoder/Log_1
[X] Registering tensor: /model/decoder/decoder/Log_1_output_0 for ONNX tensor: /model/decoder/decoder/Log_1_output_0
[X] /model/decoder/decoder/Log_1 [Log] outputs: [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/Add_1 [Add]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0
[X] Searching for input: /model/decoder/decoder/Log_1_output_0
[X] /model/decoder/decoder/Add_1 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Add_1 for ONNX node: /model/decoder/decoder/Add_1
[X] Registering tensor: /model/decoder/decoder/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/Add_1_output_0
[X] /model/decoder/decoder/Add_1 [Add] outputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid]
[X] Parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid]
[X] Searching for input: /model/decoder/decoder/Add_1_output_0
[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] inputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Sigmoid_2 for ONNX node: /model/decoder/decoder/Sigmoid_2
[X] Registering tensor: /model/decoder/decoder/Sigmoid_2_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_2_output_0
[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_965 for ONNX node: tmp_weight_965
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_966 for ONNX node: tmp_weight_966
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_967 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_968 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.0.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_969 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_970 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0
[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/act_2/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_2/Relu
[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0
[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_971 for ONNX node: tmp_weight_971
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_972 for ONNX node: tmp_weight_972
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_973 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_974 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.1.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_975 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_976 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0
[X] /model/decoder/decoder/layers.2/Add [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add for ONNX node: /model/decoder/decoder/layers.2/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_output_0
[X] /model/decoder/decoder/layers.2/Add [Add] outputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3880
[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3880 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3880 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_977 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_978 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul
[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add]
[X] Searching for input: onnx::Add_3875
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] inputs: [onnx::Add_3875 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3875 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_979 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_980 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3881
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3881 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3881 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_981 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_982 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_1
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add]
[X] Searching for input: onnx::Add_3877
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] inputs: [onnx::Add_3877 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3877 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_983 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_984 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0
[X] Searching for input: onnx::MatMul_3882
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3882 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3882 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_985 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_986 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_2
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add]
[X] Searching for input: onnx::Add_3879
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] inputs: [onnx::Add_3879 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3879 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_987 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_988 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_output_0
[X] Searching for input: _v_1669
[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_989 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0
[X] Searching for input: _v_1669
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_990 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0
[X] Searching for input: _v_1669
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_991 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_992 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_993 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_4
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_3
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/self_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_994 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_4
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_5
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0
[X] Searching for input: _v_1846
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_995 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_3
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0
[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.weight
[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.bias
[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.weight required by ONNX-TRT
[X] Using opA: 0 opB: 1
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.2/self_attn/Gemm
[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_996 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_997 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0
[X] Searching for input: _v_1675
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1675 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_998 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_6
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add_1 [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.2/Add_1 [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add_1 for ONNX node: /model/decoder/decoder/layers.2/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_1_output_0
[X] /model/decoder/decoder/layers.2/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_1_output_0
[X] Searching for input: model.decoder.decoder.layers.2.norm1.weight
[X] Searching for input: model.decoder.decoder.layers.2.norm1.bias
[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.norm1.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.2.norm1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1001 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1002 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1003 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1004 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm1/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add_2 [Add]
[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0
[X] /model/decoder/decoder/layers.2/Add_2 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add_2 for ONNX node: /model/decoder/decoder/layers.2/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_2_output_0
[X] /model/decoder/decoder/layers.2/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1005 for ONNX node: tmp_weight_1005
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1006 for ONNX node: tmp_weight_1006
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1007 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1008 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1009 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1010 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0
[X] Searching for input: _v_1848
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1011 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1012 for ONNX node: tmp_weight_1012
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1013 for ONNX node: tmp_weight_1013
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1014 for ONNX node: tmp_weight_1014
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: tmp_weight_1015 for ONNX node: tmp_weight_1015
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1016 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1017 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1018 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1019 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0_1020 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[X] Searching for input: _v_1663
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1021 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1022 for ONNX node: tmp_weight_1022
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: tmp_weight_1023 for ONNX node: tmp_weight_1023
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1024 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1025 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1026 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1027 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0_1028 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
[X] Searching for input: _v_1665
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1029 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_1030 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0
[X] Searching for input: onnx::Mul_3755
[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1031 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1032 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0
[X] Searching for input: _v_1997
[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1033 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1034 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1036 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1037 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1039 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1040 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1042 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1043 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1044 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1046 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1047 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1048 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1050 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1051 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1053 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1054 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1055 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1056 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1058 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1059 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1060 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1061 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1063 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1064 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1065 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1066 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1067 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_2
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1068 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1069 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1071 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1072 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1074 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1075 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1077 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1078 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1079 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1081 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1082 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1083 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1085 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1086 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1088 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1089 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1090 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1091 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1093 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1094 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1095 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1096 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1098 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1099 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1100 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0
[X] Searching for input: _v_1749
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1101 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1102 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1103 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1105 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1106 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1108 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1109 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1111 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1112 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1113 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1115 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1116 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1117 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1119 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1120 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1122 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1123 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1124 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1125 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1127 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1128 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1129 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1130 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1132 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1133 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1134 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_4
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1135 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1136 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1138 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1139 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1141 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1142 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1144 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1145 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1146 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1148 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1149 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1150 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1152 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1153 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1155 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1156 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1157 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1158 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1160 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1161 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1162 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1163 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1165 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1166 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1167 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_5
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1168 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1169 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1171 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1172 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1174 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1175 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1177 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1178 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1179 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1181 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1182 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1183 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1185 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1186 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1188 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1189 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1190 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1191 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1193 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1194 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1195 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1196 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1198 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1199 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1200 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_6
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Add_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1201 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1202 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_3
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1203 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1204 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Sub
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0
[X] Searching for input: _v_1850
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1205 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0
[X] Searching for input: onnx::Split_2305
[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_1206 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1207 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1208 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1209 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1210 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1211 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1212 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_1
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1213 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1214 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Concat_10
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_5
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1215 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1216 for ONNX node: tmp_weight_1216
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1217 for ONNX node: tmp_weight_1217
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1218 for ONNX node: tmp_weight_1218
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1219 for ONNX node: tmp_weight_1219
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1220 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1221 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1222 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1223 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_3 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add_3 [Add]
[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.2/Add_3 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add_3 for ONNX node: /model/decoder/decoder/layers.2/Add_3
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_3_output_0
[X] /model/decoder/decoder/layers.2/Add_3 [Add] outputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_3_output_0
[X] Searching for input: model.decoder.decoder.layers.2.norm2.weight
[X] Searching for input: model.decoder.decoder.layers.2.norm2.bias
[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.norm2.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.2.norm2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1226 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1227 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1228 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1229 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm2/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1230 for ONNX node: tmp_weight_1230
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1231 for ONNX node: tmp_weight_1231
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.linear1.weight
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.linear1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1232 for ONNX node: tmp_weight_1232
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: tmp_weight_1233 for ONNX node: tmp_weight_1233
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear1/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1234 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1235 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear1/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.linear1.bias
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.2/linear1/Add [Add] inputs: [model.decoder.decoder.layers.2.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.linear1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1236 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1237 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Add for ONNX node: /model/decoder/decoder/layers.2/linear1/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0
[X] /model/decoder/decoder/layers.2/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Add_output_0
[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/activation/Relu for ONNX node: /model/decoder/decoder/layers.2/activation/Relu
[X] Registering tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0
[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/activation/Relu_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1238 for ONNX node: tmp_weight_1238
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1239 for ONNX node: tmp_weight_1239
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.linear2.weight
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.linear2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1240 for ONNX node: tmp_weight_1240
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1241 for ONNX node: tmp_weight_1241
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear2/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1242 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1243 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear2/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.linear2.bias
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.2/linear2/Add [Add] inputs: [model.decoder.decoder.layers.2.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.linear2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1244 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1245 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Add for ONNX node: /model/decoder/decoder/layers.2/linear2/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0
[X] /model/decoder/decoder/layers.2/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_4 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add_4 [Add]
[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Add_output_0
[X] /model/decoder/decoder/layers.2/Add_4 [Add] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add_4 for ONNX node: /model/decoder/decoder/layers.2/Add_4
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_4_output_0
[X] /model/decoder/decoder/layers.2/Add_4 [Add] outputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_4_output_0
[X] Searching for input: model.decoder.decoder.layers.2.norm3.weight
[X] Searching for input: model.decoder.decoder.layers.2.norm3.bias
[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.norm3.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.2.norm3.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1248 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1249 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1250 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1251 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm3/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1252 for ONNX node: tmp_weight_1252
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1253 for ONNX node: tmp_weight_1253
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1254 for ONNX node: tmp_weight_1254
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1255 for ONNX node: tmp_weight_1255
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1256 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1257 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1258 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1259 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1260 for ONNX node: tmp_weight_1260
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1261 for ONNX node: tmp_weight_1261
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1262 for ONNX node: tmp_weight_1262
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1263 for ONNX node: tmp_weight_1263
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1264 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1265 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1266 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1267 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1268 for ONNX node: tmp_weight_1268
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1269 for ONNX node: tmp_weight_1269
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1270 for ONNX node: tmp_weight_1270
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: tmp_weight_1271 for ONNX node: tmp_weight_1271
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1272 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1273 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1274 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1275 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_6 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_6 [Clip]
[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0
[X] Searching for input: /model/decoder/decoder/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/Clip_6 [Clip] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Clip_6 for ONNX node: /model/decoder/decoder/Clip_6
[X] Registering tensor: /model/decoder/decoder/Clip_6_output_0 for ONNX tensor: /model/decoder/decoder/Clip_6_output_0
[X] /model/decoder/decoder/Clip_6 [Clip] outputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_7 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_7 [Clip]
[X] Searching for input: /model/decoder/decoder/Clip_6_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_7 [Clip] inputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1277 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1278 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1279 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1280 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_7_output_0 for ONNX tensor: /model/decoder/decoder/Clip_7_output_0
[X] /model/decoder/decoder/Clip_7 [Clip] outputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sub_2 [Sub]
[X] Parsing node: /model/decoder/decoder/Sub_2 [Sub]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/Clip_6_output_0
[X] /model/decoder/decoder/Sub_2 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1281 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1282 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/Sub_2 for ONNX node: /model/decoder/decoder/Sub_2
[X] Registering tensor: /model/decoder/decoder/Sub_2_output_0 for ONNX tensor: /model/decoder/decoder/Sub_2_output_0
[X] /model/decoder/decoder/Sub_2 [Sub] outputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_8 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_8 [Clip]
[X] Searching for input: /model/decoder/decoder/Sub_2_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_8 [Clip] inputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1284 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1285 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1286 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1287 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_8_output_0 for ONNX tensor: /model/decoder/decoder/Clip_8_output_0
[X] /model/decoder/decoder/Clip_8 [Clip] outputs: [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Div_2 [Div]
[X] Parsing node: /model/decoder/decoder/Div_2 [Div]
[X] Searching for input: /model/decoder/decoder/Clip_7_output_0
[X] Searching for input: /model/decoder/decoder/Clip_8_output_0
[X] /model/decoder/decoder/Div_2 [Div] inputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Div_2 for ONNX node: /model/decoder/decoder/Div_2
[X] Registering tensor: /model/decoder/decoder/Div_2_output_0 for ONNX tensor: /model/decoder/decoder/Div_2_output_0
[X] /model/decoder/decoder/Div_2 [Div] outputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Log_2 [Log]
[X] Parsing node: /model/decoder/decoder/Log_2 [Log]
[X] Searching for input: /model/decoder/decoder/Div_2_output_0
[X] /model/decoder/decoder/Log_2 [Log] inputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Log_2 for ONNX node: /model/decoder/decoder/Log_2
[X] Registering tensor: /model/decoder/decoder/Log_2_output_0 for ONNX tensor: /model/decoder/decoder/Log_2_output_0
[X] /model/decoder/decoder/Log_2 [Log] outputs: [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/Add_2 [Add]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0
[X] Searching for input: /model/decoder/decoder/Log_2_output_0
[X] /model/decoder/decoder/Add_2 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Add_2 for ONNX node: /model/decoder/decoder/Add_2
[X] Registering tensor: /model/decoder/decoder/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/Add_2_output_0
[X] /model/decoder/decoder/Add_2 [Add] outputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid]
[X] Parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid]
[X] Searching for input: /model/decoder/decoder/Add_2_output_0
[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] inputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Sigmoid_3 for ONNX node: /model/decoder/decoder/Sigmoid_3
[X] Registering tensor: /model/decoder/decoder/Sigmoid_3_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_3_output_0
[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_score_head.2.weight
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_score_head.2.weight -> (80, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], 
[X] Registering layer: model.decoder.dec_score_head.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1288 for ONNX node: tmp_weight_1288
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], 
[X] Registering layer: tmp_weight_1289 for ONNX node: tmp_weight_1289
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Transpose for ONNX node: /model/decoder/decoder/dec_score_head.2/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0
[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Transpose_output_0
[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1290 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1291 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/MatMul for ONNX node: /model/decoder/decoder/dec_score_head.2/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0
[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add]
[X] Searching for input: model.decoder.dec_score_head.2.bias
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/MatMul_output_0
[X] /model/decoder/decoder/dec_score_head.2/Add [Add] inputs: [model.decoder.dec_score_head.2.bias -> (80)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Registering layer: model.decoder.dec_score_head.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1292 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1293 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Add for ONNX node: /model/decoder/decoder/dec_score_head.2/Add
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0
[X] /model/decoder/decoder/dec_score_head.2/Add [Add] outputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/Sigmoid_3_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/Unsqueeze_3 for ONNX node: /model/decoder/decoder/Unsqueeze_3
[X] Registering tensor: /model/decoder/decoder/Unsqueeze_3_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_3_output_0
[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Add_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] inputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/Unsqueeze_4 for ONNX node: /model/decoder/decoder/Unsqueeze_4
[X] Registering tensor: /model/decoder/decoder/Unsqueeze_4_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_4_output_0
[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Gather_8 [Gather]
[X] Parsing node: /model/decoder/Gather_8 [Gather]
[X] Searching for input: /model/decoder/decoder/Unsqueeze_4_output_0
[X] Searching for input: /model/encoder/Constant_2_output_0
[X] /model/decoder/Gather_8 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], 
[X] Registering layer: /model/encoder/Constant_2_output_0 required by ONNX-TRT
[X] Using Gather axis: 0
[X] Registering layer: ONNXTRT_castHelper_1294 required by ONNX-TRT
[X] Registering layer: /model/decoder/Gather_8 for ONNX node: /model/decoder/Gather_8
[X] Registering tensor: /model/decoder/Gather_8_output_0 for ONNX tensor: /model/decoder/Gather_8_output_0
[X] /model/decoder/Gather_8 [Gather] outputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Gather_9 [Gather]
[X] Parsing node: /model/decoder/Gather_9 [Gather]
[X] Searching for input: /model/decoder/decoder/Unsqueeze_3_output_0
[X] Searching for input: /model/encoder/Constant_2_output_0
[X] /model/decoder/Gather_9 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], 
[X] Using Gather axis: 0
[X] Registering layer: ONNXTRT_castHelper_1295 required by ONNX-TRT
[X] Registering layer: /model/decoder/Gather_9 for ONNX node: /model/decoder/Gather_9
[X] Registering tensor: /model/decoder/Gather_9_output_0 for ONNX tensor: /model/decoder/Gather_9_output_0
[X] /model/decoder/Gather_9 [Gather] outputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Split [Split]
[X] Parsing node: /postprocessor/Split [Split]
[X] Searching for input: /model/decoder/Gather_9_output_0
[X] Searching for input: /postprocessor/Constant_output_0
[X] /postprocessor/Split [Split] inputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Constant_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_1296 required by ONNX-TRT
[X] Registering layer: /postprocessor/Split for ONNX node: /postprocessor/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1297 required by ONNX-TRT
[X] Registering layer: /postprocessor/Split_1298 for ONNX node: /postprocessor/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1299 required by ONNX-TRT
[X] Registering layer: /postprocessor/Split_1300 for ONNX node: /postprocessor/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1301 required by ONNX-TRT
[X] Registering layer: /postprocessor/Split_1302 for ONNX node: /postprocessor/Split
[X] Registering tensor: /postprocessor/Split_output_0 for ONNX tensor: /postprocessor/Split_output_0
[X] Registering tensor: /postprocessor/Split_output_1 for ONNX tensor: /postprocessor/Split_output_1
[X] Registering tensor: /postprocessor/Split_output_2 for ONNX tensor: /postprocessor/Split_output_2
[X] Registering tensor: /postprocessor/Split_output_3 for ONNX tensor: /postprocessor/Split_output_3
[X] /postprocessor/Split [Split] outputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Squeeze [Squeeze]
[X] Parsing node: /postprocessor/Squeeze [Squeeze]
[X] Searching for input: /postprocessor/Split_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Squeeze [Squeeze] inputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Squeeze for ONNX node: /postprocessor/Squeeze
[X] Registering tensor: /postprocessor/Squeeze_output_0 for ONNX tensor: /postprocessor/Squeeze_output_0
[X] /postprocessor/Squeeze [Squeeze] outputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Squeeze_1 [Squeeze]
[X] Parsing node: /postprocessor/Squeeze_1 [Squeeze]
[X] Searching for input: /postprocessor/Split_output_1
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Squeeze_1 [Squeeze] inputs: [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Squeeze_1 for ONNX node: /postprocessor/Squeeze_1
[X] Registering tensor: /postprocessor/Squeeze_1_output_0 for ONNX tensor: /postprocessor/Squeeze_1_output_0
[X] /postprocessor/Squeeze_1 [Squeeze] outputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Squeeze_2 [Squeeze]
[X] Parsing node: /postprocessor/Squeeze_2 [Squeeze]
[X] Searching for input: /postprocessor/Split_output_2
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Squeeze_2 [Squeeze] inputs: [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Squeeze_2 for ONNX node: /postprocessor/Squeeze_2
[X] Registering tensor: /postprocessor/Squeeze_2_output_0 for ONNX tensor: /postprocessor/Squeeze_2_output_0
[X] /postprocessor/Squeeze_2 [Squeeze] outputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Squeeze_3 [Squeeze]
[X] Parsing node: /postprocessor/Squeeze_3 [Squeeze]
[X] Searching for input: /postprocessor/Split_output_3
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Squeeze_3 [Squeeze] inputs: [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Squeeze_3 for ONNX node: /postprocessor/Squeeze_3
[X] Registering tensor: /postprocessor/Squeeze_3_output_0 for ONNX tensor: /postprocessor/Squeeze_3_output_0
[X] /postprocessor/Squeeze_3 [Squeeze] outputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Mul [Mul]
[X] Parsing node: /postprocessor/Mul [Mul]
[X] Searching for input: /postprocessor/Squeeze_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /postprocessor/Mul [Mul] inputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1303 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1304 required by ONNX-TRT
[X] Registering layer: /postprocessor/Mul for ONNX node: /postprocessor/Mul
[X] Registering tensor: /postprocessor/Mul_output_0 for ONNX tensor: /postprocessor/Mul_output_0
[X] /postprocessor/Mul [Mul] outputs: [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Sub [Sub]
[X] Parsing node: /postprocessor/Sub [Sub]
[X] Searching for input: /postprocessor/Squeeze_output_0
[X] Searching for input: /postprocessor/Mul_output_0
[X] /postprocessor/Sub [Sub] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], 
[X] Registering layer: /postprocessor/Sub for ONNX node: /postprocessor/Sub
[X] Registering tensor: /postprocessor/Sub_output_0 for ONNX tensor: /postprocessor/Sub_output_0
[X] /postprocessor/Sub [Sub] outputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Mul_1 [Mul]
[X] Parsing node: /postprocessor/Mul_1 [Mul]
[X] Searching for input: /postprocessor/Squeeze_3_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /postprocessor/Mul_1 [Mul] inputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1305 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1306 required by ONNX-TRT
[X] Registering layer: /postprocessor/Mul_1 for ONNX node: /postprocessor/Mul_1
[X] Registering tensor: /postprocessor/Mul_1_output_0 for ONNX tensor: /postprocessor/Mul_1_output_0
[X] /postprocessor/Mul_1 [Mul] outputs: [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Sub_1 [Sub]
[X] Parsing node: /postprocessor/Sub_1 [Sub]
[X] Searching for input: /postprocessor/Squeeze_1_output_0
[X] Searching for input: /postprocessor/Mul_1_output_0
[X] /postprocessor/Sub_1 [Sub] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], 
[X] Registering layer: /postprocessor/Sub_1 for ONNX node: /postprocessor/Sub_1
[X] Registering tensor: /postprocessor/Sub_1_output_0 for ONNX tensor: /postprocessor/Sub_1_output_0
[X] /postprocessor/Sub_1 [Sub] outputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Add [Add]
[X] Parsing node: /postprocessor/Add [Add]
[X] Searching for input: /postprocessor/Squeeze_output_0
[X] Searching for input: /postprocessor/Mul_output_0
[X] /postprocessor/Add [Add] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], 
[X] Registering layer: /postprocessor/Add for ONNX node: /postprocessor/Add
[X] Registering tensor: /postprocessor/Add_output_0 for ONNX tensor: /postprocessor/Add_output_0
[X] /postprocessor/Add [Add] outputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Add_1 [Add]
[X] Parsing node: /postprocessor/Add_1 [Add]
[X] Searching for input: /postprocessor/Squeeze_1_output_0
[X] Searching for input: /postprocessor/Mul_1_output_0
[X] /postprocessor/Add_1 [Add] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], 
[X] Registering layer: /postprocessor/Add_1 for ONNX node: /postprocessor/Add_1
[X] Registering tensor: /postprocessor/Add_1_output_0 for ONNX tensor: /postprocessor/Add_1_output_0
[X] /postprocessor/Add_1 [Add] outputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze [Unsqueeze]
[X] Searching for input: /postprocessor/Sub_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze [Unsqueeze] inputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze for ONNX node: /postprocessor/Unsqueeze
[X] Registering tensor: /postprocessor/Unsqueeze_output_0 for ONNX tensor: /postprocessor/Unsqueeze_output_0
[X] /postprocessor/Unsqueeze [Unsqueeze] outputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze]
[X] Searching for input: /postprocessor/Sub_1_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze_1 [Unsqueeze] inputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze_1 for ONNX node: /postprocessor/Unsqueeze_1
[X] Registering tensor: /postprocessor/Unsqueeze_1_output_0 for ONNX tensor: /postprocessor/Unsqueeze_1_output_0
[X] /postprocessor/Unsqueeze_1 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze]
[X] Searching for input: /postprocessor/Add_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze_2 [Unsqueeze] inputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze_2 for ONNX node: /postprocessor/Unsqueeze_2
[X] Registering tensor: /postprocessor/Unsqueeze_2_output_0 for ONNX tensor: /postprocessor/Unsqueeze_2_output_0
[X] /postprocessor/Unsqueeze_2 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze]
[X] Searching for input: /postprocessor/Add_1_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze_3 [Unsqueeze] inputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze_3 for ONNX node: /postprocessor/Unsqueeze_3
[X] Registering tensor: /postprocessor/Unsqueeze_3_output_0 for ONNX tensor: /postprocessor/Unsqueeze_3_output_0
[X] /postprocessor/Unsqueeze_3 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Concat [Concat]
[X] Parsing node: /postprocessor/Concat [Concat]
[X] Searching for input: /postprocessor/Unsqueeze_output_0
[X] Searching for input: /postprocessor/Unsqueeze_1_output_0
[X] Searching for input: /postprocessor/Unsqueeze_2_output_0
[X] Searching for input: /postprocessor/Unsqueeze_3_output_0
[X] /postprocessor/Concat [Concat] inputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Registering layer: /postprocessor/Concat for ONNX node: /postprocessor/Concat
[X] Registering tensor: /postprocessor/Concat_output_0 for ONNX tensor: /postprocessor/Concat_output_0
[X] /postprocessor/Concat [Concat] outputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Tile [Tile]
[X] Parsing node: /postprocessor/Tile [Tile]
[X] Searching for input: orig_target_sizes
[X] Searching for input: onnx::Tile_3498
[X] /postprocessor/Tile [Tile] inputs: [orig_target_sizes -> (1, 2)[INT64]], [onnx::Tile_3498 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_1307 required by ONNX-TRT
[X] Registering layer: /postprocessor/Tile for ONNX node: /postprocessor/Tile
[X] Registering tensor: /postprocessor/Tile_output_0 for ONNX tensor: /postprocessor/Tile_output_0
[X] /postprocessor/Tile [Tile] outputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze]
[X] Searching for input: /postprocessor/Tile_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /postprocessor/Unsqueeze_4 [Unsqueeze] inputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/Constant_21_output_0 required by ONNX-TRT
[X] Registering layer: /postprocessor/Unsqueeze_4 for ONNX node: /postprocessor/Unsqueeze_4
[X] Registering tensor: /postprocessor/Unsqueeze_4_output_0 for ONNX tensor: /postprocessor/Unsqueeze_4_output_0
[X] /postprocessor/Unsqueeze_4 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], 
[X] Static check for parsing node: Cast_3039 [Cast]
[X] Parsing node: Cast_3039 [Cast]
[X] Searching for input: /postprocessor/Unsqueeze_4_output_0
[X] Cast_3039 [Cast] inputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], 
[X] Casting to type: float32
[X] Registering layer: Cast_3039 for ONNX node: Cast_3039
[X] Registering tensor: onnx::Mul_3505 for ONNX tensor: onnx::Mul_3505
[X] Cast_3039 [Cast] outputs: [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Mul_2 [Mul]
[X] Parsing node: /postprocessor/Mul_2 [Mul]
[X] Searching for input: /postprocessor/Concat_output_0
[X] Searching for input: onnx::Mul_3505
[X] /postprocessor/Mul_2 [Mul] inputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], 
[X] Registering layer: /postprocessor/Mul_2 for ONNX node: /postprocessor/Mul_2
[X] Registering tensor: /postprocessor/Mul_2_output_0 for ONNX tensor: /postprocessor/Mul_2_output_0
[X] /postprocessor/Mul_2 [Mul] outputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Sigmoid [Sigmoid]
[X] Parsing node: /postprocessor/Sigmoid [Sigmoid]
[X] Searching for input: /model/decoder/Gather_8_output_0
[X] /postprocessor/Sigmoid [Sigmoid] inputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Registering layer: /postprocessor/Sigmoid for ONNX node: /postprocessor/Sigmoid
[X] Registering tensor: /postprocessor/Sigmoid_output_0 for ONNX tensor: /postprocessor/Sigmoid_output_0
[X] /postprocessor/Sigmoid [Sigmoid] outputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Flatten [Flatten]
[X] Parsing node: /postprocessor/Flatten [Flatten]
[X] Searching for input: /postprocessor/Sigmoid_output_0
[X] /postprocessor/Flatten [Flatten] inputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1308 required by ONNX-TRT
[X] Registering layer: /postprocessor/Flatten for ONNX node: /postprocessor/Flatten
[X] Registering tensor: /postprocessor/Flatten_output_0 for ONNX tensor: /postprocessor/Flatten_output_0
[X] /postprocessor/Flatten [Flatten] outputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/TopK [TopK]
[X] Parsing node: /postprocessor/TopK [TopK]
[X] Searching for input: /postprocessor/Flatten_output_0
[X] Searching for input: /model/decoder/Constant_18_output_0
[X] /postprocessor/TopK [TopK] inputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_convertToScalar_1309 required by ONNX-TRT
[X] Registering layer: /postprocessor/TopK for ONNX node: /postprocessor/TopK
[X] Registering layer: ONNXTRT_castHelper_1310 required by ONNX-TRT
[X] Registering tensor: scores_1311 for ONNX tensor: scores
[X] Registering tensor: /postprocessor/TopK_output_1 for ONNX tensor: /postprocessor/TopK_output_1
[X] /postprocessor/TopK [TopK] outputs: [scores -> (1, 300)[FLOAT]], [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], 
[X] Static check for parsing node: /postprocessor/Div [Div]
[X] Parsing node: /postprocessor/Div [Div]
[X] Searching for input: /postprocessor/TopK_output_1
[X] Searching for input: /postprocessor/Constant_14_output_0
[X] /postprocessor/Div [Div] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], 
[X] Registering layer: /postprocessor/Constant_14_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1312 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1313 required by ONNX-TRT
[X] Registering layer: /postprocessor/Div for ONNX node: /postprocessor/Div
[X] Registering tensor: /postprocessor/Div_output_0 for ONNX tensor: /postprocessor/Div_output_0
[X] /postprocessor/Div [Div] outputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], 
[X] Static check for parsing node: /postprocessor/Mul_3 [Mul]
[X] Parsing node: /postprocessor/Mul_3 [Mul]
[X] Searching for input: /postprocessor/Div_output_0
[X] Searching for input: /postprocessor/Constant_14_output_0
[X] /postprocessor/Mul_3 [Mul] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1314 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1315 required by ONNX-TRT
[X] Registering layer: /postprocessor/Mul_3 for ONNX node: /postprocessor/Mul_3
[X] Registering tensor: /postprocessor/Mul_3_output_0 for ONNX tensor: /postprocessor/Mul_3_output_0
[X] /postprocessor/Mul_3 [Mul] outputs: [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], 
[X] Static check for parsing node: /postprocessor/Sub_2 [Sub]
[X] Parsing node: /postprocessor/Sub_2 [Sub]
[X] Searching for input: /postprocessor/TopK_output_1
[X] Searching for input: /postprocessor/Mul_3_output_0
[X] /postprocessor/Sub_2 [Sub] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], 
[X] Registering layer: /postprocessor/Sub_2 for ONNX node: /postprocessor/Sub_2
[X] Registering tensor: labels_1316 for ONNX tensor: labels
[X] /postprocessor/Sub_2 [Sub] outputs: [labels -> (1, 300)[INT64]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze]
[X] Searching for input: /postprocessor/Div_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze_5 [Unsqueeze] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze_5 for ONNX node: /postprocessor/Unsqueeze_5
[X] Registering tensor: /postprocessor/Unsqueeze_5_output_0 for ONNX tensor: /postprocessor/Unsqueeze_5_output_0
[X] /postprocessor/Unsqueeze_5 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], 
[X] Static check for parsing node: /postprocessor/Tile_1 [Tile]
[X] Parsing node: /postprocessor/Tile_1 [Tile]
[X] Searching for input: /postprocessor/Unsqueeze_5_output_0
[X] Searching for input: /model/decoder/Concat_5_output_0
[X] /postprocessor/Tile_1 [Tile] inputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_1317 required by ONNX-TRT
[X] Registering layer: /postprocessor/Tile_1 for ONNX node: /postprocessor/Tile_1
[X] Registering tensor: /postprocessor/Tile_1_output_0 for ONNX tensor: /postprocessor/Tile_1_output_0
[X] /postprocessor/Tile_1 [Tile] outputs: [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], 
[X] Static check for parsing node: /postprocessor/GatherElements [GatherElements]
[X] Parsing node: /postprocessor/GatherElements [GatherElements]
[X] Searching for input: /postprocessor/Mul_2_output_0
[X] Searching for input: /postprocessor/Tile_1_output_0
[X] /postprocessor/GatherElements [GatherElements] inputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], 
[X] Using Gather axis: 1
[X] Registering layer: ONNXTRT_castHelper_1318 required by ONNX-TRT
[X] Registering layer: /postprocessor/GatherElements for ONNX node: /postprocessor/GatherElements
[X] Registering tensor: boxes_1319 for ONNX tensor: boxes
[X] /postprocessor/GatherElements [GatherElements] outputs: [boxes -> (1, 300, 4)[FLOAT]], 
[X] Marking /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0_384 as output: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
[X] Marking /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0_705 as output: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
[X] Marking /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0_1028 as output: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
[X] Marking /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0_376 as output: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[X] Marking /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0_697 as output: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[X] Marking /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0_1020 as output: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[X] Marking labels_1316 as output: labels
[W] ModelImporter.cpp:804: Make sure output labels has Int64 binding.
[X] Marking boxes_1319 as output: boxes
[X] Marking scores_1311 as output: scores
[I] Building engine with configuration:
    Flags                  | [TF32]
    Engine Capability      | EngineCapability.STANDARD
    Memory Pools           | [WORKSPACE: 1024.00 MiB, TACTIC_DRAM: 24105.06 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]
    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]
    Profiling Verbosity    | ProfilingVerbosity.DETAILED
    Preview Features       | [PROFILE_SHARING_0806]
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Original: 1920 layers
[X] After dead-layer removal: 1920 layers
[X] Graph construction completed in 0.0210734 seconds.
[X] After adding DebugOutput nodes: 1920 layers
[X] Running: ConstShuffleFusion on onnx::MatMul_3619
[X] ConstShuffleFusion: Fusing onnx::MatMul_3619 with ONNXTRT_Broadcast
[X] Running: ConstShuffleFusion on onnx::Add_3614
[X] ConstShuffleFusion: Fusing onnx::Add_3614 with ONNXTRT_Broadcast_99
[X] Running: ConstShuffleFusion on onnx::MatMul_3620
[X] ConstShuffleFusion: Fusing onnx::MatMul_3620 with ONNXTRT_Broadcast_101
[X] Running: ConstShuffleFusion on onnx::Add_3616
[X] ConstShuffleFusion: Fusing onnx::Add_3616 with ONNXTRT_Broadcast_103
[X] Running: ConstShuffleFusion on onnx::MatMul_3621
[X] ConstShuffleFusion: Fusing onnx::MatMul_3621 with ONNXTRT_Broadcast_105
[X] Running: ConstShuffleFusion on onnx::Add_3618
[X] ConstShuffleFusion: Fusing onnx::Add_3618 with ONNXTRT_Broadcast_107
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.self_attn.out_proj.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_116
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.weight
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.weight with ONNXTRT_Broadcast_121
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.bias with ONNXTRT_Broadcast_123
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear1.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear1.bias with ONNXTRT_Broadcast_131
[X] Running: ConstShuffleFusion on /model/encoder/encoder.0/layers.0/activation/Constant_output_0
[X] ConstShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/activation/Constant_output_0 with ONNXTRT_Broadcast_133
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear2.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear2.bias with ONNXTRT_Broadcast_145
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.weight
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.weight with ONNXTRT_Broadcast_149
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.bias with ONNXTRT_Broadcast_151
[X] Running: ConstShuffleFusion on model.decoder.enc_output.proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_output.proj.bias with ONNXTRT_Broadcast_275
[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.weight
[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.weight with ONNXTRT_Broadcast_279
[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.bias with ONNXTRT_Broadcast_281
[X] Running: ConstShuffleFusion on model.decoder.enc_score_head.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_score_head.bias with ONNXTRT_Broadcast_289
[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.0.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.0.bias with ONNXTRT_Broadcast_295
[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.1.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.1.bias with ONNXTRT_Broadcast_303
[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.2.bias with ONNXTRT_Broadcast_311
[X] Running: ConstShuffleFusion on onnx::MatMul_3736
[X] ConstShuffleFusion: Fusing onnx::MatMul_3736 with ONNXTRT_Broadcast_332
[X] Running: ConstShuffleFusion on onnx::Add_3731
[X] ConstShuffleFusion: Fusing onnx::Add_3731 with ONNXTRT_Broadcast_334
[X] Running: ConstShuffleFusion on onnx::MatMul_3737
[X] ConstShuffleFusion: Fusing onnx::MatMul_3737 with ONNXTRT_Broadcast_336
[X] Running: ConstShuffleFusion on onnx::Add_3733
[X] ConstShuffleFusion: Fusing onnx::Add_3733 with ONNXTRT_Broadcast_338
[X] Running: ConstShuffleFusion on onnx::MatMul_3738
[X] ConstShuffleFusion: Fusing onnx::MatMul_3738 with ONNXTRT_Broadcast_340
[X] Running: ConstShuffleFusion on onnx::Add_3735
[X] ConstShuffleFusion: Fusing onnx::Add_3735 with ONNXTRT_Broadcast_342
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.self_attn.out_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_351
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.weight with ONNXTRT_Broadcast_356
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.bias with ONNXTRT_Broadcast_358
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.value_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.value_proj.bias with ONNXTRT_Broadcast_366
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_375
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.attention_weights.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_383
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.output_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.output_proj.bias with ONNXTRT_Broadcast_577
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.weight with ONNXTRT_Broadcast_581
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.bias with ONNXTRT_Broadcast_583
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear1.bias with ONNXTRT_Broadcast_591
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear2.bias with ONNXTRT_Broadcast_599
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.weight with ONNXTRT_Broadcast_603
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.bias with ONNXTRT_Broadcast_605
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.0.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.0.bias with ONNXTRT_Broadcast_613
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.1.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.1.bias with ONNXTRT_Broadcast_621
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.2.bias with ONNXTRT_Broadcast_629
[X] Running: ConstShuffleFusion on (Unnamed Layer* 1426) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1426) [Constant] with ONNXTRT_Broadcast_634
[X] Running: ConstShuffleFusion on (Unnamed Layer* 1433) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1433) [Constant] with ONNXTRT_Broadcast_641
[X] Running: ConstShuffleFusion on onnx::MatMul_3808
[X] ConstShuffleFusion: Fusing onnx::MatMul_3808 with ONNXTRT_Broadcast_655
[X] Running: ConstShuffleFusion on onnx::Add_3803
[X] ConstShuffleFusion: Fusing onnx::Add_3803 with ONNXTRT_Broadcast_657
[X] Running: ConstShuffleFusion on onnx::MatMul_3809
[X] ConstShuffleFusion: Fusing onnx::MatMul_3809 with ONNXTRT_Broadcast_659
[X] Running: ConstShuffleFusion on onnx::Add_3805
[X] ConstShuffleFusion: Fusing onnx::Add_3805 with ONNXTRT_Broadcast_661
[X] Running: ConstShuffleFusion on onnx::MatMul_3810
[X] ConstShuffleFusion: Fusing onnx::MatMul_3810 with ONNXTRT_Broadcast_663
[X] Running: ConstShuffleFusion on onnx::Add_3807
[X] ConstShuffleFusion: Fusing onnx::Add_3807 with ONNXTRT_Broadcast_665
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.self_attn.out_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.self_attn.out_proj.bias with ONNXTRT_Broadcast_674
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.weight with ONNXTRT_Broadcast_679
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.bias with ONNXTRT_Broadcast_681
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.value_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.value_proj.bias with ONNXTRT_Broadcast_687
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_696
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.attention_weights.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_704
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.output_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.output_proj.bias with ONNXTRT_Broadcast_900
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.weight with ONNXTRT_Broadcast_904
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.bias with ONNXTRT_Broadcast_906
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear1.bias with ONNXTRT_Broadcast_914
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear2.bias with ONNXTRT_Broadcast_922
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.weight with ONNXTRT_Broadcast_926
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.bias with ONNXTRT_Broadcast_928
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.0.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.0.bias with ONNXTRT_Broadcast_936
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.1.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.1.bias with ONNXTRT_Broadcast_944
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.2.bias with ONNXTRT_Broadcast_952
[X] Running: ConstShuffleFusion on (Unnamed Layer* 1834) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1834) [Constant] with ONNXTRT_Broadcast_957
[X] Running: ConstShuffleFusion on (Unnamed Layer* 1841) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1841) [Constant] with ONNXTRT_Broadcast_964
[X] Running: ConstShuffleFusion on onnx::MatMul_3880
[X] ConstShuffleFusion: Fusing onnx::MatMul_3880 with ONNXTRT_Broadcast_978
[X] Running: ConstShuffleFusion on onnx::Add_3875
[X] ConstShuffleFusion: Fusing onnx::Add_3875 with ONNXTRT_Broadcast_980
[X] Running: ConstShuffleFusion on onnx::MatMul_3881
[X] ConstShuffleFusion: Fusing onnx::MatMul_3881 with ONNXTRT_Broadcast_982
[X] Running: ConstShuffleFusion on onnx::Add_3877
[X] ConstShuffleFusion: Fusing onnx::Add_3877 with ONNXTRT_Broadcast_984
[X] Running: ConstShuffleFusion on onnx::MatMul_3882
[X] ConstShuffleFusion: Fusing onnx::MatMul_3882 with ONNXTRT_Broadcast_986
[X] Running: ConstShuffleFusion on onnx::Add_3879
[X] ConstShuffleFusion: Fusing onnx::Add_3879 with ONNXTRT_Broadcast_988
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.self_attn.out_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.self_attn.out_proj.bias with ONNXTRT_Broadcast_997
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.weight with ONNXTRT_Broadcast_1002
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.bias with ONNXTRT_Broadcast_1004
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.value_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.value_proj.bias with ONNXTRT_Broadcast_1010
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_1019
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.attention_weights.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_1027
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.output_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.output_proj.bias with ONNXTRT_Broadcast_1223
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.weight with ONNXTRT_Broadcast_1227
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.bias with ONNXTRT_Broadcast_1229
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear1.bias with ONNXTRT_Broadcast_1237
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear2.bias with ONNXTRT_Broadcast_1245
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.weight with ONNXTRT_Broadcast_1249
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.bias with ONNXTRT_Broadcast_1251
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.0.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.0.bias with ONNXTRT_Broadcast_1259
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.1.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.1.bias with ONNXTRT_Broadcast_1267
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.2.bias with ONNXTRT_Broadcast_1275
[X] Running: ConstShuffleFusion on (Unnamed Layer* 2242) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2242) [Constant] with ONNXTRT_Broadcast_1280
[X] Running: ConstShuffleFusion on (Unnamed Layer* 2249) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2249) [Constant] with ONNXTRT_Broadcast_1287
[X] Running: ConstShuffleFusion on model.decoder.dec_score_head.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_score_head.2.bias with ONNXTRT_Broadcast_1293
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear1/Transpose with ONNXTRT_Broadcast_129
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear2/Transpose with ONNXTRT_Broadcast_143
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_output/proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_output/proj/Transpose with ONNXTRT_Broadcast_273
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_score_head/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_score_head/Transpose with ONNXTRT_Broadcast_287
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.0/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.0/Transpose with ONNXTRT_Broadcast_293
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.1/Transpose with ONNXTRT_Broadcast_301
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.2/Transpose with ONNXTRT_Broadcast_309
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_364
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_373
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_381
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_575
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear1/Transpose with ONNXTRT_Broadcast_589
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear2/Transpose with ONNXTRT_Broadcast_597
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose with ONNXTRT_Broadcast_611
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose with ONNXTRT_Broadcast_619
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose with ONNXTRT_Broadcast_627
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_685
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_694
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_702
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_898
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear1/Transpose with ONNXTRT_Broadcast_912
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear2/Transpose with ONNXTRT_Broadcast_920
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose with ONNXTRT_Broadcast_934
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose with ONNXTRT_Broadcast_942
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose with ONNXTRT_Broadcast_950
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_1008
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_1017
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_1025
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_1221
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear1/Transpose with ONNXTRT_Broadcast_1235
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear2/Transpose with ONNXTRT_Broadcast_1243
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose with ONNXTRT_Broadcast_1257
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose with ONNXTRT_Broadcast_1265
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose with ONNXTRT_Broadcast_1273
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_score_head.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_score_head.2/Transpose with ONNXTRT_Broadcast_1291
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape with /model/encoder/encoder.0/layers.0/self_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_4
[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_113
[X] Removing ONNXTRT_ShapeShuffle_113
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Transpose_5
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 with /model/encoder/encoder.0/layers.0/self_attn/Reshape_3
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_4
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_6
[X] Running: ShuffleShuffleFusion on /model/encoder/Transpose_1
[X] ShuffleShuffleFusion: Fusing /model/encoder/Transpose_1 with /model/encoder/Reshape_1
[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape with /model/decoder/Transpose
[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_1 with /model/decoder/Transpose_1
[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_2 with /model/decoder/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape with /model/decoder/decoder/layers.0/cross_attn/Transpose
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape with /model/decoder/decoder/layers.1/cross_attn/Transpose
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape with /model/decoder/decoder/layers.2/cross_attn/Transpose
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_2 with /model/decoder/decoder/layers.0/self_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape with /model/decoder/decoder/layers.0/self_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_1 with /model/decoder/decoder/layers.0/self_attn/Transpose_4
[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_348
[X] Removing ONNXTRT_ShapeShuffle_348
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Transpose_5
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Transpose_5 with /model/decoder/decoder/layers.0/self_attn/Reshape_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_4
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_4 with /model/decoder/decoder/layers.0/self_attn/Transpose_6
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_386
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_386 with /model/decoder/decoder/layers.0/cross_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_386 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_386 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2 with /model/decoder/decoder/layers.0/cross_attn/Reshape_9
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Transpose_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Transpose_1 with /model/decoder/decoder/layers.0/cross_attn/Reshape_5
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape_10
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape_10 with /model/decoder/decoder/layers.0/cross_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_2 with /model/decoder/decoder/layers.1/self_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape with /model/decoder/decoder/layers.1/self_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_1 with /model/decoder/decoder/layers.1/self_attn/Transpose_4
[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_671
[X] Removing ONNXTRT_ShapeShuffle_671
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Transpose_5
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Transpose_5 with /model/decoder/decoder/layers.1/self_attn/Reshape_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_4
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_4 with /model/decoder/decoder/layers.1/self_attn/Transpose_6
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_707
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_707 with /model/decoder/decoder/layers.1/cross_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_707 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_707 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2 with /model/decoder/decoder/layers.1/cross_attn/Reshape_9
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Transpose_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Transpose_1 with /model/decoder/decoder/layers.1/cross_attn/Reshape_5
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape_10
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape_10 with /model/decoder/decoder/layers.1/cross_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_2 with /model/decoder/decoder/layers.2/self_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape with /model/decoder/decoder/layers.2/self_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_1 with /model/decoder/decoder/layers.2/self_attn/Transpose_4
[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_994
[X] Removing ONNXTRT_ShapeShuffle_994
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Transpose_5
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Transpose_5 with /model/decoder/decoder/layers.2/self_attn/Reshape_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_4
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_4 with /model/decoder/decoder/layers.2/self_attn/Transpose_6
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1030
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1030 with /model/decoder/decoder/layers.2/cross_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1030 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1030 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2 with /model/decoder/decoder/layers.2/cross_attn/Reshape_9
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Transpose_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Transpose_1 with /model/decoder/decoder/layers.2/cross_attn/Reshape_5
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape_10
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape_10 with /model/decoder/decoder/layers.2/cross_attn/Transpose_3
[X] QDQ graph optimizer - constant folding of Q/DQ initializers
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_5
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_9
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_13
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_17
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_19
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_23
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_27
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_31
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_35
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_39
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_43
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_47
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_51
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_55
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_59
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_63
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_67
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_71
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_75
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_79
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_83
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_87
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_89
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_91
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_95
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_126
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_140
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_155
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_159
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_163
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_167
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_171
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_173
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_177
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_181
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_185
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_189
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_193
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_197
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_199
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_203
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_207
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_211
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_215
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_219
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_223
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_225
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_229
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_233
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_237
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_241
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_245
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_249
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_251
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_255
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_257
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_259
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_263
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_270
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_284
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_290
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_298
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_306
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_317
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_325
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_361
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_370
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_378
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_572
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_586
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_594
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_608
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_616
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_624
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_682
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_691
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_699
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_895
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_909
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_917
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_931
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_939
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_947
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1005
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1014
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1022
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1218
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1232
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1240
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1254
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1262
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1270
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1288
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_2
[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_10
[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_18
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_24
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_32
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_40
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_48
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_56
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_64
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_72
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_80
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_88
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_92
[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_127
[X] Removing /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_156
[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_164
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_172
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_178
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_186
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_194
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_200
[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_208
[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_216
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_224
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_230
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_238
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_246
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_252
[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_258
[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_264
[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_285
[X] Removing /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_299
[X] Removing /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_318
[X] Removing /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_362
[X] Removing /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_379
[X] Removing /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_587
[X] Removing /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_609
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_625
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_692
[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_896
[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_918
[X] Removing /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_940
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1006
[X] Removing /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1023
[X] Removing /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1233
[X] Removing /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1255
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1271
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_3
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_4
[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_7
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_8
[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_11
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_12
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_15
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_16
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_21
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_22
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_25
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_26
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_29
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_30
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_37
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_38
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_34
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_41
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_42
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_45
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_46
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_49
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_50
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_57
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_58
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_54
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_61
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_62
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_65
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_66
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_69
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_70
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_77
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_78
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_74
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_81
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_82
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_85
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_86
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_93
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_94
[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_124
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_125
[X] Removing /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_138
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_139
[X] Removing /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_153
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_154
[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_157
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_158
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_161
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_162
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_165
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_166
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_169
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_170
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_175
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_176
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_179
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_180
[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_183
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_184
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_187
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_188
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_191
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_192
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_195
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_196
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_201
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_202
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_205
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_206
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_209
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_210
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_213
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_214
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_217
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_218
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_221
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_222
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_227
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_228
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_231
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_232
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_235
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_236
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_239
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_240
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_243
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_244
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_247
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_248
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_253
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_254
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_261
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_262
[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_359
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_268
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_360
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_269
[X] Removing /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_282
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_283
[X] Removing /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_296
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_297
[X] Removing /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_304
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_305
[X] Removing /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_315
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_316
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_323
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_324
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_368
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_369
[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_570
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_571
[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_584
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_585
[X] Removing /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_592
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_593
[X] Removing /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_606
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_607
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_614
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_615
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_622
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_623
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear
[X] Removing tmp_weight_642
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear
[X] Removing tmp_weight_643
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear
[X] Removing tmp_weight_648
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear
[X] Removing tmp_weight_649
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_689
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_690
[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_893
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_894
[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_907
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_908
[X] Removing /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_915
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_916
[X] Removing /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_929
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_930
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_937
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_938
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_945
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_946
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear
[X] Removing tmp_weight_965
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear
[X] Removing tmp_weight_966
[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear
[X] Removing tmp_weight_971
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear
[X] Removing tmp_weight_972
[X] Removing /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1012
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1013
[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1216
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1217
[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1230
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1231
[X] Removing /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1238
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1239
[X] Removing /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1252
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1253
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1260
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1261
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1268
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1269
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_6
[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_14
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_20
[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_28
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_36
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_44
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_52
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_60
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_68
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_76
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_84
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_90
[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_96
[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_141
[X] Removing /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_160
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_168
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_174
[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_182
[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_190
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_198
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_204
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_212
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_220
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_226
[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_234
[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_242
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_250
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_256
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_260
[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_271
[X] Removing /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_291
[X] Removing /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_307
[X] Removing /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_326
[X] Removing /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_371
[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_573
[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_595
[X] Removing /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_617
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_683
[X] Removing /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_700
[X] Removing /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_910
[X] Removing /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_932
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_948
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1015
[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1219
[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1241
[X] Removing /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1263
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1289
[X] Removing /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_33
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_53
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_73
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_4 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_4 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.2/self_attn/Softmax to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.1/self_attn/Softmax to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_3 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_3 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.0/self_attn/Softmax to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.
[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.
[X] Found /model/encoder/encoder.0/layers.0/self_attn/Softmax to be part of self-attention pattern.
[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.
[X] Found and reassigned Myelin backends for Self-Attention nodes
[X] After Myelin optimization: 464 layers
[X] QDQ graph optimizer - constant folding of Q/DQ initializers
[X] QDQ graph optimizer forward pass - DQ motions and fusions
[X] QDQ graph optimizer backward pass
[X] QDQ graph optimizer quantization pass - Generate quantized ops
[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.0/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.0/Add with /model/backbone/res_layers.0/blocks.0/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.1/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.1/Add with /model/backbone/res_layers.0/blocks.1/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.0/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.0/Add with /model/backbone/res_layers.1/blocks.0/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.1/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.1/Add with /model/backbone/res_layers.1/blocks.1/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.0/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.0/Add with /model/backbone/res_layers.2/blocks.0/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.1/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.1/Add with /model/backbone/res_layers.2/blocks.1/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.0/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.0/Add with /model/backbone/res_layers.3/blocks.0/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.1/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.1/Add with /model/backbone/res_layers.3/blocks.1/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_1/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_1/norm/BatchNormalization with /model/backbone/conv1/conv1_1/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_2/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_2/norm/BatchNormalization with /model/backbone/conv1/conv1_2/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_3/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_3/norm/BatchNormalization with /model/backbone/conv1/conv1_3/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu
[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_1.conv.weight with /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_2.conv.weight with /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_3.conv.weight with /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight with /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.2.conv.weight with /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.0.conv.weight with /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.1.conv.weight with /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.0.conv.weight with /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight with /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight with /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight with /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.1.conv.weight with /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight with /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight with /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight with /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.0.conv.weight with /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight with /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight with /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight with /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.1.conv.weight with /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight with /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight with /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight with /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.0.conv.weight with /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.1.conv.weight with /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.2.conv.weight with /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Running: VanillaSwapWithFollowingQ on /model/backbone/MaxPool
[X] Swapping /model/backbone/MaxPool with /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_2
[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_3
[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_4
[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_5
[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize
[X] Swapping /model/encoder/Resize with /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize_1
[X] Swapping /model/encoder/Resize_1 with /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Running: HorizontalMergeQNodes on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Eliminating /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 which duplicates (Q) /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] Running: PointWiseFusion on /model/encoder/lateral_convs.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.0/act/Sigmoid with /model/encoder/lateral_convs.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv1/act/Sigmoid with /model/encoder/fpn_blocks.0/conv1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv2/act/Sigmoid with /model/encoder/fpn_blocks.0/conv2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul) with /model/encoder/fpn_blocks.0/Add
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv3/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv3/act/Sigmoid with /model/encoder/fpn_blocks.0/conv3/act/Mul
[X] Running: PointWiseFusion on /model/encoder/lateral_convs.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.1/act/Sigmoid with /model/encoder/lateral_convs.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv1/act/Sigmoid with /model/encoder/fpn_blocks.1/conv1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv2/act/Sigmoid with /model/encoder/fpn_blocks.1/conv2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul) with /model/encoder/fpn_blocks.1/Add
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv3/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv3/act/Sigmoid with /model/encoder/fpn_blocks.1/conv3/act/Mul
[X] Running: PointWiseFusion on /model/encoder/downsample_convs.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.0/act/Sigmoid with /model/encoder/downsample_convs.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv1/act/Sigmoid with /model/encoder/pan_blocks.0/conv1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv2/act/Sigmoid with /model/encoder/pan_blocks.0/conv2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul) with /model/encoder/pan_blocks.0/Add
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv3/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv3/act/Sigmoid with /model/encoder/pan_blocks.0/conv3/act/Mul
[X] Running: PointWiseFusion on /model/encoder/downsample_convs.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.1/act/Sigmoid with /model/encoder/downsample_convs.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv1/act/Sigmoid with /model/encoder/pan_blocks.1/conv1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv2/act/Sigmoid with /model/encoder/pan_blocks.1/conv2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul) with /model/encoder/pan_blocks.1/Add
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv3/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv3/act/Sigmoid with /model/encoder/pan_blocks.1/conv3/act/Mul
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_1/conv/Conv
[X] Removing /model/backbone/conv1/conv1_1/norm/BatchNormalization + /model/backbone/conv1/conv1_1/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_2/conv/Conv
[X] Removing /model/backbone/conv1/conv1_2/norm/BatchNormalization + /model/backbone/conv1/conv1_2/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_3/conv/Conv
[X] Removing /model/backbone/conv1/conv1_3/norm/BatchNormalization + /model/backbone/conv1/conv1_3/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.0/conv/Conv
[X] Removing /model/encoder/input_proj.0/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.1/conv/Conv
[X] Removing /model/encoder/input_proj.1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.0/conv/Conv
[X] Removing /model/encoder/lateral_convs.0/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.1/conv/Conv
[X] Removing /model/encoder/lateral_convs.1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.0/conv/Conv
[X] Removing /model/encoder/downsample_convs.0/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.1/conv/Conv
[X] Removing /model/encoder/downsample_convs.1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/lateral_convs.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/lateral_convs.1/conv/Conv with PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.0/conv/Conv with PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.1/conv/Conv with PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_1/conv/Conv
[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_2/conv/Conv
[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_3/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_3/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_3/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1 and /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.2/conv/Conv
[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.0/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.0/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2 and /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.0/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2
[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.0/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.0/conv/Conv
[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.0/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.0/conv/Conv
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.1/conv/Conv
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.2/conv/Conv
[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2 and /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.1/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2
[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1
[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1
[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1
[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))
[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))
[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))
[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))
[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_1/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_2/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_3/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.0/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.1/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.0/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Running: ConstWeightsFusion on model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.0/conv/Conv
[X] Running: ConstWeightsFusion on model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.1/conv/Conv
[X] Running: ConstWeightsFusion on model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.2/conv/Conv
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv with /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] After dupe layer removal: 86 layers
[X] After final dead-layer removal: 86 layers
[X] After tensor merging: 86 layers
[X] QDQ graph optimizer quantization epilogue pass
[X] QDQ optimization pass
[X] QDQ graph optimizer constant fold dangling QDQ pass
[X] Running: QDQToCopy on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Swap the layer type of /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] Swap the layer type of /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 from QUANTIZE to kQDQ
[X] After dupe layer removal: 86 layers
[X] After final dead-layer removal: 86 layers
[X] After tensor merging: 86 layers
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] Modifying configuration of model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Modifying configuration of model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Modifying configuration of model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] After vertical fusions: 86 layers
[X] After dupe layer removal: 86 layers
[X] After final dead-layer removal: 86 layers
[X] After tensor merging: 86 layers
[X] After slice removal: 86 layers
[X] Eliminating concatenation /model/encoder/Concat_5
[X] Retargeting /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Retargeting /model/encoder/Concat_5_/model/encoder/lateral_convs.0/act/Mul_output_0_clone_1 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Eliminating concatenation /model/encoder/Concat_4
[X] Retargeting /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Generating copy for /model/encoder/Resize_1_output_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.
[X] Eliminating concatenation /model/encoder/Concat_3
[X] Generating copy for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.
[X] Retargeting /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Eliminating concatenation /model/encoder/Concat_2
[X] Generating copy for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.
[X] Retargeting /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] After concat removal: 85 layers
[X] Trying to split Reshape and strided tensor
[X] Graph optimization time: 0.064138 seconds.
[X] Building graph using backend strategy 2
[V] Local timing cache in use. Profiling results in this builder pass will not be stored.
[X] Constructing optimization profile number 0 [1/1].
[X] Applying generic optimizations to the graph for inference.
[X] Reserving memory for host IO tensors. Host: 0 bytes
[X] =============== Computing costs for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(3276800,102400,320,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_chw_int8int8_tilesize16x16_k32_fltsteps1_threadspercta256_r3s3_u2v2_scalebias_relu Tactic: 0x11764d94950382f8 Time: 0.00592693
[X] Tactic Name: ampere_first_layer_filter3x3_imma_fwd Tactic: 0x9ae0c0d2fb3a01e5 Time: 0.00644431
[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00653953 seconds. Fastest Tactic: 0x11764d94950382f8 Time: 0.00592693
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x11764d94950382f8
[X] *************** Autotuning format combination: Int8(409600,409600:4,640,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize16x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x3d988d07a78b0918 Time: 0.00502384
[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x5cc792a989a1d1a6 Time: 0.00494369
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00972891
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00964388
[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0341785 seconds. Fastest Tactic: 0x5cc792a989a1d1a6 Time: 0.00494369
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5cc792a989a1d1a6
[X] *************** Autotuning format combination: Int8(409600,1:16,640,1) -> Int8(204800,1:16,640,2) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0392083
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0178914
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0145266
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0149519
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0301173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0253722
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0374364
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0140236
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.044696
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0265444
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0290382
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0160117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0298409
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0158857
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0305716
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0310545
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0337429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0405298
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0112281
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0150748
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0141036
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0110538
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0434293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0141409
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0188409
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0119851
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0171835
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0233643
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0368843
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0306095
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0302204
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0307947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0453267
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0261867
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0144538
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0139107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0184651
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0111783
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0431373
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0141182
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0141831
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.044928
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0434013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0171595
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0110631
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0160813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0111964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0148313
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0314541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0155064
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0180716
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0195402
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0149148
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.012144
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.045696
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0144347
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0155985
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0143298
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0150122
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0263171
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0315025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0441013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0143396
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0317993
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0413796
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0157503
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0156019
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0117035
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0356939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0329202
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0342475
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0131015
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0357525
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0309392
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0446187
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.012291
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0323355
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0146945
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0101702
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0162931
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0368821
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0179733
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0121954
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0123547
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0365376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0422747
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0153469
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0278871
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0153775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0170384
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0168379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0236594
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0471427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0244625
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0108852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0173216
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0274503
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0161219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.049728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0110806
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0250255
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0391064
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.00982745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0146866
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0138057
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0246027
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0159482
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0182793
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0405796
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0150201
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0287671
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.0284293
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0288996
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0137007
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0131504
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0109932
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0143347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.0391822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0180789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0131528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0127431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0280498
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.011141
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0220193
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0164719
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0150683
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0146662
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0138428
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0254766
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0165491
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.00987231
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0494446
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0134677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0283378
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0401102
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0282569
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0163373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0368011
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0280684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0486309
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0141902
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0118687
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.010663
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0166405
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0177207
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.012848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0138244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0111271
[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.339591 seconds. Fastest Tactic: 0xd805711548cbfb38 Time: 0.00982745
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd805711548cbfb38
[X] *************** Autotuning format combination: Int8(409600,409600:32,640,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0264271
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0196687
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0340448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0355659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0165237
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0195046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0301084
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0175077
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0261629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0203395
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0293218
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0292427
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.016547
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0244968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0272656
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0298427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0274338
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0144298
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0246667
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0163759
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0198136
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.015744
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0151281
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.021276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.027762
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0431253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0296293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0184079
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0194554
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0144937
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0232043
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0310798
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0120019
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0450133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0160757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0270999
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0172421
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0483794
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0202673
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0305668
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0151555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0286391
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0259348
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.027616
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0299769
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.01228
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0205064
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0355339
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0253448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0217313
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0272976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0162824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0197496
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0195147
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0381084
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0436027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.016096
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.029448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0266757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0194975
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0159446
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0492556
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0170704
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0125286
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0129612
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0186039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0144009
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0134677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0439747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0260464
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.031423
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0199209
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0302844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0195656
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.02704
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0190999
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0448093
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.021792
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0149496
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0269875
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0155219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0169627
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0269227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0343403
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0189582
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0263598
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0295076
[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.196195 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.0120019
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73
[X] =============== Computing costs for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(3276800,102400,320,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0205409
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0201117
[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00567501 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0201117
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(204800,1:16,640,2) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0349365
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0217587
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0166672
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0167168
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0402299
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0323423
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0509074
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0153168
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.039872
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0239764
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0374554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0210347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0410347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0197245
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0272796
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0420267
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.030288
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0360309
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0132279
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0163972
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0139267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0127743
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0384581
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0183079
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0258823
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0129514
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0227036
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.029832
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.050397
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0272771
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0403413
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.042684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0404646
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0238171
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0151239
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0136597
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0209593
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0119874
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.03824
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0137254
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0145929
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0391704
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0383858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0199228
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0127317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0188764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0133124
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0168421
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0432027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.016776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0258576
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0258429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0160147
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0137028
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0636782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.017778
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0173056
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0174112
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0163606
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0338645
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0470667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0600622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.014196
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.041248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0374222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0164805
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0193642
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0140351
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0505646
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0289431
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0305552
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0145322
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0508587
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0274634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.039027
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0139462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.028592
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.016688
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.011708
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.025555
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0369888
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0262769
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.015247
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0151699
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0365301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0641636
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0228473
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0450907
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0189001
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0259307
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0225131
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0318555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0730944
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0246667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0132779
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0263089
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0448773
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0204555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0756949
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0133922
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0344117
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0381286
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0121486
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0189902
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0177948
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0365771
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.021556
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0265698
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0390981
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0178756
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0462467
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.0459693
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0478507
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0185612
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0155307
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0136102
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.020966
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.0597262
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0262146
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0160274
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0153581
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0409837
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0140911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.031903
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0207235
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0199285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0184651
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.016768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0256689
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0254065
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.0120373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0745429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0164836
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.043432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.038611
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0273969
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0259298
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0367712
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0432747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0739733
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0159624
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0144231
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0143036
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0220367
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0264738
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0163586
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0166757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0133517
[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.354607 seconds. Fastest Tactic: 0x575184c61f040550 Time: 0.011708
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x575184c61f040550
[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xde3a6a3727f31f34 Time: 0.941056
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0257723
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0189339
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbbff0ceb48c87bac Time: 0.902485
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x9c9fd7d74c020c9d Time: 0.488139
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0338976
[X] Fast skip Tactic:0xb97409e537081e4c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb97409e537081e4c Time: 3.56659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0346581
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe5f40c565f9c8a09 Time: 0.0473413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0137591
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x4e679e1c8dcfbe3c Time: 0.939349
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0172416
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0280782
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x645c57c8d2bdcafa Time: 0.0524678
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x13cb22041bcdf2f5 Time: 0.856747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0162946
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0256755
[X] Fast skip Tactic:0xa1513318dd2f5314 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa1513318dd2f5314 Time: 2.35213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.019872
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0287964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.027214
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0154933
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0237397
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2d0a836ca3b48b55 Time: 0.619861
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x307c1c762709b00e Time: 0.830805
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0265789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.027767
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9826a9122a4e1bac Time: 0.406485
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0265091
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0127218
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.022597
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0158492
[X] Fast skip Tactic:0x9c391ea4731a473c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9c391ea4731a473c Time: 1.86163
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe6fb49f176c8ac20 Time: 0.240155
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0195561
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x136deb7724d5b954 Time: 0.993963
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x718a86dfcb201f10 Time: 0.209088
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0147344
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x415d0d459f475d7a Time: 0.0701845
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0144253
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x7f9cac2d273e24da Time: 0.0398993
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x730183d1b4e07af0 Time: 0.0600782
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.020972
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xab1e201ca705d0dd Time: 0.624555
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x2c80f3b4623a1878 Time: 0.212283
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4dc022b90c990350 Time: 0.0373464
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0272574
[X] Fast skip Tactic:0x8286a69028b3e3f0 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x8286a69028b3e3f0 Time: 4.05811
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0427853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0290276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0174619
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0186056
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0138308
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4480741a5fe7a6b0 Time: 0.0308451
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x8c7efb20a3cfa7ec Time: 0.578219
[X] Fast skip Tactic:0xb7622317db162586 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb7622317db162586 Time: 1.07315
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0225529
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0285609
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00999435
[X] Fast skip Tactic:0x89a3827f636f5c26 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x89a3827f636f5c26 Time: 1.03936
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xdb35ad3ee7e5f3a9 Time: 0.0632533
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xfe34f7b3aa1f6429 Time: 0.0412919
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0444827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0130252
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0266995
[X] Fast skip Tactic:0x3836d6c48cd9dbee which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3836d6c48cd9dbee Time: 1.2032
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x0f57663c97a6a89c Time: 0.475477
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5801f8b0fa1b5d5c Time: 0.803499
[X] Fast skip Tactic:0x48fd1cd53c4157a8 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x48fd1cd53c4157a8 Time: 4.65101
[X] Fast skip Tactic:0x31768067dfa77e0e which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x31768067dfa77e0e Time: 1.1176
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x0a6a5850a77efc64 Time: 0.954709
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0150776
[X] Fast skip Tactic:0xe47e7c8e9e121924 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xe47e7c8e9e121924 Time: 3.69357
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x09cde4f526284108 Time: 0.110384
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x244ad5cff0ca2eb5 Time: 0.502923
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x853ead83f0b1020c Time: 0.764587
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0479909
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0191597
[X] Fast skip Tactic:0x3620fc3660c7e024 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3620fc3660c7e024 Time: 1.90374
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xaca2d8f22e95cba6 Time: 0.584587
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0283182
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x23f62d21795a35ce Time: 0.838912
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0133145
[X] Fast skip Tactic:0x6b2a895dc9dde74c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x6b2a895dc9dde74c Time: 2.04902
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0267553
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5600d95cf91aed70 Time: 0.0370394
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x032a0ef3f4005984 Time: 0.772779
[X] Fast skip Tactic:0xdf8cd3fb81a9e498 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xdf8cd3fb81a9e498 Time: 3.93504
[X] Fast skip Tactic:0x2e05c6cb8ae0ad7c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2e05c6cb8ae0ad7c Time: 4.19021
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0254149
[X] Fast skip Tactic:0xd916513230270d0c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xd916513230270d0c Time: 1.14688
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0260299
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0294702
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0111858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.019707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0350699
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0242164
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0208044
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0265789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.013501
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xecb45af50ce22fe9 Time: 0.0350549
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0195028
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0192006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0372919
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.043136
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0155855
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0274954
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0260037
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0191959
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0149222
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0488594
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0164912
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.01049
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0118532
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0171232
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0127795
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.011989
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.043664
[X] Fast skip Tactic:0x5642a4e167e8f364 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x5642a4e167e8f364 Time: 2.09818
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0252465
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0307345
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0184747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0280622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0175512
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.026336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0171424
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9efe56660532ec2c Time: 0.474923
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0443627
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0214847
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9ebc2bdb9bc0f238 Time: 0.123335
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0138176
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa25e76bff47b753d Time: 0.772075
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0263516
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x11aaa3b552fd1244 Time: 0.687445
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xb99e8d5a01f89b1d Time: 0.557397
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbe2275b488688066 Time: 0.869717
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x34abf9381f0785c4 Time: 0.526677
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.014503
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0162905
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xc52cdc7983541cc4 Time: 0.419157
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x06f777ac34a0a24e Time: 0.63488
[X] Fast skip Tactic:0xc1336bcfda004054 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xc1336bcfda004054 Time: 1.73261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0261087
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x36ca788956376575 Time: 0.447851
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0339093
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0171397
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x76dcfa8e7440813a Time: 0.0381262
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x54c7919e8f324660 Time: 0.11052
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0260168
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0275175
[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.592625 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.00999435
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73
[X] =============== Computing costs for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(6553600,102400,320,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(204800,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0323103
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0318895
[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00548051 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0318895
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(409600,1:16,1280,4) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0360203
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0383301
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0293458
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0356245
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0679616
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0362795
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0941493
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.028032
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0451653
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0258732
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.072288
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0353152
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0719061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.034512
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0310933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0744043
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0326739
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0370176
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0233792
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0341717
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0276431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0205358
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0448427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0315976
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0476968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0236978
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0383656
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0362944
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0928053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0305125
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0679936
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0731947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.045324
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.025783
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0311651
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0293716
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0402939
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.021832
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.044604
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.032385
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0280836
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0456533
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.044776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0262827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0196737
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.022134
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0242796
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0192279
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0742059
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0284844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.047276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0468573
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0338581
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0251596
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0698155
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0311476
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0307394
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0331976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0219327
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.038765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0860107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.108355
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0313202
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0707157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0377019
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0348661
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0331093
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0240427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0940453
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0320291
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0331084
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.026679
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0946347
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0309246
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0454
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.025635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0315568
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0189594
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0232576
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0462867
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0373452
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0476312
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0272057
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0272796
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0372053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0766059
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0375016
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0831936
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0376936
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0481829
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0414981
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.047304
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0786709
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.025952
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.023907
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0480152
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0822229
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0368384
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0807467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0247611
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0472747
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.04452
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0217753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0235456
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0338048
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.068608
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0367296
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.048864
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.045452
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0338261
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0851787
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.084224
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0876373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0356459
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.034528
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0237788
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0362571
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.107349
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0485242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0287538
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0279422
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0701099
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0257961
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.037056
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0346293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0232178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0269932
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0264181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0267643
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0467333
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.0207655
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0876427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0230087
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0733845
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0452173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0306356
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0476251
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0370916
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0745856
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0813056
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0328989
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0271163
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0251261
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0387437
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0478491
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0294151
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0314628
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0256517
[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.372635 seconds. Fastest Tactic: 0xbe784bf72795274c Time: 0.0189594
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbe784bf72795274c
[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(204800,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0282782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0194376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0342091
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.034848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0167781
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.020962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0342677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.016641
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0139382
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0256206
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.020091
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.012595
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.028872
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0362837
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0162184
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0242354
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0270236
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.034336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0266773
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0134639
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0269054
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0166146
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.020138
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0155733
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0150488
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.021208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0301022
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0428867
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0129953
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0299244
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.0194613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0183298
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0187419
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0143889
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0230763
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0356427
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.015439
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.044636
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0183113
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0268595
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0176775
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.047997
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0211207
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0350635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0148707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0269038
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.018973
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0255444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0261744
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.02952
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0183747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0221553
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0351915
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0275717
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0233884
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0269374
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0190827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0201782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0196304
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0374092
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0432253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0162616
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0368736
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.026185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0195816
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0151743
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0489859
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0172624
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0162337
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0191834
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.017559
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0203388
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0192338
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.043728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0256837
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0314948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0216087
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0344736
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.028984
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0263278
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0281049
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0444747
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0216987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0222955
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0134212
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0263877
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.01481
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0171264
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0263335
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0339605
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0247177
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0111545
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0264566
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0363765
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0133247
[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.213419 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.0111545
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d
[X] =============== Computing costs for /model/backbone/MaxPool
[X] *************** Autotuning format combination: Int8(1638400,102400:4,320,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0xfa45342d0e1d409a Time: 0.0115012
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0xa88280db27a5d09f Time: 0.0100442
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0x3acbbd865df539ed Time: 0.0103538
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0x3d35b618fd3968f1 Time: 0.0101776
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0xbeae815d02985cde Time: 0.0126092
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0xe09c44661dba1b5c Time: 0.00891593
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0xb331e89337ca2bad Time: 0.0100725
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0xc4335d27b08156bf Time: 0.0123524
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0xbadabf84bb0f736c Time: 0.00825392
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x80d8e857bc044afb Time: 0.0123257
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0x199aaf5950022512 Time: 0.0127313
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x67734dfa5b8c00c1 Time: 0.00773527
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0xf307ae442c39b4a3 Time: 0.0137737
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0x2eae5c3accbac70e Time: 0.0127198
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x9fe4504b077a26fb Time: 0.0113124
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0x63077323e21b2f73 Time: 0.0117381
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x9dff93820f6f4021 Time: 0.0102591
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x923de46f0f4ddb62 Time: 0.0111085
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0xc9170fb073b2e283 Time: 0.00939022
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0xd3ce7ffb6015b945 Time: 0.0117834
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x072517eca2b23ed1 Time: 0.0118591
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0x1340f65033fdc032 Time: 0.00884632
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0x47a86a624f206290 Time: 0.00976579
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x9a01981cafa3113d Time: 0.00970057
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x69dd2a2a81e4ca53 Time: 0.00878821
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x4a8c38f58c13d6ac Time: 0.011955
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0x5d711a295c873956 Time: 0.0121505
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0x1dee9180e9950aa0 Time: 0.008344
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x899a723e9e20bec2 Time: 0.0133632
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0xedb816f1de89af60 Time: 0.0159614
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0xa01139e8f028471d Time: 0.0171392
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x898e8c271f222050 Time: 0.0143213
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0xc04763fe0916790d Time: 0.00923128
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0x6e2321b421289b4f Time: 0.0121238
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0x27ecc653ee9e3337 Time: 0.0125195
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x9351f452d5078ab3 Time: 0.00915632
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0xbee85cb73ffdd634 Time: 0.0111474
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x0165782a59f89027 Time: 0.00672832
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0xcee9042ed37eb39f Time: 0.0099931
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0xb474d8546167b9fe Time: 0.00909521
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0x74fa51ff328fc089 Time: 0.0153978
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0x543380407ea3cd6f Time: 0.0140031
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0x3465da56879df37f Time: 0.00906696
[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kMAX Tactic: 0x1f6c40e3e09ec730 Time: 0.0061281
[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.0897924 seconds. Fastest Tactic: 0x1f6c40e3e09ec730 Time: 0.0061281
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x1f6c40e3e09ec730
[X] *************** Autotuning format combination: Int8(204800,102400:32,320,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])
[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX Tactic: 0x94215b398b8eb3ba Time: 0.00648328
[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.00243485 seconds. Fastest Tactic: 0x94215b398b8eb3ba Time: 0.00648328
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x94215b398b8eb3ba
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0207969
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0204248
[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00547839 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0204248
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0129301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0146806
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0100355
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0102006
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0118573
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0131586
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0131713
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.010754
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0148601
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.00996925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0128779
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0140476
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0131651
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0114663
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0120705
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0121486
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0102381
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0138313
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.011801
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0118433
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0115906
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0088699
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0130199
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0138078
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00909578
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0135821
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0111115
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0114972
[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0698098 seconds. Fastest Tactic: 0x5f5aa01645d48746 Time: 0.0088699
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5f5aa01645d48746
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0151272
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00945303
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0153615
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0182018
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00950874
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0108249
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0157789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00786307
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00820215
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0160452
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00954026
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0074221
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0125756
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0165277
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0088247
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0127032
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0128484
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0157978
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0117392
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0095811
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.013542
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0107823
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0124772
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00848747
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00891537
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.012331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0156281
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0200797
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00679424
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.01392
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.0105813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00942429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0082867
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00887972
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0124448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0163383
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00932029
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.023178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0100913
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.012768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00986541
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0219127
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0107337
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0161605
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0100031
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0117032
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00961128
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0159685
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0113849
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0126044
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0111314
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0125262
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0165252
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0138598
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0131208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0151714
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0105323
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0124488
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0121707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0192847
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0225159
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00972221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0167317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.016285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0122937
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00920303
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0222251
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0109698
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0096128
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0118279
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00793402
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00928385
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00887383
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0201562
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0125713
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0142356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0111698
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0159863
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0132771
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0162799
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0129329
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0204499
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0125981
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0103832
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00701355
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0115355
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00897039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00994071
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0114524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0160848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0123002
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00670208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.014861
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0168027
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00703578
[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.253239 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.00670208
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0217927
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0207768
[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00560957 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0207768
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0103457
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0168501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0118566
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0151976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0122979
[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0123896 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.0103457
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0175136
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0133243
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0238819
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0160549
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.016752
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.013924
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0167733
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0136516
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.014356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.016386
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0156916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.016639
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0137933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0172107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0134711
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0133739
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0125614
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0170597
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0134729
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0155486
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0135642
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0173227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0241029
[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0572727 seconds. Fastest Tactic: 0xb936321f82fd390c Time: 0.0125614
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb936321f82fd390c
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00824716
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00819954
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0083752
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.008516
[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0121312 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00819954
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.0112601
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0103483
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0123425
[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0075262 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.0103483
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00957287
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00781395
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00799289
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00901391
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00820163
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00893137
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0119893
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00864137
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00996486
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00950933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00950874
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00919985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00798095
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00827525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00895242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0106117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0110582
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00804902
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00847707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00821646
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00877221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0103829
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00976701
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00908396
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00806781
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00826199
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00786878
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0085056
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0108139
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00970362
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00901564
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00879804
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0108917
[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0903429 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00781395
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x0f47434ace2a7d18, 0.0204248 ms
[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4)] got cached result: CaskConvolution, tactic 0x5f5aa01645d48746, 0.0088699 ms
[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x9dafb2758560cc1d, 0.00670208 ms
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0225884
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.021532
[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00549567 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.021532
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0124393
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0214433
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0141182
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.018034
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0144569
[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0120918 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.0124393
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0149046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0146676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0221227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0175978
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.017682
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0113383
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.014995
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0148452
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0119242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.01704
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0170373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0173173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0110593
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0153299
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0116601
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0133717
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0136235
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0179222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0134643
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0131298
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0119451
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0147659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0225756
[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0575432 seconds. Fastest Tactic: 0x0e07dc8353bf7e9f Time: 0.0110593
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0e07dc8353bf7e9f
[X] =============== Computing costs for /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(102400,6400:4,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00314617
[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.0040293 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00314617
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(12800,6400:32,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00301357
[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00399706 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00301357
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0160147
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0153086
[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00570151 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0153086
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.00739803
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00664721
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00866215
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00600667
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.00726701
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00748658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00730829
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00624514
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0065851
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.00921369
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00723472
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00653658
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00724153
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0110937
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00696
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00751004
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00621807
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00768994
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00684647
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0114887
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0112764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00757452
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00715688
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00652923
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00800026
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00757499
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00647938
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.011103
[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0773116 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00600667
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00749653
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0060461
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00632554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.00816234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00659911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0066079
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0091569
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00757818
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.00938785
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.006036
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0120945
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00969326
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00712124
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00959695
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00766764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00927941
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0114948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0062643
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00755793
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.00793854
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00658719
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00673237
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00692158
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00749013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00763346
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0112153
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00636619
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00577747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00826224
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00677717
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.00914162
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00963322
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00764436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00957287
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00827682
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00751668
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00676245
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0121204
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00686149
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00967223
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00660162
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0118584
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.00917333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0114432
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0122907
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00812318
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00763273
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00939022
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00765479
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00799518
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00699711
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0084376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.009376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00635049
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00829789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.00923329
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00628931
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0100533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00923877
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.00929333
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00872315
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0122629
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00822764
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00792682
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00837813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00779932
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00680729
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00660999
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0112839
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.00734956
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00628405
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00698444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00932148
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0087144
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00946963
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00817249
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0113404
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00756978
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00693377
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0113628
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00843893
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00662985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0113355
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.00915546
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00820371
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00677376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0106717
[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.234598 seconds. Fastest Tactic: 0x705baf38e41eee0b Time: 0.00577747
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x705baf38e41eee0b
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0280116
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0217687
[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00540639 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0217687
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00896814
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.013542
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00899593
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0114844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00922551
[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.01325 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.00896814
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.021038
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0116292
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0202698
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.010273
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0127897
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0137109
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0131906
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0110132
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.014982
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0126017
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0102526
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0128398
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0134831
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0133952
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00970636
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00960457
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0102219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0131253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0097152
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0116785
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00985819
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0207818
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0205453
[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0539636 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00960457
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] *************** Autotuning format combination: Int8(102400,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00707822
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00705067
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00729113
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.007096
[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0116908 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00705067
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00688261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00665872
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00809499
[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00780645 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00665872
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00943259
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00541093
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00606623
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00604686
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00621827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0087739
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00631749
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00673301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00659493
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00618746
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00616514
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00620326
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0055568
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00555051
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00688414
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00604934
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00612635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00554999
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.005992
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00548721
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0085801
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00668501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00647898
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00614478
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00548424
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00561493
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00544482
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00595524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00652616
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00620148
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00597143
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00593104
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00708511
[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0865782 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00541093
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0294231
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0267003
[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00538673 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0267003
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0106947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00878288
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0132431
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00817119
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.010608
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0107802
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0102549
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00845653
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00870428
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.014336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0101569
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0086865
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0101159
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0177331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00926163
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0108198
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00835787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0105993
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00902458
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0183001
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0180525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0109502
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0102529
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00870209
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0117628
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.010504
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00864957
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0177145
[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0708868 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00817119
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0103425
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0074073
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00807289
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.011684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00896253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00823649
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0124346
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0105753
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00677632
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0140662
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00775418
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00635975
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.019011
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0131729
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00962133
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0137788
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0108105
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0124409
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0178285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00874421
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0103984
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0113902
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00899004
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00924079
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00928652
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.010561
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.00851493
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0107067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0176196
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00957287
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00772703
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00746572
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00702311
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0162438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0121269
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00917765
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0132341
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0130039
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0101434
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.00995482
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00783405
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0144983
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00928178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.010955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00920274
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0191324
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00821906
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.00966278
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0129153
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00911741
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0178768
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00740174
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0139867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0175837
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.019293
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0112124
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0108332
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0142658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.01051
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0110937
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00965547
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00956952
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0143156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00885614
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0121025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0141876
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00873764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0130942
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0140964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0141018
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0124784
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0193416
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0115752
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0104663
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0114212
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0108501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00763418
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00718593
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0178234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.010539
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00793278
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00842693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0127747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.010508
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0142747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0105697
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0179672
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0107247
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00870975
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00943881
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0179149
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0119722
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00901449
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0177224
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0139898
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0100587
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00946311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00937185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0139413
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00947081
[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.268179 seconds. Fastest Tactic: 0x214f03e23f252333 Time: 0.00635975
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x214f03e23f252333
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0286596
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.022496
[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00526231 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.022496
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00987388
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0164058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00964084
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0129555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0101107
[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0119964 seconds. Fastest Tactic: 0xa8b56a226b057463 Time: 0.00964084
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa8b56a226b057463
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.021616
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.011552
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.019261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00983467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0132316
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0128158
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.012482
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0112466
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0141356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0125242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00968746
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0128976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.012595
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0126653
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00874667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00971459
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0105057
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0136939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00982494
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0104145
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00914249
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0213233
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0196568
[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0552275 seconds. Fastest Tactic: 0xad886d4d69834922 Time: 0.00874667
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xad886d4d69834922
[X] =============== Computing costs for /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,1600:4,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.0028203
[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00310891 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.0028203
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00276196
[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00333674 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00276196
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0274379
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0257936
[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00540221 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0257936
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.010534
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00709288
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.013081
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00682209
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0104397
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0106027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00662191
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00686389
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00692049
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0141058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00654034
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00691788
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00647631
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0176629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00761091
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0105873
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00700644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00690481
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00749914
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0181749
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0181069
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0108095
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00638471
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00702778
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0116278
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00684103
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.007084
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0175686
[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0710598 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00638471
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00861375
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00719455
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0075072
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0114727
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00616631
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00578115
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00743609
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0107747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0141071
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00776384
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0190791
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00813815
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0097091
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0137152
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0106343
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00756314
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0180867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00863316
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00674837
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.00845467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00871439
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00918774
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0083704
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00976213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00877193
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0179076
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00689045
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00699222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0121516
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00829398
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0132459
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00759418
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0085448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0144271
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00676693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.010988
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00635975
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0191976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00602915
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00747425
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00893839
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0184146
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0141098
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0178936
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0193861
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00911221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00685605
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0142422
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00697289
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00712964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00881544
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00682101
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00870236
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00855904
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0122316
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0141569
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00852107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00824377
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0142342
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.00863426
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0128036
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0193677
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00869251
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00881825
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00921715
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0110703
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0079426
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00755792
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0178554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0104777
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00700311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00606255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00753114
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00715279
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0143169
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00694889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0181243
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00997365
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00917766
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0182102
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.012336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00882358
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0179408
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0141369
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00685518
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00867173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0103001
[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.236024 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.00578115
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0498347
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0378216
[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00516609 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0378216
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0115189
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0199605
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00933807
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00919553
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0119375
[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0119544 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00919553
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0343157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.010456
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0338571
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0114674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0110727
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0188812
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0184337
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00994165
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0214387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0107864
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0113977
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0109344
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.018525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0186684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0113931
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00858557
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00865614
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0113234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00957348
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0150303
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0116101
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0340117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0341557
[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.055095 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00858557
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00812165
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00770667
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0083187
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00781966
[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0114884 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00770667
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00652595
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00543054
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00551921
[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00821713 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00543054
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0103554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00414933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00606095
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00471466
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00627931
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00962224
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00528633
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00701622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00943229
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00439859
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00445212
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00477851
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0041968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00491325
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0071419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00479695
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00492016
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00419027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00445355
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00432574
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00948918
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00961737
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00645108
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0061314
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00476861
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00420946
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00426815
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00476968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00492376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00602438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00542761
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.0059447
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00980023
[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0909718 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00414933
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0536274
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0484739
[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00533997 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0484739
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0170565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0103871
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0222457
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0102778
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0169216
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0171397
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00962712
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0102617
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0103515
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0243238
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00955733
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0103118
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00955916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0308994
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0109681
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0170976
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0103994
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00984816
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0108625
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0315724
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0316945
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0175876
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00945067
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0103163
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0191135
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00983121
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0104837
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0306385
[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.065799 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00945067
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0135185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0104397
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0112828
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0189013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00847067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00731687
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0103551
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0171648
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.009336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0238994
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0116881
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00933363
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0330434
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0110087
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0146667
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0221153
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0172635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0103525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0311088
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0136418
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00938044
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0131475
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0136213
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0141547
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0127151
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0156126
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0101776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0136738
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0308732
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0159517
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0101311
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00741073
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0102743
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0292453
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0198833
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0126823
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0215027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.010581
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0127506
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0161666
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00969051
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0242149
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00912144
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0178296
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00862386
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0332751
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00843387
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.0160686
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.010459
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0138782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0311505
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00822634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.023923
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0308238
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.033376
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0131971
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0101036
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0241166
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00957135
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0103887
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0137323
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00915402
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.013725
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0135543
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0200151
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0240038
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0135326
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0111524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.024016
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0135723
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0202541
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0333472
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0133948
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0129817
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0135219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0173099
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0108325
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0104126
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0309207
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0169749
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0103745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00830986
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.010539
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00877019
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0241074
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00917564
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0315268
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0156305
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0137126
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0159904
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0315646
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0194376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.013693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0308722
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.023917
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00793829
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0159126
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136115
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.014899
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0159853
[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.234503 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.00731687
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0505646
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0385849
[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0054761 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0385849
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0125345
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0227748
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0101905
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0101065
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0129641
[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0129335 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0101065
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0348352
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00999404
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0344523
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0115575
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111516
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0190151
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0183753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00929629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0214867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0108157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0113148
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0108924
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0186904
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0187247
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0112437
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00819174
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00840027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.011402
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00926903
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0145586
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0115785
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0346293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0346112
[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0562448 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00819174
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9
[X] =============== Computing costs for /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,400:4,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00264914
[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00313766 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00264914
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00282775
[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00283857 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00282775
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0516663
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0477836
[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00522979 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0477836
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0170565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0102552
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0222193
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0100088
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0170672
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0171104
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0078725
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0101896
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0102946
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0242042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0079426
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0100875
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00782412
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0309789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0108845
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.017152
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0103202
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00838853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0107929
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0318652
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.031649
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0175798
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00783529
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0102116
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0190678
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0082841
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0104243
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0306647
[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0691395 seconds. Fastest Tactic: 0x08af511817b7463e Time: 0.00782412
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x08af511817b7463e
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0133798
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.010434
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0112565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0188847
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00871138
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00737437
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00715597
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0173163
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0240312
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0119253
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0330987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00800025
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.015104
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0228366
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0174912
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00724985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0312776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.013626
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00918112
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0120175
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0137792
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.014496
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0128377
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0158415
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0134558
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0310322
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0103234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0102109
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0200778
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0128439
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.02203
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00853173
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.012336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0243985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00917823
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0181345
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00866161
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0333835
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0084928
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00918746
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0139649
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0315481
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.024112
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0311379
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0334379
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0115737
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0100718
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0242697
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0093034
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0102358
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0137732
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00902775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0136141
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0134541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0201876
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0241105
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0134306
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00819252
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0241455
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0134844
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.021064
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0333728
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0121589
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0127032
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.011822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.01755
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0114738
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0108253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0310817
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0171237
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0104847
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00854427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00735582
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00855138
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0242278
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00893839
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0317285
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0157086
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0141982
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0317644
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0203344
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0136738
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0310594
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0241097
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.007456
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136631
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00819304
[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.205205 seconds. Fastest Tactic: 0x322f337abc345152 Time: 0.00715597
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x322f337abc345152
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0930987
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.070176
[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.005277 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.070176
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.018121
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0333062
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0141187
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0127024
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0187277
[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0117554 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0127024
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0608107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0147631
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0603822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0175753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0110517
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0320834
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0315578
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0133198
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0371141
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.010282
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0174272
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0104613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0318255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0319564
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0174208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0113668
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0113344
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0111868
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0137203
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0242499
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0176646
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0604142
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.06072
[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0516358 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.010282
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] *************** Autotuning format combination: Int8(25600,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0104593
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00927259
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0107685
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00973166
[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0111859 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00927259
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00704244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00539785
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00535399
[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00854329 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00535399
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0113447
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0038942
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00692462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00454227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00716686
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0112302
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0053738
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00719977
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0109258
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00398552
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00406777
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00454212
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00394039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00526683
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00742637
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00481112
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00495043
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00383431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00410341
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00415144
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.010997
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0111612
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00677717
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00678037
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00512291
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0039045
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.0040666
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00442484
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00472954
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00583963
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00601791
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00661438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.011179
[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0934164 seconds. Fastest Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00383431
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65fbe45b4cb1d8a5
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(204800,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.106232
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.091664
[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00566135 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.091664
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0302987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0169595
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0405404
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0167296
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0302436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0303583
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.012267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0167856
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0169307
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0443413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0123116
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0166875
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0122926
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0572764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0175275
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0303777
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0169333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0129957
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0174331
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0581316
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.058848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0310448
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0123544
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0168379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0339947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0127905
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0170075
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0569956
[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0631739 seconds. Fastest Tactic: 0x22ebff09f6ab32eb Time: 0.012267
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x22ebff09f6ab32eb
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0232939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0169637
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.018835
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0337237
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0135356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0108979
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.010805
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0303855
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.015841
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.043804
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0196624
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0157847
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0610489
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0109258
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0253143
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0394501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0304718
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0101876
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0576427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0235598
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0141884
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0205804
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.02352
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.024429
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.02154
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0276423
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0165877
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0234859
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0574044
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0292276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0167099
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00978285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.016816
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0556729
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0356704
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0215127
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0386003
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0136119
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0206337
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0294222
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.016034
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0442333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0144862
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0321028
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.013539
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0617671
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0133313
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.0293164
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0151072
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0237156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0575716
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0113465
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0440467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0572658
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0614898
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0188468
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0166568
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.044192
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0143964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0168565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0236544
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0142987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0235442
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0234731
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0360768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0439053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0232846
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0112555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.043984
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0233458
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0361301
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0616676
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0206877
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0210773
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0191058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0307219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0177011
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0170171
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0572871
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0302462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0168144
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0135411
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0103063
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0132771
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0441147
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0142893
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0586613
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0270416
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0237703
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0290462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0587627
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0347285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0235897
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0574578
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.043992
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0113337
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.02912
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0235364
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.012099
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0292213
[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.266416 seconds. Fastest Tactic: 0x1d53511430a5d47e Time: 0.00978285
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1d53511430a5d47e
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0938053
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0709525
[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00526197 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0709525
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0191259
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0361589
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.014982
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0135031
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0198274
[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0113807 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0135031
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.061328
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0151485
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.060928
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0177808
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.011131
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0320659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0315782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0136222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0372326
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0102319
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0175301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0104068
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0318245
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0320058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0176887
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0114503
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0116925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.011317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0139093
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0178594
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0608356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0612338
[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0536227 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.0102319
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb
[X] =============== Computing costs for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146119
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0119208
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0149992
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126242
[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0100844 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0119208
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00636679
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00464059
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00438891
[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00831814 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00438891
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.013853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00367438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00827838
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00454083
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0085168
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.013609
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00565279
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00856151
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0131106
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00425438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0043081
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00454299
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00375
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00579586
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00900435
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00540852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00549386
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.003788
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00404948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.003969
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0133525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0132185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0082828
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00812394
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00568714
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00382788
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.0039071
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00391392
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00463792
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00673984
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00596438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00805384
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0133508
[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0881705 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00367438
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]}
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023])
[V] Compiler backend is used during engine build.
[X]  (foreignNode) Set user's cuda kernel library
[X] Subgraph compilation completed in 3 seconds.
[X] Tactic: 0x0000000000000000 Time: 0.0467627
[X] {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023]) profiling completed in 3.02463 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0467627
[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000
[X] =============== Computing costs for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00851414
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00822452
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00899116
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00852347
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0088567
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0147703 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00822452
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.008508
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00826667
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0090066
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00844747
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00886962
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0147024 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00826667
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(204800,1:16,2560,32) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00591925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00787944
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00550837
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00663906
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00601429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00676352
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0066652
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.0066468
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00739362
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00581039
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00673685
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.0057486
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00682013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00641723
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.006044
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00779907
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00574805
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0055402
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00544155
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00755295
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00650462
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00643282
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00584889
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00598267
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00659681
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00665307
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00594115
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00664387
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0754223 seconds. Fastest Tactic: 0x712e1cc2c7813ee9 Time: 0.00544155
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x712e1cc2c7813ee9
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00557885
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00591757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00782388
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00555244
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00623803
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00664429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00601334
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00676928
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00672938
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00666646
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00740614
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00580193
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0067424
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00570974
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00570504
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00686955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00639477
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00603486
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00556942
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00784719
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00575522
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00553023
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00547008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00756717
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00579384
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.00639457
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00644349
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00650072
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.00577214
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00648041
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00675499
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00740452
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00572185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.00556605
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00597848
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00647241
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00681491
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00672363
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00590802
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0066424
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00770788
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.00645539
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.116942 seconds. Fastest Tactic: 0x712e1cc2c7813ee9 Time: 0.00547008
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x712e1cc2c7813ee9
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.0057139
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00683515
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00666837
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00627615
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.0071562
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.0068049
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00730435
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00556569
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00546512
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00701733
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00818602
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00757191
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00731641
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00596838
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00614846
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00610735
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00553635
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.005287
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00679936
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00550103
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0061186
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00552603
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00582547
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.0068197
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00557618
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00518498
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00487235
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.0072844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00645005
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00557458
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00569329
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00545428
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00696111
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00554859
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.0055019
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00614225
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00541333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.0059069
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00568262
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00670891
[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00532741
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00555891
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00648431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0051945
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00576828
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00586798
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00606313
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00726006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00610793
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00606449
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00583338
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00603924
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00505406
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00728719
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00564462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00565352
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00509188
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00602514
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00638632
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0061184
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00631427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00565243
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00535619
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00675541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00597657
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00575743
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00566219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00657527
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00617561
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00578244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00662609
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00606584
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00603181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00624909
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00498866
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00556231
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0077263
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00548721
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00546478
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00563876
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00749819
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00559716
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00538804
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00563947
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00541196
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00681752
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00594321
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00572583
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00540697
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00581996
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00624534
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.005285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.005222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00698844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00611937
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00575264
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00618054
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00591719
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00630199
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00730017
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.0065319
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00783281
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.0052745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00704867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00685257
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0056192
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00746003
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0061843
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00536787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00551292
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00522933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00683232
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00608912
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00536957
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00532165
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00620385
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0058532
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.0055374
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.0053834
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00598838
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00523183
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00574713
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00631366
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00684081
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.0052275
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00815298
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.352694 seconds. Fastest Tactic: 0x5e4918ccf433630e Time: 0.00487235
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4918ccf433630e
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00523617
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00625343
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00603048
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00567033
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00648595
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00617699
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00669674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00506634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00498483
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00637142
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00737484
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00689306
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00667157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.0054006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00599962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00607321
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00554929
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00531589
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00681077
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00548896
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0061025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00552761
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00583927
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00682971
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00553268
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00525433
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00486833
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00730435
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0064199
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00555244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00569546
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00550033
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00701711
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00552411
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00554912
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00613683
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00538443
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00589081
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00570902
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00670763
[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00538142
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00566292
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.0065439
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0051479
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00569691
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00575117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00607748
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00724403
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00611918
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00604305
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00584
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00604305
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00501843
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00728116
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00564853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00565207
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00508509
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00603276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00643323
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0060541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00634667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00560213
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00536753
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00677781
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00598057
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00576736
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00571336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00658615
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00612752
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.0057749
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00665558
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00604114
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.006052
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00625027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00498388
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00558596
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00776037
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00554754
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00549998
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00567575
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00752758
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00559396
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00536923
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00564972
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00544017
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00686128
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00599829
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.0057327
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00538497
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00584386
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00627773
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00537465
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00518695
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00697933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0060769
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00578042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00624336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00591869
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00627536
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00730991
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00659033
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00784124
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00539492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00705911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00687673
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00562454
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00748634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00624632
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0053528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00551082
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00517169
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00689545
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00612771
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00547322
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00530387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.0061999
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00589886
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00551257
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00539286
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00603372
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00521117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00575081
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.0063199
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00678208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00524117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00818394
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.373682 seconds. Fastest Tactic: 0x5e4918ccf433630e Time: 0.00486833
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4918ccf433630e
[X] =============== Computing costs for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00722201
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00709469
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00902486
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00842027
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00876417
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146039 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00709469
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00722632
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00716709
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00902861
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.0084088
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0087826
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0147119 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00716709
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00581517
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00490357
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0052275
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00431781
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00563005
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00406361
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00427583
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00741238
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00441474
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0041808
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00407519
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00532555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00433025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00726608
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00609687
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00484947
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.0043861
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00528933
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00425613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00429675
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00737646
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00723086
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00538512
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00430236
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00406725
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00411252
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00564089
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00430632
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0803642 seconds. Fastest Tactic: 0x22ebff09f6ab32eb Time: 0.00406361
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x22ebff09f6ab32eb
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.0052993
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00581554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00487374
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.005214
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00719909
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00432519
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00564249
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00408234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0042564
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00738388
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00440477
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00419253
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00407688
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00533181
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00425775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00430605
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00724198
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00609416
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00415947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00482804
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00430509
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00530963
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00421968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00430017
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00588108
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.00723608
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00390475
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00734794
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.00541557
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.007296
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00424749
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00441165
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00537143
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.0052145
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00432178
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00398108
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00406156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00410797
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00562222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00431836
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00484916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.0042402
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.12029 seconds. Fastest Tactic: 0x788dd0382d5ebd44 Time: 0.00390475
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x788dd0382d5ebd44
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00560925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00444182
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00440126
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00466089
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00788515
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00412905
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.0083109
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00534265
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00536279
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00787572
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00434992
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00440786
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00799492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00458711
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.0071562
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00441109
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00635311
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00429962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00459017
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00502129
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00668395
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00457673
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00485905
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00423142
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00467333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00551379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00526383
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00562133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00760145
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00464089
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00477684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00630722
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00528833
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00446194
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00390016
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00411005
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00446691
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00730782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00434937
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00475613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00458009
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00469784
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00589436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00477181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00643015
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00743941
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00457848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00756243
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00428308
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00661396
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00430523
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00540439
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00434327
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00438538
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0040787
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00478339
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00393562
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00445354
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00500521
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00783677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.0062404
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00497974
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00823233
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.0044608
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00451719
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00641046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00438652
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00437541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00403406
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0044014
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00403804
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00490525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00461267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00377301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00476861
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00578244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00625008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00621847
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00637746
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00461194
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00638873
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00621887
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00626272
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.005992
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00822452
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00451301
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.0044773
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00438608
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00469634
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00488286
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00578042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00556676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00486292
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00761382
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00425802
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00441095
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00424574
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00414262
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00454241
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00429908
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00448284
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00491467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.0045874
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00453838
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00652452
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00437139
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00764848
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00497576
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00462276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00449635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00503674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00750364
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00460157
[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00671552
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00576074
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.0073964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00492016
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00608737
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00432533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00751834
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00481767
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00393587
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00457819
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00457468
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00393512
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00539544
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.357796 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00377301
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00565026
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00446293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00437693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00466548
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00791318
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00413959
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00815948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00536652
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00538546
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00791144
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00435505
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00441417
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00808889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00458783
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00717549
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00438234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00633016
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00427504
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00460435
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00502352
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00668522
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00457483
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00486106
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00428882
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00469007
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00553705
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00523067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00560249
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00761406
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00462978
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00481249
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00624988
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00524967
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00447331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00390661
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.0041038
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00443972
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00731154
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00430331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00480686
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00455985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00468889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00587603
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00472744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00642667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00746193
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00456072
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00758351
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00423331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00663404
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00431822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00540439
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00432588
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00441333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0040852
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00479131
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00394918
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00447161
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0050041
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00788961
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00624731
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00501429
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00824845
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00448953
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00450868
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00640785
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00441263
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00435297
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00399327
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00438123
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00403431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00491059
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00459967
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00378667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00474967
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00580009
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00627082
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00621393
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00638611
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00462466
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00637484
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.0061537
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00623013
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00599829
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00821906
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00457951
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00448924
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00441207
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.004684
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00481143
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00575448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00555876
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00486214
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00756764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00424601
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00442316
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00426167
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00413695
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00453391
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00432191
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00450695
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00492423
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00460917
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00453146
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00652677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00431371
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00765333
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00497576
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00463867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00449962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00503116
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00748444
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00458579
[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00668992
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00578317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00737878
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00486369
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00606095
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00429196
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00747093
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00483602
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00394039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00459397
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00457483
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00392684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00536957
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.362407 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00378667
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27
[X] =============== Computing costs for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv
[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00970758
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0086493
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0099567
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00893558
[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0114003 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0086493
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00594283
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00447744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00428964
[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00876667 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00428964
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0109213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00325157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.006885
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0038438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00708378
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0105283
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00487343
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00709447
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0102533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.0037603
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00385042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00389271
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00337379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00490635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00730388
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00463837
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00466533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00341584
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00367906
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.0035419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0103589
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.010481
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00669077
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00675669
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00483169
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00348622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00345006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00349756
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00408845
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00565478
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0049509
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00663404
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.010474
[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0924448 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00325157
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul)
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00244617
[X] Tactic: 0x0000000000000001 Time: 0.00236026
[X] Tactic: 0x0000000000000002 Time: 0.00237647
[X] Tactic: 0x0000000000000003 Time: 0.00248517
[X] Tactic: 0x0000000000000004 Time: 0.00234018
[X] Tactic: 0x0000000000000005 Time: 0.00224386
[X] Tactic: 0x0000000000000006 Time: 0.00305629
[X] Tactic: 0x0000000000000007 Time: 0.0025673
[X] Tactic: 0x0000000000000008 Time: 0.00244913
[X] Tactic: 0x0000000000000009 Time: 0.00236409
[X] Tactic: 0x000000000000001c Time: 0.00244025
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.461111 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00224386
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00242334
[X] Tactic: 0x0000000000000001 Time: 0.00234122
[X] Tactic: 0x0000000000000002 Time: 0.00237443
[X] Tactic: 0x0000000000000003 Time: 0.00248581
[X] Tactic: 0x0000000000000004 Time: 0.00233007
[X] Tactic: 0x0000000000000005 Time: 0.00222997
[X] Tactic: 0x0000000000000006 Time: 0.00299905
[X] Tactic: 0x0000000000000007 Time: 0.00258002
[X] Tactic: 0x0000000000000008 Time: 0.00248059
[X] Tactic: 0x0000000000000009 Time: 0.00237488
[X] Tactic: 0x000000000000001c Time: 0.00244189
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0298111 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00222997
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00245122
[X] Tactic: 0x0000000000000001 Time: 0.00235718
[X] Tactic: 0x0000000000000002 Time: 0.00236475
[X] Tactic: 0x0000000000000003 Time: 0.00251244
[X] Tactic: 0x0000000000000004 Time: 0.00232733
[X] Tactic: 0x0000000000000005 Time: 0.00225737
[X] Tactic: 0x0000000000000006 Time: 0.00306173
[X] Tactic: 0x0000000000000007 Time: 0.00257469
[X] Tactic: 0x0000000000000008 Time: 0.00243239
[X] Tactic: 0x0000000000000009 Time: 0.00236838
[X] Tactic: 0x000000000000001c Time: 0.00243574
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0294882 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00225737
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00224386
[X] Tactic: 0x0000000000000001 Time: 0.00246596
[X] Tactic: 0x0000000000000002 Time: 0.00236064
[X] Tactic: 0x0000000000000003 Time: 0.00280955
[X] Tactic: 0x0000000000000004 Time: 0.00274317
[X] Tactic: 0x0000000000000005 Time: 0.00260101
[X] Tactic: 0x0000000000000006 Time: 0.00348122
[X] Tactic: 0x0000000000000007 Time: 0.00331765
[X] Tactic: 0x0000000000000008 Time: 0.00333611
[X] Tactic: 0x0000000000000009 Time: 0.00309748
[X] Tactic: 0x000000000000000a Time: 0.00247403
[X] Tactic: 0x000000000000000b Time: 0.00247498
[X] Tactic: 0x000000000000000c Time: 0.00232533
[X] Tactic: 0x000000000000000d Time: 0.00271168
[X] Tactic: 0x000000000000000e Time: 0.00247285
[X] Tactic: 0x000000000000000f Time: 0.00236274
[X] Tactic: 0x0000000000000010 Time: 0.00350322
[X] Tactic: 0x0000000000000011 Time: 0.00295495
[X] Tactic: 0x0000000000000012 Time: 0.00273644
[X] Tactic: 0x0000000000000013 Time: 0.00271679
[X] Tactic: 0x0000000000000014 Time: 0.00246447
[X] Tactic: 0x0000000000000015 Time: 0.00243154
[X] Tactic: 0x0000000000000016 Time: 0.00262233
[X] Tactic: 0x0000000000000017 Time: 0.00309136
[X] Tactic: 0x000000000000001c Time: 0.00241932
[X] Tactic: 0x000000000000001d Time: 0.00269806
[X] Tactic: 0x000000000000001e Time: 0.00272252
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 1.32502 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00224386
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000018 Time: 0.00251549
[X] Tactic: 0x0000000000000019 Time: 0.00270262
[X] Tactic: 0x000000000000001a Time: 0.00312214
[X] Tactic: 0x000000000000001b Time: 0.00390946
[X] Tactic: 0x000000000000001f Time: 0.00249059
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.214579 seconds. Fastest Tactic: 0x000000000000001f Time: 0.00249059
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001f
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000001c Time: 0.0038844
[X] Tactic: 0x000000000000001d Time: 0.00387372
[X] Tactic: 0x000000000000001e Time: 0.00409886
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.136591 seconds. Fastest Tactic: 0x000000000000001d Time: 0.00387372
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001d
[X] =============== Computing costs for /model/encoder/Resize
[X] *************** Autotuning format combination: Int8(102400,400,20,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])
[X] Tactic: 0x0000000000000000 Time: 0.00995012
[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.123645 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00995012
[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])
[X] Tactic: 0x0000000000000003 Time: 0.00270409
[X] Tactic: 0x0000000000000005 Time: 0.00269075
[X] Tactic: 0x0000000000000006 Time: 0.00301838
[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.123849 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00269075
[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005
[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.016193
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0131007
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.01664
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0138987
[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0109451 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0131007
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00744344
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0055824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0055416
[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0081828 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.0055416
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0153648
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00508525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0091471
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00512259
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00942103
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0150284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00635794
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0094557
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.012386
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00489925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00501875
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00510998
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.005224
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00633439
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.010026
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0062084
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0062159
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00476647
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00510125
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00472909
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0146231
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0126159
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00914018
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0092085
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00618055
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00486956
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00466785
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00539407
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00544224
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00749061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00661814
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00902659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0128242
[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0867017 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00466785
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e
[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0115285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00483741
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0112956
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00817561
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00488116
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00570016
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00810032
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00518219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00463378
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0115038
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00486369
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00654703
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.0046248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.0056368
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00580432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00491984
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00570558
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00488286
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00451993
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0114571
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0116528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.0076897
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.0060301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00797765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00777228
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00762255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00655812
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00671317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00502193
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00560213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0122842
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00458068
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0061025
[X] model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0977064 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00451993
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3
[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0120949
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00613974
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00772897
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00615661
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.0057852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0197898
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00853867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00780725
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.011804
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00587939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0116337
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0197873
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00785067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0119676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0077263
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00642626
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00592131
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00579899
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0201889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00574247
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0197302
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00793253
[X] model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.056549 seconds. Fastest Tactic: 0x6176c23707257237 Time: 0.00574247
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6176c23707257237
[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6176c23707257237, 0.00574247 ms
[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0273173
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0216127
[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00501705 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0216127
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00817743
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0133585
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00651057
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00625857
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0083528
[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0126704 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00625857
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0209347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.006256
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0188089
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00757001
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00560462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0121166
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0117208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00620188
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0133786
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00543948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00748918
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00548949
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0119528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0119916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00744628
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0056828
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0054505
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0057874
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00629776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00942015
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00760461
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0206695
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0190756
[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0589656 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.00543948
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb
[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))
[X] *************** Autotuning format combination: Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00301434
[X] Tactic: 0x0000000000000001 Time: 0.00273819
[X] Tactic: 0x0000000000000002 Time: 0.00273032
[X] Tactic: 0x0000000000000003 Time: 0.00281994
[X] Tactic: 0x0000000000000004 Time: 0.00273792
[X] Tactic: 0x0000000000000005 Time: 0.00272772
[X] Tactic: 0x0000000000000006 Time: 0.00335872
[X] Tactic: 0x0000000000000007 Time: 0.00309689
[X] Tactic: 0x0000000000000008 Time: 0.00284728
[X] Tactic: 0x0000000000000009 Time: 0.00283245
[X] Tactic: 0x000000000000001c Time: 0.00298314
[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.518538 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00272772
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000000a Time: 0.00360224
[X] Tactic: 0x000000000000000b Time: 0.0041388
[X] Tactic: 0x000000000000000c Time: 0.00550557
[X] Tactic: 0x000000000000000d Time: 0.00549281
[X] Tactic: 0x000000000000000e Time: 0.00668373
[X] Tactic: 0x000000000000000f Time: 0.00976
[X] Tactic: 0x0000000000000010 Time: 0.00862496
[X] Tactic: 0x0000000000000011 Time: 0.0105243
[X] Tactic: 0x0000000000000012 Time: 0.0160081
[X] Tactic: 0x0000000000000013 Time: 0.0195911
[X] Tactic: 0x0000000000000014 Time: 0.0035099
[X] Tactic: 0x0000000000000015 Time: 0.00321477
[X] Tactic: 0x0000000000000016 Time: 0.00462203
[X] Tactic: 0x0000000000000017 Time: 0.00656335
[X] Tactic: 0x0000000000000018 Time: 0.00327415
[X] Tactic: 0x0000000000000019 Time: 0.00346887
[X] Tactic: 0x000000000000001a Time: 0.00365855
[X] Tactic: 0x000000000000001b Time: 0.00448867
[X] Tactic: 0x000000000000001d Time: 0.00600648
[X] Tactic: 0x000000000000001e Time: 0.00402339
[X] Tactic: 0x000000000000001f Time: 0.00337078
[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 1.67829 seconds. Fastest Tactic: 0x0000000000000015 Time: 0.00321477
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000015
[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00670187
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00424452
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00673621
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00553338
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00441838
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00423763
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00533283
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00430167
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00683232
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00426801
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00521583
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00387433
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00424898
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00441712
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00430099
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00438905
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00437472
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.0038844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00675008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00683842
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00512
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00461998
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00537703
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00520041
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00504645
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00441712
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00414248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00413524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.0041492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00714508
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00425384
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00455971
[X] model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0965482 seconds. Fastest Tactic: 0x4a25dfdaea3c22a0 Time: 0.00387433
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4a25dfdaea3c22a0
[X] =============== Computing costs for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00833947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00500601
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00849413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00660057
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00496204
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00476343
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00630682
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00512113
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00477379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00862414
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00483291
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00583154
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00435546
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00478903
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00495937
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00515627
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00491702
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00492753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.0042921
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00859679
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0085384
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.0060831
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00515758
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00625778
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00608136
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00595505
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00486446
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00472744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00470175
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00483849
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00891818
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00484267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00502448
[X] model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0916499 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.0042921
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3
[X] =============== Computing costs for /model/encoder/Resize_1
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])
[X] Tactic: 0x0000000000000000 Time: 0.0320524
[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00188898 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0320524
[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])
[X] Tactic: 0x0000000000000003 Time: 0.00399835
[X] Tactic: 0x0000000000000005 Time: 0.00354134
[X] Tactic: 0x0000000000000006 Time: 0.00448825
[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.008088 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00354134
[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005
[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0167509
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0134033
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0176376
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0141236
[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0104382 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0134033
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00935852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00881909
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0114748
[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00737384 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00881909
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0154885
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0110737
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0106677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0101204
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0108532
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0151903
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00978072
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0111154
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0141644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00918948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00924223
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.010113
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0112299
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00907301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0116039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00945155
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00842027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00937955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0098045
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00890807
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0134242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0130457
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0099658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00987388
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00810768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00949244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00881179
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0103037
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00843067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00961676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00831116
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00970849
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0131524
[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0843824 seconds. Fastest Tactic: 0x7720f198395e7d3d Time: 0.00810768
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7720f198395e7d3d
[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(3276800,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1:16,2560,32) -> Int8(51200,1:16,640,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.010643
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00995294
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0104773
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00748563
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00845573
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00587116
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00728765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0100264
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00891846
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0106333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00905658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00751383
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00641539
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00603829
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00670336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.0101873
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00630622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00921052
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00636418
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0105673
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0108394
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00707889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00690003
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00734052
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00717753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00708489
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00603867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00676373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00871139
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00594377
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0111826
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00869908
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00617916
[X] model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.08633 seconds. Fastest Tactic: 0x458f02d2b10db57c Time: 0.00587116
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x458f02d2b10db57c
[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0108989
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0103751
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00720227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.01335
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.0080414
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0179054
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00817249
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0078204
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0106727
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0131319
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0108177
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0179043
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00731362
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0109853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00775467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00850214
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0131286
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0126839
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0183489
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0100627
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0179599
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00759297
[X] model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0622224 seconds. Fastest Tactic: 0xfdf7509af98902e0 Time: 0.00720227
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfdf7509af98902e0
[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0xfdf7509af98902e0, 0.00720227 ms
[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0217687 ms
[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x23b890da05937b9e, 0.00896814 ms
[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0x2d8ab2aa0639fda9, 0.00960457 ms
[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))
[X] *************** Autotuning format combination: Float(819200,6400,80,1), Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00530692
[X] Tactic: 0x0000000000000001 Time: 0.00417867
[X] Tactic: 0x0000000000000002 Time: 0.00412602
[X] Tactic: 0x0000000000000003 Time: 0.00395595
[X] Tactic: 0x0000000000000004 Time: 0.00382194
[X] Tactic: 0x0000000000000005 Time: 0.00367672
[X] Tactic: 0x0000000000000006 Time: 0.0039164
[X] Tactic: 0x0000000000000007 Time: 0.00376773
[X] Tactic: 0x0000000000000008 Time: 0.00383154
[X] Tactic: 0x0000000000000009 Time: 0.00389222
[X] Tactic: 0x000000000000001c Time: 0.00530455
[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0296064 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00367672
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000000a Time: 0.00799899
[X] Tactic: 0x000000000000000b Time: 0.0066629
[X] Tactic: 0x000000000000000c Time: 0.00813079
[X] Tactic: 0x000000000000000d Time: 0.00750009
[X] Tactic: 0x000000000000000e Time: 0.0080546
[X] Tactic: 0x000000000000000f Time: 0.0110923
[X] Tactic: 0x0000000000000010 Time: 0.00906667
[X] Tactic: 0x0000000000000011 Time: 0.0113262
[X] Tactic: 0x0000000000000012 Time: 0.0164485
[X] Tactic: 0x0000000000000013 Time: 0.0200822
[X] Tactic: 0x0000000000000014 Time: 0.00705933
[X] Tactic: 0x0000000000000015 Time: 0.00672832
[X] Tactic: 0x0000000000000016 Time: 0.00653867
[X] Tactic: 0x0000000000000017 Time: 0.00709089
[X] Tactic: 0x0000000000000018 Time: 0.00594433
[X] Tactic: 0x0000000000000019 Time: 0.00591907
[X] Tactic: 0x000000000000001a Time: 0.00590784
[X] Tactic: 0x000000000000001b Time: 0.00662965
[X] Tactic: 0x000000000000001d Time: 0.00870455
[X] Tactic: 0x000000000000001e Time: 0.00679339
[X] Tactic: 0x000000000000001f Time: 0.00595296
[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.045982 seconds. Fastest Tactic: 0x000000000000001a Time: 0.00590784
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001a
[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00612073
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00700578
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00620919
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00561191
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00781867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00594826
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00539114
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00759927
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00690939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00640886
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00744984
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00794083
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00597391
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00618667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00653762
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.0071537
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00638853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00750222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00574192
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00636096
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00632674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00517579
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00690852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00536229
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00519975
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00509172
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00619457
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00651979
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00699444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00598534
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0065736
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00682949
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00642134
[X] model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0931367 seconds. Fastest Tactic: 0x65a38dbc9e991257 Time: 0.00509172
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65a38dbc9e991257
[X] =============== Computing costs for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Float(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0122579
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0120309
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0129112
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126392
[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107867 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0120309
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.011275
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0100279
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0115634
[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00738108 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.0100279
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0145377
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0100599
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0122632
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0101401
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0123173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0140751
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0119288
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0126633
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0136273
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.0102406
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.010465
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0100916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0102397
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.0115645
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0125049
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0113991
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.011577
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.0101036
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0110063
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00981365
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0137485
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.013728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.011883
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0121322
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0112996
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0102035
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00957958
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0107083
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0108759
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0137818
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0119962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.0119505
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0139067
[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0888476 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00957958
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e
[X] =============== Computing costs for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(25600,1:16,640,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0177622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0104165
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104783
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0129227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00836293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0313901
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0118613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.010471
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0174997
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0114738
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.017264
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.031327
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0112434
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0175461
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0103334
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.0084976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.012839
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0113664
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0318516
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.010124
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0314715
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0105973
[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0556261 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.00836293
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0177774
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0104145
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.010421
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0128997
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00838
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0314541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0118702
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.010432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0174837
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0115373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0314172
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0113205
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0175512
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0103638
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00850693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0128234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0113607
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0318962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0101844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0314958
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0106447
[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0588052 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.00838
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84
[X] =============== Computing costs for model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0xff6944b17d5b2e32, 0.0131007 ms
[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x5e4f6d7c83746fd6, 0.0055416 ms
[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x9ec201b34455146e, 0.00466785 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) [Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x2eba0b6a8ec55fa3, 0.00451993 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6176c23707257237, 0.00574247 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6176c23707257237, 0.00574247 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0216127 ms
[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x85c1a5f7f239cf84, 0.00625857 ms
[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x45f7566cdb2b10fb, 0.00543948 ms
[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))
[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000005, 0.00272772 ms
[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000015, 0.00321477 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x4a25dfdaea3c22a0, 0.00387433 ms
[X] =============== Computing costs for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00984219
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00866434
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0101114
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00890863
[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0114076 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00866434
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00559787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00477318
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00498022
[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0093263 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00477318
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0108573
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00480518
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00694755
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0053462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00711262
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0107063
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00546501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00717526
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0104383
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00494196
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00507636
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00527283
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00481356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00542262
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00733055
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.005211
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00531776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00491216
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00526217
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00524267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0103421
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0104583
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00691156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00691722
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00524633
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0049189
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00514068
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00489569
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00521124
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00623091
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00552271
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00670741
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0106443
[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0940881 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00480518
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(102400,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(3200,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(6400,1:16,320,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0177993
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00755745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00620484
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.0073484
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0312601
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0116237
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0102814
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175056
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00675392
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0171979
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0313988
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0109423
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0175792
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0102604
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.0085456
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00573849
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00663592
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.031871
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00702466
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0312465
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0105797
[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0581866 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00573849
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0177746
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00756361
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0103832
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00622341
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00730481
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0312756
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0116318
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0103024
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175168
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00674496
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0313552
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0110015
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0175613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0102432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00860198
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.005856
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00664492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0318555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00703378
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0312669
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0105563
[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0608293 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.005856
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4
[X] =============== Computing costs for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0145929
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0119512
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0149537
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0125894
[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.011052 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0119512
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00670805
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00499184
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00476434
[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00930533 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00476434
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0138436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00336118
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00825834
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00452785
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00854027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0134643
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00573288
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00856889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0110445
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00426599
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00436059
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00450623
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00348667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00574228
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0090635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00546082
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00554579
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00372824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00398984
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00395633
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0129251
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0112171
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0081946
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00822842
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00563698
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00376701
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00387212
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00377133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00464637
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00660978
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00597715
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00813763
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.011429
[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0962184 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00336118
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(51200,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(1600,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(3200,1:16,160,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0104677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00359249
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0101188
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00746501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00427378
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00511046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00714349
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00384
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00369501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0103602
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00400597
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00589736
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00411641
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00505261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00521417
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00355779
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00519418
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00392012
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.0040194
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0102377
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0106573
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00694226
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.0054037
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00699422
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00683842
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.0067313
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00523017
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.0043725
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00390425
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00497333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0111115
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00367977
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00551117
[X] model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0961884 seconds. Fastest Tactic: 0x44824770683c7b80 Time: 0.00355779
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x44824770683c7b80
[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0108707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00560871
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00699511
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00465896
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00525633
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0177987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00780453
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00697489
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0106293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00495592
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0105407
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0178291
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00709492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0107187
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00690242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00591813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00440042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00479756
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0181457
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0051369
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0177982
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00722723
[X] model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0576418 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00440042
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4
[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1)] got cached result: CaskConvolution, tactic 0xc985777c89c6b3a4, 0.00440042 ms
[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0269727
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0215967
[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00489113 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0215967
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00813243
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0133679
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00659765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00619615
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0083584
[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.012721 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00619615
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0209233
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00624652
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0188012
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00751123
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00432397
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0121181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.011605
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00609397
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0134165
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00475974
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00744391
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00491419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.011968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.012125
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00745078
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00547008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00531674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00465081
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00613469
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00939318
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00776633
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0205948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0190643
[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0583603 seconds. Fastest Tactic: 0xd14bd6d95fefd45e Time: 0.00432397
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd14bd6d95fefd45e
[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))
[X] *************** Autotuning format combination: Float(51200,400,20,1), Float(51200,400,20,1) -> Int8(51200,400,20,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00233346
[X] Tactic: 0x0000000000000001 Time: 0.00249544
[X] Tactic: 0x0000000000000002 Time: 0.00236618
[X] Tactic: 0x0000000000000003 Time: 0.00267631
[X] Tactic: 0x0000000000000004 Time: 0.00245482
[X] Tactic: 0x0000000000000005 Time: 0.00238362
[X] Tactic: 0x0000000000000006 Time: 0.00335776
[X] Tactic: 0x0000000000000007 Time: 0.00286532
[X] Tactic: 0x0000000000000008 Time: 0.00265405
[X] Tactic: 0x0000000000000009 Time: 0.00246392
[X] Tactic: 0x000000000000001c Time: 0.00232807
[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0302966 seconds. Fastest Tactic: 0x000000000000001c Time: 0.00232807
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c
[X] *************** Autotuning format combination: Float(1600,400:32,20,1), Float(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000000a Time: 0.00334176
[X] Tactic: 0x000000000000000b Time: 0.00397143
[X] Tactic: 0x000000000000000c Time: 0.00534959
[X] Tactic: 0x000000000000000d Time: 0.00548214
[X] Tactic: 0x000000000000000e Time: 0.0065038
[X] Tactic: 0x000000000000000f Time: 0.00939111
[X] Tactic: 0x0000000000000010 Time: 0.008608
[X] Tactic: 0x0000000000000011 Time: 0.010145
[X] Tactic: 0x0000000000000012 Time: 0.015078
[X] Tactic: 0x0000000000000013 Time: 0.019184
[X] Tactic: 0x0000000000000014 Time: 0.00272304
[X] Tactic: 0x0000000000000015 Time: 0.00306965
[X] Tactic: 0x0000000000000016 Time: 0.00412708
[X] Tactic: 0x0000000000000017 Time: 0.00587079
[X] Tactic: 0x0000000000000018 Time: 0.00245733
[X] Tactic: 0x0000000000000019 Time: 0.00261592
[X] Tactic: 0x000000000000001a Time: 0.00309047
[X] Tactic: 0x000000000000001b Time: 0.0041359
[X] Tactic: 0x000000000000001d Time: 0.00503705
[X] Tactic: 0x000000000000001e Time: 0.00286642
[X] Tactic: 0x000000000000001f Time: 0.00248881
[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0593969 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00245733
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018
[X] =============== Computing costs for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(102400,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(3200,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(6400,1:16,320,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00609164
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00310815
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00601543
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00509156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00354055
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00386146
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00490463
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00336373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00320041
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0061632
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00346216
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00451748
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.0033376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00374294
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.0040275
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00311674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00386477
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00338726
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00324733
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00612073
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00618331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00464163
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.004256
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00484344
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00465081
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00453261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00397651
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00349878
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00332684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00356153
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00645928
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00311726
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00414143
[X] model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.102894 seconds. Fastest Tactic: 0xc6cdb1e47323bb01 Time: 0.00310815
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc6cdb1e47323bb01
[X] =============== Computing costs for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv
[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00968655
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00870236
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00998619
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00894176
[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.010671 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00870236
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00561476
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00412444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00399479
[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0086888 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00399479
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0108284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00334229
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00687891
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0038709
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00709265
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0104543
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00488394
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00699644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0103564
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00372231
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00379091
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00390586
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00339728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00489322
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00722406
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00464459
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00472774
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00340822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00359972
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00357356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0102164
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0102691
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00663927
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00675178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00475748
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00346502
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00348922
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00358675
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00402596
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00554265
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00505648
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00664157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0106213
[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0921339 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00334229
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]}
[X] *************** Autotuning format combination: Int64(2,1), Float(1638400,6400,80,1), Float(409600,1600,40,1), Float(102400,400,20,1) -> Float(28800,96,1), Float(28800,96,1), Float(28800,96,1), Float(57600,192,1), Float(57600,192,1), Float(57600,192,1), Int64(300,1), Float(1200,4,1), Float(300,1) ***************
[X] --------------- Timing Runner: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023])
[X]  (foreignNode) Set user's cuda kernel library
[X] Subgraph compilation completed in 7.32 seconds.
[X] Tactic: 0x0000000000000000 Time: 0.411648
[X] {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023]) profiling completed in 7.44306 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.411648
[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(1228800,409600,640,1) ***************
[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0049247
[X] Tactic: 0x00000000000003ea Time: 0.0057139
[X] Tactic: 0x0000000000000000 Time: 0.00488116
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0100154 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00488116
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************
[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00926074
[X] Tactic: 0x00000000000003ea Time: 0.005859
[X] Tactic: 0x0000000000000000 Time: 0.00400813
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00868314 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00400813
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************
[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0286667
[X] Tactic: 0x00000000000003ea Time: 0.0239832
[X] Tactic: 0x0000000000000000 Time: 0.0287004
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00526399 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0239832
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************
[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.21624
[X] Tactic: 0x00000000000003ea Time: 0.0188012
[X] Tactic: 0x0000000000000000 Time: 0.216064
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00606248 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0188012
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.010166
[X] Tactic: 0x00000000000003ea Time: 0.00582731
[X] Tactic: 0x0000000000000000 Time: 0.0100897
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00795554 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00582731
[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.02956
[X] Tactic: 0x00000000000003ea Time: 0.0251718
[X] Tactic: 0x0000000000000000 Time: 0.0295164
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00547535 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0251718
[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.215723
[X] Tactic: 0x00000000000003ea Time: 0.0224939
[X] Tactic: 0x0000000000000000 Time: 0.215877
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00622555 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0224939
[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(1228800,409600,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00847333
[X] Tactic: 0x00000000000003ea Time: 0.00901622
[X] Tactic: 0x0000000000000000 Time: 0.00848
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00706809 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00847333
[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,1:16,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.030032
[X] Tactic: 0x00000000000003ea Time: 0.0120709
[X] Tactic: 0x0000000000000000 Time: 0.0299733
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00613499 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0120709
[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,409600:32,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.216235
[X] Tactic: 0x00000000000003ea Time: 0.0305813
[X] Tactic: 0x0000000000000000 Time: 0.0158857
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00604527 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0158857
[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(1228800,409600,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00896056
[X] Tactic: 0x00000000000003ea Time: 0.0500861
[X] Tactic: 0x0000000000000000 Time: 0.00895214
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00681465 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00895214
[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:4,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0108315
[X] Tactic: 0x00000000000003ea Time: 0.0141462
[X] Tactic: 0x0000000000000000 Time: 0.0107888
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00690523 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0107888
[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:32,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.217115
[X] Tactic: 0x00000000000003ea Time: 0.0240533
[X] Tactic: 0x0000000000000000 Time: 0.21704
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00669297 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0240533
[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(1228800,409600,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0117083
[X] Tactic: 0x00000000000003ea Time: 0.0504274
[X] Tactic: 0x0000000000000000 Time: 0.0117554
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00691784 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0117083
[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,409600:4,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0130695
[X] Tactic: 0x00000000000003ea Time: 0.0144391
[X] Tactic: 0x0000000000000000 Time: 0.0051922
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00813214 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0051922
[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,1:16,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0301102
[X] Tactic: 0x00000000000003ea Time: 0.0121211
[X] Tactic: 0x0000000000000000 Time: 0.0300151
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00653009 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0121211
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0263737
[X] Tactic: 0x00000000000003ea Time: 0.0113963
[X] Tactic: 0x0000000000000000 Time: 0.026304
[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00640135 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0113963
[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0639396
[X] Tactic: 0x00000000000003ea Time: 0.0119413
[X] Tactic: 0x0000000000000000 Time: 0.0641458
[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00635111 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0119413
[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0100163
[X] Tactic: 0x00000000000003ea Time: 0.0116193
[X] Tactic: 0x0000000000000000 Time: 0.00470625
[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0085782 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00470625
[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0198707
[X] Tactic: 0x00000000000003ea Time: 0.0104517
[X] Tactic: 0x0000000000000000 Time: 0.0199504
[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00667047 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0104517
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0113963 ms
[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0119413 ms
[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00470625 ms
[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0104517 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(1638400,102400:4,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0502278
[X] Tactic: 0x00000000000003ea Time: 0.0120171
[X] Tactic: 0x0000000000000000 Time: 0.0501973
[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0064341 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0120171
[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(204800,102400:32,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.13878
[X] Tactic: 0x00000000000003ea Time: 0.0115638
[X] Tactic: 0x0000000000000000 Time: 0.138809
[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00693077 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0115638
[X] *************** Autotuning Reformat: Int8(204800,102400:32,320,1) -> Int8(1638400,102400:4,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0157741
[X] Tactic: 0x00000000000003ea Time: 0.0156625
[X] Tactic: 0x0000000000000000 Time: 0.0065874
[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00785504 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0065874
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0106363
[X] Tactic: 0x00000000000003ea Time: 0.0116576
[X] Tactic: 0x0000000000000000 Time: 0.010532
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00757688 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.010532
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0113817
[X] Tactic: 0x00000000000003ea Time: 0.0106473
[X] Tactic: 0x0000000000000000 Time: 0.0113806
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00726741 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0106473
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0263836
[X] Tactic: 0x00000000000003ea Time: 0.0105707
[X] Tactic: 0x0000000000000000 Time: 0.00370465
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00831115 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00370465
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0136947
[X] Tactic: 0x00000000000003ea Time: 0.00936444
[X] Tactic: 0x0000000000000000 Time: 0.0136977
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00680075 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00936444
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00625896
[X] Tactic: 0x00000000000003ea Time: 0.00819538
[X] Tactic: 0x0000000000000000 Time: 0.0032846
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00918878 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0032846
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0102297
[X] Tactic: 0x00000000000003ea Time: 0.00603086
[X] Tactic: 0x0000000000000000 Time: 0.0102096
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00706741 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00603086
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00948504
[X] Tactic: 0x00000000000003ea Time: 0.00570938
[X] Tactic: 0x0000000000000000 Time: 0.00426234
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00806689 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00426234
[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0108686
[X] Tactic: 0x00000000000003ea Time: 0.00615525
[X] Tactic: 0x0000000000000000 Time: 0.0108446
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00699525 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00615525
[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0276833
[X] Tactic: 0x00000000000003ea Time: 0.00572637
[X] Tactic: 0x0000000000000000 Time: 0.027712
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00651569 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00572637
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0101857
[X] Tactic: 0x00000000000003ea Time: 0.00564089
[X] Tactic: 0x0000000000000000 Time: 0.0101776
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00707347 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00564089
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0263294
[X] Tactic: 0x00000000000003ea Time: 0.00595753
[X] Tactic: 0x0000000000000000 Time: 0.00365241
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00760775 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00365241
[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0135334
[X] Tactic: 0x00000000000003ea Time: 0.00576442
[X] Tactic: 0x0000000000000000 Time: 0.0135582
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00674502 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00576442
[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0340245
[X] Tactic: 0x00000000000003ea Time: 0.00571101
[X] Tactic: 0x0000000000000000 Time: 0.0340213
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00624349 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00571101
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00624079
[X] Tactic: 0x00000000000003ea Time: 0.00596286
[X] Tactic: 0x0000000000000000 Time: 0.00328032
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00878769 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00328032
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0102077
[X] Tactic: 0x00000000000003ea Time: 0.00568154
[X] Tactic: 0x0000000000000000 Time: 0.0101954
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00703903 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00568154
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00576442 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571101 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00328032 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00426234 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572637 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00365241 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00576442 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571101 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00328032 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0412504
[X] Tactic: 0x00000000000003ea Time: 0.00689937
[X] Tactic: 0x0000000000000000 Time: 0.0413464
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00875202 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00689937
[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0220013
[X] Tactic: 0x00000000000003ea Time: 0.00679637
[X] Tactic: 0x0000000000000000 Time: 0.0220553
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00657591 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00679637
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0412871
[X] Tactic: 0x00000000000003ea Time: 0.0070922
[X] Tactic: 0x0000000000000000 Time: 0.0413274
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00644224 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0070922
[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.022116
[X] Tactic: 0x00000000000003ea Time: 0.00785315
[X] Tactic: 0x0000000000000000 Time: 0.0220747
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00660377 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00785315
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00514609
[X] Tactic: 0x00000000000003ea Time: 0.00730759
[X] Tactic: 0x0000000000000000 Time: 0.00516316
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0083674 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00514609
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00837227
[X] Tactic: 0x00000000000003ea Time: 0.0056687
[X] Tactic: 0x0000000000000000 Time: 0.00370999
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00807856 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00370999
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00927911
[X] Tactic: 0x00000000000003ea Time: 0.0053908
[X] Tactic: 0x0000000000000000 Time: 0.00924944
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00677358 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0053908
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0276439
[X] Tactic: 0x00000000000003ea Time: 0.00543948
[X] Tactic: 0x0000000000000000 Time: 0.0276242
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00611273 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00543948
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.022331
[X] Tactic: 0x00000000000003ea Time: 0.00653354
[X] Tactic: 0x0000000000000000 Time: 0.022245
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00597726 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00653354
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0224768
[X] Tactic: 0x00000000000003ea Time: 0.00789383
[X] Tactic: 0x0000000000000000 Time: 0.0223822
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00610295 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00789383
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00924857
[X] Tactic: 0x00000000000003ea Time: 0.00794794
[X] Tactic: 0x0000000000000000 Time: 0.00921773
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0069164 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00794794
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.040531
[X] Tactic: 0x00000000000003ea Time: 0.0077263
[X] Tactic: 0x0000000000000000 Time: 0.0405677
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00612747 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0077263
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00426234 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00615525 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572637 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564089 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00365241 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00576442 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571101 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00328032 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00568154 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00576442 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571101 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00328032 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00689937 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00679637 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00514609 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00370999 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0053908 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00543948 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00653354 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00789383 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00794794 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0077263 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00426234 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572637 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00365241 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00576442 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571101 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00328032 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00426234 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00615525 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572637 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564089 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00365241 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00576442 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571101 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00328032 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00568154 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0128923
[X] Tactic: 0x00000000000003ea Time: 0.00753304
[X] Tactic: 0x0000000000000000 Time: 0.0128706
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00639643 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00753304
[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.022469
[X] Tactic: 0x00000000000003ea Time: 0.00781494
[X] Tactic: 0x0000000000000000 Time: 0.0224284
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00590506 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00781494
[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00465363
[X] Tactic: 0x00000000000003ea Time: 0.00788664
[X] Tactic: 0x0000000000000000 Time: 0.00288589
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00849854 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00288589
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(102400,6400:4,80,1) -> Int8(12800,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00935615
[X] Tactic: 0x00000000000003ea Time: 0.00715847
[X] Tactic: 0x0000000000000000 Time: 0.0026135
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00757969 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0026135
[X] *************** Autotuning Reformat: Int8(12800,6400:32,80,1) -> Int8(102400,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00358962
[X] Tactic: 0x00000000000003ea Time: 0.00570197
[X] Tactic: 0x0000000000000000 Time: 0.00255731
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00838114 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00255731
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0218827
[X] Tactic: 0x00000000000003ea Time: 0.00554929
[X] Tactic: 0x0000000000000000 Time: 0.0219427
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00613657 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554929
[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0122126
[X] Tactic: 0x00000000000003ea Time: 0.00551886
[X] Tactic: 0x0000000000000000 Time: 0.0122579
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00636716 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551886
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0219153
[X] Tactic: 0x00000000000003ea Time: 0.00607438
[X] Tactic: 0x0000000000000000 Time: 0.0219447
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00609491 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00607438
[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0122667
[X] Tactic: 0x00000000000003ea Time: 0.00665307
[X] Tactic: 0x0000000000000000 Time: 0.0122583
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00638036 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00665307
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00374532
[X] Tactic: 0x00000000000003ea Time: 0.00647651
[X] Tactic: 0x0000000000000000 Time: 0.00373049
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0079939 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00373049
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0054006
[X] Tactic: 0x00000000000003ea Time: 0.00559467
[X] Tactic: 0x0000000000000000 Time: 0.00295571
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00786072 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00295571
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00989772
[X] Tactic: 0x00000000000003ea Time: 0.00552324
[X] Tactic: 0x0000000000000000 Time: 0.00993004
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00643092 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552324
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0154739
[X] Tactic: 0x00000000000003ea Time: 0.0055696
[X] Tactic: 0x0000000000000000 Time: 0.0154448
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0061011 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055696
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0125357
[X] Tactic: 0x00000000000003ea Time: 0.0060221
[X] Tactic: 0x0000000000000000 Time: 0.0125258
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00625048 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0060221
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0127064
[X] Tactic: 0x00000000000003ea Time: 0.00652779
[X] Tactic: 0x0000000000000000 Time: 0.0127538
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00630975 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00652779
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00582087
[X] Tactic: 0x00000000000003ea Time: 0.00641108
[X] Tactic: 0x0000000000000000 Time: 0.00579439
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00718399 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00579439
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0222
[X] Tactic: 0x00000000000003ea Time: 0.00646995
[X] Tactic: 0x0000000000000000 Time: 0.022156
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00610114 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00646995
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00617758
[X] Tactic: 0x00000000000003ea Time: 0.00712737
[X] Tactic: 0x0000000000000000 Time: 0.00322462
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00788418 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00322462
[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00920735
[X] Tactic: 0x00000000000003ea Time: 0.00607166
[X] Tactic: 0x0000000000000000 Time: 0.00916612
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00667443 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00607166
[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0156703
[X] Tactic: 0x00000000000003ea Time: 0.0067008
[X] Tactic: 0x0000000000000000 Time: 0.0157222
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00614453 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0067008
[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00645887
[X] Tactic: 0x00000000000003ea Time: 0.00714689
[X] Tactic: 0x0000000000000000 Time: 0.00642195
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00696857 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00642195
[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0154085
[X] Tactic: 0x00000000000003ea Time: 0.00676352
[X] Tactic: 0x0000000000000000 Time: 0.00311796
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00720073 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00311796
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00753304 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781494 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288589 ms
[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00629656
[X] Tactic: 0x00000000000003ea Time: 0.0057486
[X] Tactic: 0x0000000000000000 Time: 0.00628583
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00696546 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0057486
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00753304 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781494 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288589 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554929 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00551886 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00373049 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00295571 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552324 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055696 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0060221 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00652779 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00579439 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00646995 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00322462 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0067008 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00311796 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00753304 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781494 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288589 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00322462 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00607166 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0067008 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00642195 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00311796 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00753304 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781494 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288589 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0057486 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0078973
[X] Tactic: 0x00000000000003ea Time: 0.00584111
[X] Tactic: 0x0000000000000000 Time: 0.00792484
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00727829 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00584111
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.013081
[X] Tactic: 0x00000000000003ea Time: 0.00643836
[X] Tactic: 0x0000000000000000 Time: 0.0130942
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00631774 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00643836
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00358549
[X] Tactic: 0x00000000000003ea Time: 0.00642544
[X] Tactic: 0x0000000000000000 Time: 0.00261508
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0084586 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00261508
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00628662
[X] Tactic: 0x00000000000003ea Time: 0.00567196
[X] Tactic: 0x0000000000000000 Time: 0.00243613
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00750253 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00243613
[X] *************** Autotuning Reformat: Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00312135
[X] Tactic: 0x00000000000003ea Time: 0.00576993
[X] Tactic: 0x0000000000000000 Time: 0.00259266
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00842025 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00259266
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0122747
[X] Tactic: 0x00000000000003ea Time: 0.00559022
[X] Tactic: 0x0000000000000000 Time: 0.0122674
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00633532 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559022
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00737113
[X] Tactic: 0x00000000000003ea Time: 0.00554562
[X] Tactic: 0x0000000000000000 Time: 0.00741523
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00678655 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554562
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0123006
[X] Tactic: 0x00000000000003ea Time: 0.0055632
[X] Tactic: 0x0000000000000000 Time: 0.0123196
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00633089 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055632
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00738296
[X] Tactic: 0x00000000000003ea Time: 0.00552761
[X] Tactic: 0x0000000000000000 Time: 0.00742353
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00678277 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552761
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00300646
[X] Tactic: 0x00000000000003ea Time: 0.00544723
[X] Tactic: 0x0000000000000000 Time: 0.00297619
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0087277 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00297619
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00386575
[X] Tactic: 0x00000000000003ea Time: 0.0055346
[X] Tactic: 0x0000000000000000 Time: 0.00249305
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0079346 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00249305
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00697578
[X] Tactic: 0x00000000000003ea Time: 0.00557725
[X] Tactic: 0x0000000000000000 Time: 0.00695644
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00675384 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557725
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0092529
[X] Tactic: 0x00000000000003ea Time: 0.00549893
[X] Tactic: 0x0000000000000000 Time: 0.0092036
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00655797 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00549893
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00756504
[X] Tactic: 0x00000000000003ea Time: 0.0055776
[X] Tactic: 0x0000000000000000 Time: 0.00760048
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00668661 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055776
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00773261
[X] Tactic: 0x00000000000003ea Time: 0.00549753
[X] Tactic: 0x0000000000000000 Time: 0.0077343
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00676211 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00549753
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00413787
[X] Tactic: 0x00000000000003ea Time: 0.0055012
[X] Tactic: 0x0000000000000000 Time: 0.00413801
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00780133 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00413787
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0127056
[X] Tactic: 0x00000000000003ea Time: 0.00557636
[X] Tactic: 0x0000000000000000 Time: 0.012736
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00628521 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557636
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00437832
[X] Tactic: 0x00000000000003ea Time: 0.00555069
[X] Tactic: 0x0000000000000000 Time: 0.00285137
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00837274 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00285137
[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00731177
[X] Tactic: 0x00000000000003ea Time: 0.00561618
[X] Tactic: 0x0000000000000000 Time: 0.0073498
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00687096 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561618
[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00945718
[X] Tactic: 0x00000000000003ea Time: 0.00572475
[X] Tactic: 0x0000000000000000 Time: 0.00946696
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00651253 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00572475
[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00463926
[X] Tactic: 0x00000000000003ea Time: 0.00574731
[X] Tactic: 0x0000000000000000 Time: 0.00465629
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00745343 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00463926
[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00941926
[X] Tactic: 0x00000000000003ea Time: 0.00567991
[X] Tactic: 0x0000000000000000 Time: 0.00301146
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00751338 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00301146
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584111 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00643836 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261508 ms
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00438639
[X] Tactic: 0x00000000000003ea Time: 0.00566527
[X] Tactic: 0x0000000000000000 Time: 0.00438863
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00760946 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00438639
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584111 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00643836 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261508 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(409600,1600,40,1) -> Float(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559022 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554562 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00297619 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00249305 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557725 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00549893 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055776 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00549753 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00413787 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557636 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00285137 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572475 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00301146 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584111 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00643836 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261508 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00285137 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561618 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572475 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00463926 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00301146 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584111 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00643836 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261508 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00438639 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00567286
[X] Tactic: 0x00000000000003ea Time: 0.00563947
[X] Tactic: 0x0000000000000000 Time: 0.00562098
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00763669 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00562098
[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00838347
[X] Tactic: 0x00000000000003ea Time: 0.00563325
[X] Tactic: 0x0000000000000000 Time: 0.0083968
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00660865 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563325
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00311141
[X] Tactic: 0x00000000000003ea Time: 0.00553792
[X] Tactic: 0x0000000000000000 Time: 0.00244586
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00838994 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00244586
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00450105
[X] Tactic: 0x00000000000003ea Time: 0.00559591
[X] Tactic: 0x0000000000000000 Time: 0.0026055
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00803468 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0026055
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00286423
[X] Tactic: 0x00000000000003ea Time: 0.0055728
[X] Tactic: 0x0000000000000000 Time: 0.00228237
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00876211 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00228237
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00732707
[X] Tactic: 0x00000000000003ea Time: 0.0056678
[X] Tactic: 0x0000000000000000 Time: 0.00725635
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00676059 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0056678
[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00500919
[X] Tactic: 0x00000000000003ea Time: 0.00542916
[X] Tactic: 0x0000000000000000 Time: 0.00499582
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00741973 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00499582
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00726191
[X] Tactic: 0x00000000000003ea Time: 0.00551519
[X] Tactic: 0x0000000000000000 Time: 0.00728232
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00677714 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551519
[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00499518
[X] Tactic: 0x00000000000003ea Time: 0.00541041
[X] Tactic: 0x0000000000000000 Time: 0.00499885
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00740087 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00499518
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(204800,400,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00262669
[X] Tactic: 0x00000000000003ea Time: 0.00536804
[X] Tactic: 0x0000000000000000 Time: 0.00258117
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00846202 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00258117
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00310104
[X] Tactic: 0x00000000000003ea Time: 0.00540972
[X] Tactic: 0x0000000000000000 Time: 0.00235914
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00830822 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00235914
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.005184
[X] Tactic: 0x00000000000003ea Time: 0.00548546
[X] Tactic: 0x0000000000000000 Time: 0.00516004
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00727667 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00516004
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00589156
[X] Tactic: 0x00000000000003ea Time: 0.00552166
[X] Tactic: 0x0000000000000000 Time: 0.00586742
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00703911 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552166
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(204800,400,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00503435
[X] Tactic: 0x00000000000003ea Time: 0.00544293
[X] Tactic: 0x0000000000000000 Time: 0.00502782
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00732063 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00502782
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00511257
[X] Tactic: 0x00000000000003ea Time: 0.00556018
[X] Tactic: 0x0000000000000000 Time: 0.00509463
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00774184 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00509463
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00322903
[X] Tactic: 0x00000000000003ea Time: 0.00544946
[X] Tactic: 0x0000000000000000 Time: 0.00330149
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00963592 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00322903
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00780998
[X] Tactic: 0x00000000000003ea Time: 0.00545721
[X] Tactic: 0x0000000000000000 Time: 0.00784
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00739586 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00545721
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00337627
[X] Tactic: 0x00000000000003ea Time: 0.0056546
[X] Tactic: 0x0000000000000000 Time: 0.0025987
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0106428 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0025987
[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0056032
[X] Tactic: 0x00000000000003ea Time: 0.00551746
[X] Tactic: 0x0000000000000000 Time: 0.00557049
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00815493 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551746
[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0061023
[X] Tactic: 0x00000000000003ea Time: 0.00555156
[X] Tactic: 0x0000000000000000 Time: 0.00608407
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00789355 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00555156
[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00371011
[X] Tactic: 0x00000000000003ea Time: 0.00559076
[X] Tactic: 0x0000000000000000 Time: 0.00369688
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00889151 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00369688
[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00592019
[X] Tactic: 0x00000000000003ea Time: 0.00563627
[X] Tactic: 0x0000000000000000 Time: 0.00293193
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00939163 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00293193
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00562098 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563325 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00244586 ms
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00348611
[X] Tactic: 0x00000000000003ea Time: 0.00565013
[X] Tactic: 0x0000000000000000 Time: 0.00350467
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00834437 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00348611
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00562098 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563325 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00244586 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(204800,400,20,1) -> Float(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0056678 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(6400,400:32,20,1) -> Float(204800,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00499582 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00235914 ms
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552166 ms
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00509463 ms
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00545721 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00293193 ms
[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00244586 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00368516
[X] Tactic: 0x00000000000003ea Time: 0.00551589
[X] Tactic: 0x0000000000000000 Time: 0.00368293
[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00807744 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00368293
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00322462 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00607166 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0067008 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00642195 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00311796 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00753304 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781494 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288589 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0057486 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(3276800,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.02273
[X] Tactic: 0x00000000000003ea Time: 0.00629152
[X] Tactic: 0x0000000000000000 Time: 0.0227221
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00598581 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00629152
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0232114
[X] Tactic: 0x00000000000003ea Time: 0.00556676
[X] Tactic: 0x0000000000000000 Time: 0.0232249
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00614833 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00556676
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0417413
[X] Tactic: 0x00000000000003ea Time: 0.00557422
[X] Tactic: 0x0000000000000000 Time: 0.0419293
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00582756 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557422
[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(3276800,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0128952
[X] Tactic: 0x00000000000003ea Time: 0.00551729
[X] Tactic: 0x0000000000000000 Time: 0.0128857
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00629986 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551729
[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00625087
[X] Tactic: 0x00000000000003ea Time: 0.0057806
[X] Tactic: 0x0000000000000000 Time: 0.0032777
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00783444 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0032777
[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(204800,1:16,2560,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0102332
[X] Tactic: 0x00000000000003ea Time: 0.00561298
[X] Tactic: 0x0000000000000000 Time: 0.0101977
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00651958 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561298
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(3276800,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.022204
[X] Tactic: 0x00000000000003ea Time: 0.00632292
[X] Tactic: 0x0000000000000000 Time: 0.0221707
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00628499 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00632292
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0227001
[X] Tactic: 0x00000000000003ea Time: 0.00556498
[X] Tactic: 0x0000000000000000 Time: 0.0228203
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00649081 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00556498
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(204800,1:16,2560,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0101779
[X] Tactic: 0x00000000000003ea Time: 0.00562738
[X] Tactic: 0x0000000000000000 Time: 0.0101831
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00702402 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562738
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0415028
[X] Tactic: 0x00000000000003ea Time: 0.00560765
[X] Tactic: 0x0000000000000000 Time: 0.041453
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00623118 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00560765
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.012855
[X] Tactic: 0x00000000000003ea Time: 0.00636257
[X] Tactic: 0x0000000000000000 Time: 0.0128976
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00627581 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00636257
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00627259
[X] Tactic: 0x00000000000003ea Time: 0.00601714
[X] Tactic: 0x0000000000000000 Time: 0.00329035
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00742546 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00329035
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0102183
[X] Tactic: 0x00000000000003ea Time: 0.00563858
[X] Tactic: 0x0000000000000000 Time: 0.010122
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00652697 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563858
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0273083
[X] Tactic: 0x00000000000003ea Time: 0.00557813
[X] Tactic: 0x0000000000000000 Time: 0.00373499
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00707746 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00373499
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00285137 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561618 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572475 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00463926 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00301146 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584111 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00643836 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261508 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00438639 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(819200,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00866598
[X] Tactic: 0x00000000000003ea Time: 0.00546448
[X] Tactic: 0x0000000000000000 Time: 0.0086906
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00676164 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00546448
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00889319
[X] Tactic: 0x00000000000003ea Time: 0.00558365
[X] Tactic: 0x0000000000000000 Time: 0.0088772
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00660072 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558365
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0135979
[X] Tactic: 0x00000000000003ea Time: 0.00567467
[X] Tactic: 0x0000000000000000 Time: 0.013571
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00619805 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00567467
[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0055952
[X] Tactic: 0x00000000000003ea Time: 0.00555209
[X] Tactic: 0x0000000000000000 Time: 0.00560569
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00717481 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00555209
[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00362128
[X] Tactic: 0x00000000000003ea Time: 0.00554439
[X] Tactic: 0x0000000000000000 Time: 0.00256738
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00845179 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00256738
[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00441333
[X] Tactic: 0x00000000000003ea Time: 0.00577619
[X] Tactic: 0x0000000000000000 Time: 0.00442133
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00756672 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00441333
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00767394
[X] Tactic: 0x00000000000003ea Time: 0.00544327
[X] Tactic: 0x0000000000000000 Time: 0.00770667
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00687024 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544327
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00795226
[X] Tactic: 0x00000000000003ea Time: 0.00558365
[X] Tactic: 0x0000000000000000 Time: 0.00795124
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00665326 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558365
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00438765
[X] Tactic: 0x00000000000003ea Time: 0.00562987
[X] Tactic: 0x0000000000000000 Time: 0.00439621
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00761801 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00438765
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0130663
[X] Tactic: 0x00000000000003ea Time: 0.00561493
[X] Tactic: 0x0000000000000000 Time: 0.0130388
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00623381 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561493
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00560836
[X] Tactic: 0x00000000000003ea Time: 0.00558329
[X] Tactic: 0x0000000000000000 Time: 0.00558489
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0071343 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558329
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00357697
[X] Tactic: 0x00000000000003ea Time: 0.00561085
[X] Tactic: 0x0000000000000000 Time: 0.00257887
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00831921 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00257887
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00441698
[X] Tactic: 0x00000000000003ea Time: 0.00565173
[X] Tactic: 0x0000000000000000 Time: 0.004432
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00761573 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00441698
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00993663
[X] Tactic: 0x00000000000003ea Time: 0.0056112
[X] Tactic: 0x0000000000000000 Time: 0.00273197
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00771895 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00273197
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(25600,400:4,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00274684
[X] Tactic: 0x00000000000003ea Time: 0.00541936
[X] Tactic: 0x0000000000000000 Time: 0.00222022
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00906923 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00222022
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00421106
[X] Tactic: 0x00000000000003ea Time: 0.00547427
[X] Tactic: 0x0000000000000000 Time: 0.00428554
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00767689 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00421106
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0026055 ms
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00228237 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00354758
[X] Tactic: 0x00000000000003ea Time: 0.00551816
[X] Tactic: 0x0000000000000000 Time: 0.00233801
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00838616 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00233801
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00257026
[X] Tactic: 0x00000000000003ea Time: 0.00551991
[X] Tactic: 0x0000000000000000 Time: 0.00518449
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00781783 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00257026
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00356233
[X] Tactic: 0x00000000000003ea Time: 0.00548651
[X] Tactic: 0x0000000000000000 Time: 0.00235936
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00801677 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00235936
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00484854
[X] Tactic: 0x00000000000003ea Time: 0.00544688
[X] Tactic: 0x0000000000000000 Time: 0.00484823
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00745502 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00484823
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00439719
[X] Tactic: 0x00000000000003ea Time: 0.0056631
[X] Tactic: 0x0000000000000000 Time: 0.00517924
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00743541 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00439719
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00368293 ms
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0026682
[X] Tactic: 0x00000000000003ea Time: 0.00551065
[X] Tactic: 0x0000000000000000 Time: 0.00264281
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00855855 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00264281
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00371425
[X] Tactic: 0x00000000000003ea Time: 0.00537738
[X] Tactic: 0x0000000000000000 Time: 0.00366075
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00801042 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00366075
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00274125
[X] Tactic: 0x00000000000003ea Time: 0.00550855
[X] Tactic: 0x0000000000000000 Time: 0.00275331
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00912072 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00274125
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0112135
[X] Tactic: 0x00000000000003ea Time: 0.00591214
[X] Tactic: 0x0000000000000000 Time: 0.0111648
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00634106 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00591214
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00357742
[X] Tactic: 0x00000000000003ea Time: 0.00548232
[X] Tactic: 0x0000000000000000 Time: 0.00236913
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0080822 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00236913
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00260458
[X] Tactic: 0x00000000000003ea Time: 0.00550557
[X] Tactic: 0x0000000000000000 Time: 0.00517218
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00769839 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00260458
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00354951
[X] Tactic: 0x00000000000003ea Time: 0.00552481
[X] Tactic: 0x0000000000000000 Time: 0.00237481
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00808613 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00237481
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00489275
[X] Tactic: 0x00000000000003ea Time: 0.0054271
[X] Tactic: 0x0000000000000000 Time: 0.00489384
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00746997 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00489275
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00440224
[X] Tactic: 0x00000000000003ea Time: 0.00554859
[X] Tactic: 0x0000000000000000 Time: 0.00519089
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.007423 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00440224
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00363571
[X] Tactic: 0x00000000000003ea Time: 0.00541247
[X] Tactic: 0x0000000000000000 Time: 0.00232763
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00813533 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00232763
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00367625
[X] Tactic: 0x00000000000003ea Time: 0.00550645
[X] Tactic: 0x0000000000000000 Time: 0.00374556
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00796534 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00367625
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00270693
[X] Tactic: 0x00000000000003ea Time: 0.00550435
[X] Tactic: 0x0000000000000000 Time: 0.00268245
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00891153 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268245
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00582419
[X] Tactic: 0x00000000000003ea Time: 0.00551239
[X] Tactic: 0x0000000000000000 Time: 0.00578961
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00712649 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551239
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0109708
[X] Tactic: 0x00000000000003ea Time: 0.00588463
[X] Tactic: 0x0000000000000000 Time: 0.0108862
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00641557 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00588463
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00227773
[X] Tactic: 0x00000000000003ea Time: 0.00558382
[X] Tactic: 0x0000000000000000 Time: 0.00545239
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00793389 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00227773
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00345182
[X] Tactic: 0x00000000000003ea Time: 0.00547952
[X] Tactic: 0x0000000000000000 Time: 0.00349034
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00816642 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00345182
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00355212
[X] Tactic: 0x00000000000003ea Time: 0.00548354
[X] Tactic: 0x0000000000000000 Time: 0.00356085
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00804596 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00355212
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00483505
[X] Tactic: 0x00000000000003ea Time: 0.00548791
[X] Tactic: 0x0000000000000000 Time: 0.00485101
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00744842 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00483505
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00435297
[X] Tactic: 0x00000000000003ea Time: 0.0166227
[X] Tactic: 0x0000000000000000 Time: 0.00438695
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00703787 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00435297
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00373286
[X] Tactic: 0x00000000000003ea Time: 0.0054068
[X] Tactic: 0x0000000000000000 Time: 0.00231084
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00804658 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00231084
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00263173
[X] Tactic: 0x00000000000003ea Time: 0.00548564
[X] Tactic: 0x0000000000000000 Time: 0.00263391
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0085952 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00263173
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00375707
[X] Tactic: 0x00000000000003ea Time: 0.00539355
[X] Tactic: 0x0000000000000000 Time: 0.00373772
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0079924 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00373772
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00583228
[X] Tactic: 0x00000000000003ea Time: 0.00549351
[X] Tactic: 0x0000000000000000 Time: 0.00583743
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0071188 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00549351
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0111865
[X] Tactic: 0x00000000000003ea Time: 0.00585001
[X] Tactic: 0x0000000000000000 Time: 0.0112594
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0066814 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00585001
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00367238
[X] Tactic: 0x00000000000003ea Time: 0.00554247
[X] Tactic: 0x0000000000000000 Time: 0.00367391
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00799864 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00367238
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0026997
[X] Tactic: 0x00000000000003ea Time: 0.00548826
[X] Tactic: 0x0000000000000000 Time: 0.00269703
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00882139 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00269703
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00370415
[X] Tactic: 0x00000000000003ea Time: 0.00545961
[X] Tactic: 0x0000000000000000 Time: 0.00369207
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00800426 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00369207
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00273644
[X] Tactic: 0x00000000000003ea Time: 0.00554195
[X] Tactic: 0x0000000000000000 Time: 0.00276152
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00901583 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00273644
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0111399
[X] Tactic: 0x00000000000003ea Time: 0.00584515
[X] Tactic: 0x0000000000000000 Time: 0.0110951
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00644855 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00584515
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00273242
[X] Tactic: 0x00000000000003ea Time: 0.0159355
[X] Tactic: 0x0000000000000000 Time: 0.00550505
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00742884 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00273242
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00353875
[X] Tactic: 0x00000000000003ea Time: 0.0169803
[X] Tactic: 0x0000000000000000 Time: 0.00353729
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0074847 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00353729
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00274151
[X] Tactic: 0x00000000000003ea Time: 0.0160173
[X] Tactic: 0x0000000000000000 Time: 0.00274684
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0084014 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00274151
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00357572
[X] Tactic: 0x00000000000003ea Time: 0.0169851
[X] Tactic: 0x0000000000000000 Time: 0.00361119
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00741373 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00357572
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00481234
[X] Tactic: 0x00000000000003ea Time: 0.0168853
[X] Tactic: 0x0000000000000000 Time: 0.00482514
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00680995 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00481234
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00234018
[X] Tactic: 0x00000000000003ea Time: 0.00546013
[X] Tactic: 0x0000000000000000 Time: 0.00237428
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00873579 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00234018
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(102400,400,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00421106 ms
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0037458
[X] Tactic: 0x00000000000003ea Time: 0.00549648
[X] Tactic: 0x0000000000000000 Time: 0.00370415
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00788971 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00370415
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00527933
[X] Tactic: 0x00000000000003ea Time: 0.00553827
[X] Tactic: 0x0000000000000000 Time: 0.00526917
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00726592 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00526917
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00237874
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00294007 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00237874
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.004224
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00255429 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.004224
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00371425
[X] Tactic: 0x00000000000003ea Time: 0.00549596
[X] Tactic: 0x0000000000000000 Time: 0.00373096
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00790459 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00371425
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00530438
[X] Tactic: 0x00000000000003ea Time: 0.00546323
[X] Tactic: 0x0000000000000000 Time: 0.00531369
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00725248 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00530438
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0037414
[X] Tactic: 0x00000000000003ea Time: 0.00544275
[X] Tactic: 0x0000000000000000 Time: 0.00370309
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00792842 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00370309
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00523533
[X] Tactic: 0x00000000000003ea Time: 0.00545669
[X] Tactic: 0x0000000000000000 Time: 0.0052515
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00729232 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00523533
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00278427
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00319706 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00278427
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00432834
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00253144 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00432834
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00458813
[X] Tactic: 0x00000000000003ea Time: 0.00558702
[X] Tactic: 0x0000000000000000 Time: 0.00460449
[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00757726 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00458813
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00333515
[X] Tactic: 0x00000000000003ea Time: 0.00555751
[X] Tactic: 0x0000000000000000 Time: 0.00333339
[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00834888 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00333339
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00275375
[X] Tactic: 0x00000000000003ea Time: 0.00546571
[X] Tactic: 0x0000000000000000 Time: 0.00272425
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00911475 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00272425
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00437832
[X] Tactic: 0x00000000000003ea Time: 0.00555943
[X] Tactic: 0x0000000000000000 Time: 0.00289048
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00829233 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00289048
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.007296
[X] Tactic: 0x00000000000003ea Time: 0.00560942
[X] Tactic: 0x0000000000000000 Time: 0.00730713
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00685687 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00560942
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0095296
[X] Tactic: 0x00000000000003ea Time: 0.00560729
[X] Tactic: 0x0000000000000000 Time: 0.00948118
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00650863 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00560729
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00561085
[X] Tactic: 0x00000000000003ea Time: 0.00559502
[X] Tactic: 0x0000000000000000 Time: 0.00561725
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00715757 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559502
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00359364
[X] Tactic: 0x00000000000003ea Time: 0.00568118
[X] Tactic: 0x0000000000000000 Time: 0.00261075
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.0083827 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00261075
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0044007
[X] Tactic: 0x00000000000003ea Time: 0.00563485
[X] Tactic: 0x0000000000000000 Time: 0.00444309
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00763545 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0044007
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00964541
[X] Tactic: 0x00000000000003ea Time: 0.00559858
[X] Tactic: 0x0000000000000000 Time: 0.00273416
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00768648 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00273416
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00620563
[X] Tactic: 0x00000000000003ea Time: 0.0056144
[X] Tactic: 0x0000000000000000 Time: 0.00336754
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00765449 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00336754
[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0158628
[X] Tactic: 0x00000000000003ea Time: 0.00559822
[X] Tactic: 0x0000000000000000 Time: 0.0158954
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00613856 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559822
[X] *************** Autotuning Reformat: Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0153799
[X] Tactic: 0x00000000000003ea Time: 0.00557369
[X] Tactic: 0x0000000000000000 Time: 0.00387895
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00699951 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00387895
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0138942
[X] Tactic: 0x00000000000003ea Time: 0.0055808
[X] Tactic: 0x0000000000000000 Time: 0.0138735
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00622058 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055808
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0232412
[X] Tactic: 0x00000000000003ea Time: 0.00563502
[X] Tactic: 0x0000000000000000 Time: 0.023205
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00605038 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563502
[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00463422
[X] Tactic: 0x00000000000003ea Time: 0.00578042
[X] Tactic: 0x0000000000000000 Time: 0.0028655
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00819525 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0028655
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559822 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00387895 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563502 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00259266 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(204800,1600,40,1) -> Float(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00732661
[X] Tactic: 0x00000000000003ea Time: 0.00538323
[X] Tactic: 0x0000000000000000 Time: 0.00725751
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00680632 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00538323
[X] *************** Autotuning Reformat: Float(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00502909
[X] Tactic: 0x00000000000003ea Time: 0.00556676
[X] Tactic: 0x0000000000000000 Time: 0.00501508
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00736532 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00501508
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00538323 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00501508 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00626232
[X] Tactic: 0x00000000000003ea Time: 0.00560693
[X] Tactic: 0x0000000000000000 Time: 0.00620701
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00702239 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00560693
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00563218
[X] Tactic: 0x00000000000003ea Time: 0.00553128
[X] Tactic: 0x0000000000000000 Time: 0.00562916
[X] Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006]) profiling completed in 0.00716392 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00553128
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00441712
[X] Tactic: 0x00000000000003ea Time: 0.00566002
[X] Tactic: 0x0000000000000000 Time: 0.00439775
[X] Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006]) profiling completed in 0.00756974 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00439775
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572475 ms
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00765357
[X] Tactic: 0x00000000000003ea Time: 0.00548249
[X] Tactic: 0x0000000000000000 Time: 0.00770764
[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0068266 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00548249
[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00643836 ms
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00561707
[X] Tactic: 0x00000000000003ea Time: 0.00554142
[X] Tactic: 0x0000000000000000 Time: 0.00559378
[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00717238 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554142
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00358435
[X] Tactic: 0x00000000000003ea Time: 0.00549718
[X] Tactic: 0x0000000000000000 Time: 0.00355484
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00815241 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00355484
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00974445
[X] Tactic: 0x00000000000003ea Time: 0.00561173
[X] Tactic: 0x0000000000000000 Time: 0.00438608
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00719015 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00438608
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0204273
[X] Tactic: 0x00000000000003ea Time: 0.0062564
[X] Tactic: 0x0000000000000000 Time: 0.0204179
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00607399 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0062564
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0283698
[X] Tactic: 0x00000000000003ea Time: 0.00555401
[X] Tactic: 0x0000000000000000 Time: 0.0283253
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00599192 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00555401
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0129387
[X] Tactic: 0x00000000000003ea Time: 0.00575356
[X] Tactic: 0x0000000000000000 Time: 0.0128459
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00624533 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00575356
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00626153
[X] Tactic: 0x00000000000003ea Time: 0.00601924
[X] Tactic: 0x0000000000000000 Time: 0.00331512
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00777933 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00331512
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0101938
[X] Tactic: 0x00000000000003ea Time: 0.00579936
[X] Tactic: 0x0000000000000000 Time: 0.0101799
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00645235 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00579936
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0272952
[X] Tactic: 0x00000000000003ea Time: 0.00600933
[X] Tactic: 0x0000000000000000 Time: 0.0037458
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00695227 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0037458
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0167733
[X] Tactic: 0x00000000000003ea Time: 0.00809549
[X] Tactic: 0x0000000000000000 Time: 0.00614148
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00638243 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00614148
[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0531307
[X] Tactic: 0x00000000000003ea Time: 0.00644369
[X] Tactic: 0x0000000000000000 Time: 0.0532434
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00611173 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00644369
[X] *************** Autotuning Reformat: Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0477912
[X] Tactic: 0x00000000000003ea Time: 0.00843733
[X] Tactic: 0x0000000000000000 Time: 0.0110393
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00601874 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00843733
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.042988
[X] Tactic: 0x00000000000003ea Time: 0.00747188
[X] Tactic: 0x0000000000000000 Time: 0.0430507
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00595581 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00747188
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0795392
[X] Tactic: 0x00000000000003ea Time: 0.0083856
[X] Tactic: 0x0000000000000000 Time: 0.0795712
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00632916 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0083856
[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00968106
[X] Tactic: 0x00000000000003ea Time: 0.00895635
[X] Tactic: 0x0000000000000000 Time: 0.00422994
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.007628 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00422994
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00644369 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00843733 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0083856 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288589 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554929 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00551886 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554929 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00551886 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0067008 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0129321
[X] Tactic: 0x00000000000003ea Time: 0.00769382
[X] Tactic: 0x0000000000000000 Time: 0.0128406
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00633872 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00769382
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00627477
[X] Tactic: 0x00000000000003ea Time: 0.00748492
[X] Tactic: 0x0000000000000000 Time: 0.00330287
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00781294 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00330287
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,1:16,1280,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0102242
[X] Tactic: 0x00000000000003ea Time: 0.00696889
[X] Tactic: 0x0000000000000000 Time: 0.0102235
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.0064491 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00696889
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(409600,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00979139
[X] Tactic: 0x00000000000003ea Time: 0.00691766
[X] Tactic: 0x0000000000000000 Time: 0.00427583
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00745212 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00427583
[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0283147
[X] Tactic: 0x00000000000003ea Time: 0.00648718
[X] Tactic: 0x0000000000000000 Time: 0.0283618
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00590576 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00648718
[X] *************** Autotuning Reformat: Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0263762
[X] Tactic: 0x00000000000003ea Time: 0.00735884
[X] Tactic: 0x0000000000000000 Time: 0.0044313
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00693657 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0044313
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(409600,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.022811
[X] Tactic: 0x00000000000003ea Time: 0.00713736
[X] Tactic: 0x0000000000000000 Time: 0.022816
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00602712 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00713736
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.041613
[X] Tactic: 0x00000000000003ea Time: 0.0072589
[X] Tactic: 0x0000000000000000 Time: 0.0416059
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00609047 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0072589
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00623862
[X] Tactic: 0x00000000000003ea Time: 0.00794565
[X] Tactic: 0x0000000000000000 Time: 0.00327906
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00789788 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00327906
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00648718 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0044313 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0072589 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00555209 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00256738 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00441333 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558329 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00257887 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00441698 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00273197 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/Resize_1_output_0 copy
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00274177
[X] Tactic: 0x00000000000003ea Time: 0.00597486
[X] Tactic: 0x0000000000000000 Time: 0.00273556
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00912611 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00273556
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00438849
[X] Tactic: 0x00000000000003ea Time: 0.00556551
[X] Tactic: 0x0000000000000000 Time: 0.00285083
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00834214 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00285083
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0073127
[X] Tactic: 0x00000000000003ea Time: 0.00559076
[X] Tactic: 0x0000000000000000 Time: 0.0072953
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00690738 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559076
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00933244
[X] Tactic: 0x00000000000003ea Time: 0.00564178
[X] Tactic: 0x0000000000000000 Time: 0.00928948
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00649865 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00564178
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00770909
[X] Tactic: 0x00000000000003ea Time: 0.00550453
[X] Tactic: 0x0000000000000000 Time: 0.00768267
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00685139 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00550453
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00794717
[X] Tactic: 0x00000000000003ea Time: 0.00571878
[X] Tactic: 0x0000000000000000 Time: 0.00790995
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00672965 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00571878
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00438765
[X] Tactic: 0x00000000000003ea Time: 0.00560836
[X] Tactic: 0x0000000000000000 Time: 0.00441712
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00760296 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00438765
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0129895
[X] Tactic: 0x00000000000003ea Time: 0.00551257
[X] Tactic: 0x0000000000000000 Time: 0.0129957
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00633024 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551257
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00560693
[X] Tactic: 0x00000000000003ea Time: 0.00543037
[X] Tactic: 0x0000000000000000 Time: 0.00557405
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00723388 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00543037
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00358939
[X] Tactic: 0x00000000000003ea Time: 0.00552761
[X] Tactic: 0x0000000000000000 Time: 0.00257764
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00883783 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00257764
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00443214
[X] Tactic: 0x00000000000003ea Time: 0.00552499
[X] Tactic: 0x0000000000000000 Time: 0.00444112
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00769281 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00443214
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00988957
[X] Tactic: 0x00000000000003ea Time: 0.00556302
[X] Tactic: 0x0000000000000000 Time: 0.0027492
[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00767669 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0027492
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00336754 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559822 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00387895 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055808 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563502 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0028655 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559822 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00387895 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563502 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00259266 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00538323 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00501508 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00538323 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00501508 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00560693 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) [Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00553128 ms
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0035419
[X] Tactic: 0x00000000000003ea Time: 0.00563787
[X] Tactic: 0x0000000000000000 Time: 0.00258828
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.0084023 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00258828
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x0000000000000000, 0.00439775 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00285137 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572475 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00301146 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584111 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00643836 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261508 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572475 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00301146 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00643836 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(204800,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0033487
[X] Tactic: 0x00000000000003ea Time: 0.00544929
[X] Tactic: 0x0000000000000000 Time: 0.003344
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00827763 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.003344
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00286551
[X] Tactic: 0x00000000000003ea Time: 0.00562436
[X] Tactic: 0x0000000000000000 Time: 0.00225578
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.008734 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00225578
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00310331
[X] Tactic: 0x00000000000003ea Time: 0.00548546
[X] Tactic: 0x0000000000000000 Time: 0.00310479
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00835336 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00310331
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(204800,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00339028
[X] Tactic: 0x00000000000003ea Time: 0.00548774
[X] Tactic: 0x0000000000000000 Time: 0.00339232
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00821032 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00339028
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00287043
[X] Tactic: 0x00000000000003ea Time: 0.0055061
[X] Tactic: 0x0000000000000000 Time: 0.00229187
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00850649 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00229187
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00310114
[X] Tactic: 0x00000000000003ea Time: 0.00545893
[X] Tactic: 0x0000000000000000 Time: 0.00312314
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00851296 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00310114
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00455164
[X] Tactic: 0x00000000000003ea Time: 0.00556071
[X] Tactic: 0x0000000000000000 Time: 0.0024618
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00761061 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0024618
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00235786
[X] Tactic: 0x00000000000003ea Time: 0.00541763
[X] Tactic: 0x0000000000000000 Time: 0.00235883
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00829852 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00235786
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00275888
[X] Tactic: 0x00000000000003ea Time: 0.00538219
[X] Tactic: 0x0000000000000000 Time: 0.00223602
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00893937 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00223602
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00375023
[X] Tactic: 0x00000000000003ea Time: 0.00534773
[X] Tactic: 0x0000000000000000 Time: 0.00374033
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00792317 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00374033
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00433344
[X] Tactic: 0x00000000000003ea Time: 0.00540181
[X] Tactic: 0x0000000000000000 Time: 0.00431836
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00757875 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00431836
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00373322
[X] Tactic: 0x00000000000003ea Time: 0.00542882
[X] Tactic: 0x0000000000000000 Time: 0.00373488
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00787192 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00373322
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0038449
[X] Tactic: 0x00000000000003ea Time: 0.00540301
[X] Tactic: 0x0000000000000000 Time: 0.00382812
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00788098 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00382812
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00292987
[X] Tactic: 0x00000000000003ea Time: 0.00545308
[X] Tactic: 0x0000000000000000 Time: 0.00294073
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0085776 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00292987
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00531742
[X] Tactic: 0x00000000000003ea Time: 0.00539493
[X] Tactic: 0x0000000000000000 Time: 0.00535653
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00718997 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00531742
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00235117
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00274278 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00235117
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00277642
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00313231 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00277642
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00372729
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00267004 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00372729
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00434978
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.002502 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00434978
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00374412
[X] Tactic: 0x00000000000003ea Time: 0.00540938
[X] Tactic: 0x0000000000000000 Time: 0.00372539
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00790148 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00372539
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00385435
[X] Tactic: 0x00000000000003ea Time: 0.00544929
[X] Tactic: 0x0000000000000000 Time: 0.00385471
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00790278 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00385435
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00296505
[X] Tactic: 0x00000000000003ea Time: 0.00553058
[X] Tactic: 0x0000000000000000 Time: 0.00295646
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00870532 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00295646
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00536212
[X] Tactic: 0x00000000000003ea Time: 0.00549753
[X] Tactic: 0x0000000000000000 Time: 0.00538306
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00721003 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00536212
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00371556
[X] Tactic: 0x00000000000003ea Time: 0.00555947
[X] Tactic: 0x0000000000000000 Time: 0.00371674
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00780923 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00371556
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00384932
[X] Tactic: 0x00000000000003ea Time: 0.00544946
[X] Tactic: 0x0000000000000000 Time: 0.00382194
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00792368 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00382194
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00297185
[X] Tactic: 0x00000000000003ea Time: 0.00539355
[X] Tactic: 0x0000000000000000 Time: 0.00298733
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00841485 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00297185
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00535026
[X] Tactic: 0x00000000000003ea Time: 0.00547672
[X] Tactic: 0x0000000000000000 Time: 0.00532301
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00718164 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00532301
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0027824
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0031501 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0027824
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00279307
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0032416 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00279307
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00379273
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00263774 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00379273
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0043423
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00249564 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0043423
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0025987 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00555156 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00293193 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00562098 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563325 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00244586 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00555156 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00293193 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563325 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(1600,400:32,20,1) -> Int8(12800,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00273041
[X] Tactic: 0x00000000000003ea Time: 0.0054443
[X] Tactic: 0x0000000000000000 Time: 0.00212003
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00901551 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00212003
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(51200,400,20,1) -> Float(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.003485
[X] Tactic: 0x00000000000003ea Time: 0.0054692
[X] Tactic: 0x0000000000000000 Time: 0.0035099
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00807537 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.003485
[X] *************** Autotuning Reformat: Float(1600,400:32,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00300992
[X] Tactic: 0x00000000000003ea Time: 0.005288
[X] Tactic: 0x0000000000000000 Time: 0.00299733
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00871524 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00299733
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(51200,400,20,1) -> Float(1600,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.003485 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(1600,400:32,20,1) -> Float(51200,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299733 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,400,20,1) -> Int8(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00374676
[X] Tactic: 0x00000000000003ea Time: 0.00548127
[X] Tactic: 0x0000000000000000 Time: 0.00369395
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00796544 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00369395
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00228237 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0218867
[X] Tactic: 0x00000000000003ea Time: 0.00677098
[X] Tactic: 0x0000000000000000 Time: 0.0219087
[X] Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00596771 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00677098
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/decoder/input_proj.1/conv/Conv_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554562 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/Conv_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00368293 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] Formats and tactics selection completed in 28.068 seconds.
[X] After reformat layers: 85 layers
[X] Total number of blocks in pre-optimized block assignment: 81
[V] Detected 2 inputs and 9 output network tensors.
[X] Layer: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Host Persistent: 4880 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: /model/backbone/MaxPool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Host Persistent: 4944 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 13107200 bytes
[X] Layer: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Host Persistent: 308 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 54067200 bytes
[X] Skipped printing memory information for 17 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.
[V] Total Host Persistent Memory: 307312 bytes
[V] Total Device Persistent Memory: 0 bytes
[V] Max Scratch Memory: 54067200 bytes
[V] [BlockAssignment] Started assigning block shifts. This will take 82 steps to complete.
[X] STILL ALIVE: Started step 76 of 82
[V] [BlockAssignment] Algorithm ShiftNTopDown took 1.21038ms to assign 6 blocks to 82 nodes requiring 63129600 bytes.
[X] Total number of blocks in optimized block assignment: 6
[V] Total Activation Memory: 63129600 bytes
[V] Total Weights Memory: 23931268 bytes
[X] Finalize: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Set kernel index: 0
[X] Finalize: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Set kernel index: 1
[X] Finalize: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Set kernel index: 2
[X] Finalize: /model/backbone/MaxPool Set kernel index: 3
[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Set kernel index: 2
[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Set kernel index: 4
[X] Finalize: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Set kernel index: 5
[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Set kernel index: 2
[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Set kernel index: 4
[X] Finalize: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Set kernel index: 6
[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Set kernel index: 7
[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Set kernel index: 4
[X] Finalize: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Set kernel index: 5
[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Set kernel index: 8
[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Set kernel index: 9
[X] Finalize: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Set kernel index: 6
[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Set kernel index: 10
[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Set kernel index: 11
[X] Finalize: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Set kernel index: 12
[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Set kernel index: 10
[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Set kernel index: 11
[X] Finalize: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Set kernel index: 6
[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Set kernel index: 13
[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Set kernel index: 14
[X] Finalize: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Set kernel index: 15
[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Set kernel index: 16
[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Set kernel index: 14
[X] Finalize: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Set kernel index: 17
[X] Finalize: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Set kernel index: 18
[X] Finalize: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Set kernel index: 19
[X] Finalize: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Set kernel index: 17
[X] Finalize: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Set kernel index: 20
[X] Finalize: /model/encoder/Resize Set kernel index: 21
[X] Finalize: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Set kernel index: 22
[X] Finalize: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Set kernel index: 23
[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 24
[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 24
[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 14
[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Set kernel index: 25
[X] Finalize: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Set kernel index: 26
[X] Finalize: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Set kernel index: 23
[X] Finalize: /model/encoder/Resize_1 Set kernel index: 21
[X] Finalize: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Set kernel index: 27
[X] Finalize: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Set kernel index: 28
[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 29
[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 29
[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 11
[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Set kernel index: 30
[X] Finalize: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Set kernel index: 31
[X] Finalize: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Set kernel index: 5
[X] Finalize: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Set kernel index: 32
[X] Finalize: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Set kernel index: 22
[X] Finalize: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Set kernel index: 23
[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 24
[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 24
[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 14
[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Set kernel index: 25
[X] Finalize: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Set kernel index: 26
[X] Finalize: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Set kernel index: 5
[X] Finalize: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Set kernel index: 33
[X] Finalize: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Set kernel index: 12
[X] Finalize: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Set kernel index: 34
[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 33
[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 33
[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 35
[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Set kernel index: 36
[X] Finalize: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Set kernel index: 37
[X] Finalize: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Set kernel index: 17
[X] Total number of generated kernels selected for the engine: 38
[X] Kernel: 0 CASK_STATIC
[X] Kernel: 1 CASK_STATIC
[X] Kernel: 2 CASK_STATIC
[X] Kernel: 3 CASK_STATIC
[X] Kernel: 4 CASK_STATIC
[X] Kernel: 5 CASK_STATIC
[X] Kernel: 6 CASK_STATIC
[X] Kernel: 7 CASK_STATIC
[X] Kernel: 8 CASK_STATIC
[X] Kernel: 9 CASK_STATIC
[X] Kernel: 10 CASK_STATIC
[X] Kernel: 11 CASK_STATIC
[X] Kernel: 12 CASK_STATIC
[X] Kernel: 13 CASK_STATIC
[X] Kernel: 14 CASK_STATIC
[X] Kernel: 15 CASK_STATIC
[X] Kernel: 16 CASK_STATIC
[X] Kernel: 17 CASK_STATIC
[X] Kernel: 18 CASK_STATIC
[X] Kernel: 19 CASK_STATIC
[X] Kernel: 20 TRT_SERIALIZABLE:generatedNativePointwise
[X] Kernel: 21 TRT_SERIALIZABLE:ResizeVectorizedC4x4NearestKernel
[X] Kernel: 22 CASK_STATIC
[X] Kernel: 23 CASK_STATIC
[X] Kernel: 24 CASK_STATIC
[X] Kernel: 25 TRT_SERIALIZABLE:generatedNativePointwise
[X] Kernel: 26 CASK_STATIC
[X] Kernel: 27 CASK_STATIC
[X] Kernel: 28 CASK_STATIC
[X] Kernel: 29 CASK_STATIC
[X] Kernel: 30 TRT_SERIALIZABLE:generatedNativePointwise
[X] Kernel: 31 CASK_STATIC
[X] Kernel: 32 CASK_STATIC
[X] Kernel: 33 CASK_STATIC
[X] Kernel: 34 CASK_STATIC
[X] Kernel: 35 CASK_STATIC
[X] Kernel: 36 TRT_SERIALIZABLE:generatedNativePointwise
[X] Kernel: 37 CASK_STATIC
[V] Compiler backend is used during engine execution.
[X] Disabling unused tactic source: JIT_CONVOLUTIONS
[V] Engine generation completed in 29.0165 seconds.
[X] Layers:
    Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: images, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 864}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8, TacticValue: 0x5cc792a989a1d1a6, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_1/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 9216}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3, TacticValue: 0x13463e9bf9ae0d73, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_2/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 18432}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_3/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear]
    Name: /model/backbone/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/MaxPool]
    Name: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0x23b890da05937b9e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 4096}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0x23b890da05937b9e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 73728}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x705baf38e41eee0b, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0x23b890da05937b9e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 8192}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x214f03e23f252333, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0xa8b56a226b057463, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 294912}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 1179648}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32, TacticValue: 0x322f337abc345152, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x65fbe45b4cb1d8a5, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x1d53511430a5d47e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/act/Relu]
    Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye9030_0_myl37_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddResTra_myl37_1, LayerType: kgen, Inputs: [ { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_MulAddResTra_0x862813689358e08ec79eab32f31fafdf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/encoder/Reshape][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1]
    Name: __mye8947_myl37_2, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_TraAdd_myl37_3, LayerType: kgen, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Constant_output_0_constantFloat, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_TraAdd_0x5a9388c92c5b2a167638420a28fa3cf0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Transpose][ONNX Layer: /model/encoder/encoder.0/layers.0/Add]
    Name: __mye8949_myl37_4, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_2_myl37_5, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8849dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8277/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8278/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8626_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2]
    Name: __mye8951_myl37_6, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_1+/model/encoder/encoder_0/layers_0/self_attn/MatMul_myl37_7, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8854dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye8319/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8320/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8774_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye8684, Dimensions: [2,400,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add]
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_3_myl37_8, LayerType: gemm, Inputs: [ { Name: __mye8684, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8684, Dimensions: [8,32,400], Format/Datatype: Float }, { Name: __mye8642, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye8333/model/encoder/encoder_0/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x128x16_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl37_9, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x486901888507314d28178a529899ff30, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax]
    Name: __mye8953_myl37_10, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_myl37_11, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8343/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8344/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl37_12, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0x053154cc4b930530fcf23b0caf04c63a, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3]
    Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_myl37_13, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8859dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8357/model/encoder/encoder_0/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8358/model/encoder/encoder_0/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_116_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl37_14, LayerType: kgen, Inputs: [ { Name: __mye8585_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8575_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye9026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x81c6f38dc18b20647aef42cb9b16a94b, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/encoder/encoder_0/layers_0/linear1/MatMul_myl37_15, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Int8 }, { Name: __mye8864dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye8646_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye8653zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear1_bias _ ONNXTRT_Broadcast_131_constantFloat, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_gelu_erf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Div][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Erf][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl37_16, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }, { Name: __mye8869dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye8657_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8664zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear2_bias _ ONNXTRT_Broadcast_145_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/Add_2]
    Name: __myl_ResMeaSubMulMea_myl37_17, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMea_0xade3a566ff3432c2f2753f66a7f593a6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization]
    Name: __myl_AddSqrDivMulMulAddResTra_myl37_18, LayerType: kgen, Inputs: [ { Name: __mye8539_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8549_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Reshape_1_output_0, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_AddSqrDivMulMulAddResTra_0x0caebe133d43683f5670c896620d9227, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization][ONNX Layer: /model/encoder/Transpose_1]
    [ONNX Layer: /model/encoder/Reshape_1]
    Name: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x5e4918ccf433630e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x483ad1560c6e5e27, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Reshape_1_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.0/act/Mul]
    Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    Name: /model/encoder/Resize, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize]
    Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_2]
    Name: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000015, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish, TacticValue: 0x4a25dfdaea3c22a0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/Resize_1, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize_1]
    Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_3]
    Name: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x7720f198395e7d3d, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x000000000000001a, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x65a38dbc9e991257, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/Resize_1_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_4]
    Name: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000015, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/Add]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish, TacticValue: 0x4a25dfdaea3c22a0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish, TacticValue: 0x44824770683c7b80, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xd14bd6d95fefd45e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/Add]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye157990_0_myl84_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^signal^1_myl84_1, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^wait^1_myl84_2, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl84_3, LayerType: kgen, Inputs: [ { Name: __mye155438_dconst, Dimensions: [1,1,6400], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xb7911a963641d99b9b7644b75b6b02a0, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulMinMaxRouCasResTra_myl84_4, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: __mye157890_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x53ec280dcdcbc7be42089db5a99e26ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl84_5, LayerType: kgen, Inputs: [ { Name: __mye155461_dconst, Dimensions: [1,1,1600], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xc7826108fa2ff5e34bf8bfa07dbc52f7, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/input_proj.1/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __myl_MulMinMaxRouCasResTra_myl84_6, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: __mye157890_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x8592f20b4eb6c9ee9a9e56f44ec5871e, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __mye157314_myl84_7, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __mye157316_myl84_8, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_myl84_9, LayerType: kgen, Inputs: [ { Name: __mye157890_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __mye155484_dconst, Dimensions: [1,1,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __mye153891_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_0x14d97ab92d57b85a1bd3815e99f6e152, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Concat_3][ONNX Layer: /model/decoder/Reshape_2]
    [ONNX Layer: /model/decoder/Transpose_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear]
    Name: __mye157318_myl84_10, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157320_myl84_11, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_1/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_0/cross_attn/value_proj/MatMul_myl84_12, LayerType: gemm, Inputs: [ { Name: __mye153891_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156133dconst, Dimensions: [3,256,256], Format/Datatype: Int8 }, { Name: __mye153915_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye153936_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye154934_dconst, Dimensions: [3,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153891, Dimensions: [3,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add]
    Name: /model/decoder/enc_output/proj/MatMul_myl84_13, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156138dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153194_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153201zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_proj_bias _ ONNXTRT_Broadcast_275_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/proj/MatMul][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/Add]
    Name: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_14, LayerType: kgen, Inputs: [ { Name: model_decoder_enc_output_norm_weight _ ONNXTRT_Broadcast_279_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_norm_bias _ ONNXTRT_Broadcast_281_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: __mye157900_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0xf1c80ff651c1b506b1815818d6281ad3, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/norm/LayerNormalization][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear]
    Name: __mye157322_myl84_15, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157324_myl84_16, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_score_head/MatMul_myl84_17, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156148dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153232_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153239zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_enc_score_head_bias _ ONNXTRT_Broadcast_289_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/enc_score_head/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/Add]
    Name: __myl_Max_myl84_18, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400,1], Format/Datatype: Float }], TacticName: __myl_Max_0x4330a02939b906fc5f8c1bd769456467, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/ReduceMax]
    Name: __myl_Top_myl84_19, LayerType: kgen, Inputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/TopK_output_0'.1, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x7e62297dffa2e596ee60049838a70f81, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/TopK]
    Name: __mye157326_myl84_20, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_bbox_head/layers_0/MatMul_myl84_21, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157388_xformed___mye156143dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157396_xformed___mye153216_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153212zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157392_xformed___mye153225_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.0/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/Add]
    Name: /model/decoder/enc_bbox_head/layers_1/MatMul_myl84_22, LayerType: gemm, Inputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157400_xformed___mye156153dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157408_xformed___mye153254_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153250zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157404_xformed___mye153263_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.1/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act_1/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/Add]
    Name: __myl_FcAdd_myl84_23, LayerType: fusion, Inputs: [ { Name: model_decoder_anchors_constantFloat, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156163dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153270_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153277zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_enc_bbox_head_layers_2_bias _ ONNXTRT_Broadcast_311_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.2/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/Add][ONNX Layer: /model/decoder/Add]
    Name: __mye157328_myl84_24, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_myl84_25, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __mye157904_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], Outputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,4], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], TacticName: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_0xea994e8a02766a6b87cc77a0ab1bb663, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/Unsqueeze][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Sigmoid][ONNX Layer: /model/decoder/GatherElements]
    Name: __myl_MovCon_myl84_26, LayerType: kgen, Inputs: [ { Name: __mye156485, Dimensions: [1,300,12], Format/Datatype: Int8 }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,4], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MovCon_0x9482c2d60923b5d68d1030431d0b6d2e, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_0/MatMul_myl84_27, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156499_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153292_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153288zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153301_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1/MatMul_myl84_28, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156173dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153308_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153315zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye149975_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/Add]
    Name: __myl_RepGatResAdd_myl84_29, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], Outputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_RepGatResAdd_0x3585782c9d9cf8f0d2b18744e46affde, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/GatherElements_1][ONNX Layer: /model/decoder/decoder/layers.0/Add]
    Name: __mye157330_myl84_30, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157332_myl84_31, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_2_myl84_32, LayerType: gemm, Inputs: [ { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156158dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149287/model/decoder/decoder/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149288/model/decoder/decoder/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye153089_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_2]
    Name: __mye157334_myl84_33, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_1+/model/decoder/decoder/layers_0/self_attn/MatMul_myl84_34, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156178dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149341/model/decoder/decoder/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149342/model/decoder/decoder/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155184_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153876, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add]
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_3_myl84_35, LayerType: gemm, Inputs: [ { Name: __mye153876, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153876, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153149, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149376/model/decoder/decoder/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_36, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Softmax]
    Name: __mye157336_myl84_37, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_myl84_38, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149386/model/decoder/decoder/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149387/model/decoder/decoder/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_39, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_0/self_attn/Gemm_myl84_40, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156183dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149400/model/decoder/decoder/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149401/model/decoder/decoder/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_351_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_41, LayerType: kgen, Inputs: [ { Name: __mye152985_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152975_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157908_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x91b2c7046943674462a660380f1917c4, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/Add_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157338_myl84_42, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157340_myl84_43, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/cross_attn/attention_weights/MatMul_myl84_44, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156188dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153319_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153326zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_383_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add]
    Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/MatMul_myl84_45, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156193dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153330_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153337zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_375_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add]
    Name: __myl_MaxSubExpSum_myl84_46, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_44, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_43, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax]
    Name: __mye157342_myl84_47, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl84_48, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18222, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18237, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18252, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18267, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18446, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18461, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18476, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18491, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18670, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18685, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18700, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18715, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150465_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150475_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150485_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150579, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150575, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye149996_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_47, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_46, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_45, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0xf96d9f98493dda1d394fe8f1b3a4a64a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8]
    Name: __mye157344_myl84_49, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_50, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_44, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157922_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_46, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_45, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_47, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_43, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150954_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_51, LayerType: kgen, Inputs: [ { Name: __mye150954_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_49, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_52, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_49, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155354_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153341_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153348zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_577_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_50, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_53, LayerType: kgen, Inputs: [ { Name: __mye152940_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152930_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157926_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_50, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_52, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_0/linear1/MatMul_myl84_54, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157412_xformed___mye156198dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153363_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153359zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153372_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_53, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl84_55, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_52, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_53, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156203dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153379_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153386zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_linear2_bias _ ONNXTRT_Broadcast_599_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_54, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_56, LayerType: kgen, Inputs: [ { Name: __mye152904_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152890_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157930_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_54, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157346_myl84_57, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157348_myl84_58, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_2_myl84_59, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156213dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149485/model/decoder/decoder/layers_1/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149486/model/decoder/decoder/layers_1/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152875_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_2]
    Name: __mye157350_myl84_60, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/MatMul_myl84_61, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157416_xformed___mye156208dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153401_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153397zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153410_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_58, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_1/MatMul_myl84_62, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_58, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157420_xformed___mye156218dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153428_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153424zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153437_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_59, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/MatMul_myl84_63, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_59, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156223dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153444_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153451zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_0_layers_2_bias _ ONNXTRT_Broadcast_629_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_60, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add]
    Name: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_myl84_64, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_60, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156506, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye157904_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_0xefac8e563c6580f9cd110df4750663ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log][ONNX Layer: /model/decoder/decoder/Sigmoid_1][ONNX Layer: /model/decoder/decoder/Add][ONNX Layer: /model/decoder/decoder/Div][ONNX Layer: /model/decoder/decoder/Sub][ONNX Layer: /model/decoder/decoder/Clip]
    Name: /model/decoder/decoder/query_pos_head/layers_0_1/MatMul_myl84_65, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156522_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153466_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153462zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153475_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_63, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_1/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_1/MatMul_myl84_66, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_63, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156233dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153482_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153489zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150074_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add]
    Name: __myl_Add_myl84_67, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_1+/model/decoder/decoder/layers_1/self_attn/MatMul_myl84_68, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156238dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149550/model/decoder/decoder/layers_1/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149551/model/decoder/decoder/layers_1/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155194_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153853, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_3_myl84_69, LayerType: gemm, Inputs: [ { Name: __mye153853, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153853, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153153, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149585/model/decoder/decoder/layers_1/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_67, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_70, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_67, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_67, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_68, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Softmax]
    Name: __mye157352_myl84_71, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_myl84_72, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_68, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149595/model/decoder/decoder/layers_1/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149596/model/decoder/decoder/layers_1/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_69, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_73, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_69, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_1/self_attn/Gemm_myl84_74, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156243dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149609/model/decoder/decoder/layers_1/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149610/model/decoder/decoder/layers_1/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_self_attn_out_proj_bias _ ONNXTRT_Broadcast_674_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_75, LayerType: kgen, Inputs: [ { Name: __mye152825_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152815_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157937_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_73, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157354_myl84_76, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157356_myl84_77, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_1/cross_attn/attention_weights/MatMul_myl84_78, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156248dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153493_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153500zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_704_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add]
    Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/MatMul_myl84_79, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156253dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153504_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153511zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_696_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add]
    Name: __myl_MaxSubExpSum_myl84_80, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_77, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_76, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax]
    Name: __mye157358_myl84_81, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl84_82, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18921, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18936, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18951, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18966, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19145, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19160, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19175, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19190, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19369, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19384, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19399, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19414, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150495_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150505_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150515_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150607, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150603, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye150095_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_80, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_79, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_78, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0x11b440b78a8fa74f33eb0bb614ceee80, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8]
    Name: __mye157360_myl84_83, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_84, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_77, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157950_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_79, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_78, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_80, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_76, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150960_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_85, LayerType: kgen, Inputs: [ { Name: __mye150960_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_82, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_86, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_73, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_82, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155300_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153515_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153522zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_900_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_83, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_87, LayerType: kgen, Inputs: [ { Name: __mye152780_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152770_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157954_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_83, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_85, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_1/linear1/MatMul_myl84_88, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157424_xformed___mye156258dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153537_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153533zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153546_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_86, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.1/linear1/Add]
    Name: __myl_FcAdd_myl84_89, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_85, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_86, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156263dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153553_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153560zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_linear2_bias _ ONNXTRT_Broadcast_922_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_87, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_90, LayerType: kgen, Inputs: [ { Name: __mye152744_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152730_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157958_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_87, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157362_myl84_91, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157364_myl84_92, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_2_myl84_93, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156273dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149694/model/decoder/decoder/layers_2/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149695/model/decoder/decoder/layers_2/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152715_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_2]
    Name: __mye157366_myl84_94, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/MatMul_myl84_95, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157428_xformed___mye156268dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153575_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153571zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153584_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_91, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_1/MatMul_myl84_96, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_91, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157432_xformed___mye156278dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153602_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153598zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153611_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_92, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/MatMul_myl84_97, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_92, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156283dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153618_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153625zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_1_layers_2_bias _ ONNXTRT_Broadcast_952_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_93, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add]
    Name: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_myl84_98, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_93, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156529, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye157904_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_95, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_94, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_0xa06819df43d11e9f71ec4d6314dfc9b2, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log_1][ONNX Layer: /model/decoder/decoder/Add_1][ONNX Layer: /model/decoder/decoder/Sigmoid_2][ONNX Layer: /model/decoder/decoder/Div_1][ONNX Layer: /model/decoder/decoder/Sub_1][ONNX Layer: /model/decoder/decoder/Clip_3]
    Name: /model/decoder/decoder/query_pos_head/layers_0_2/MatMul_myl84_99, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_94, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156545_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153640_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153636zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153649_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_96, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_2/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_2/MatMul_myl84_100, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_96, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156293dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153656_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153663zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150169_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add]
    Name: __myl_Add_myl84_101, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_1+/model/decoder/decoder/layers_2/self_attn/MatMul_myl84_102, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156298dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149759/model/decoder/decoder/layers_2/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149760/model/decoder/decoder/layers_2/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155204_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153830, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_3_myl84_103, LayerType: gemm, Inputs: [ { Name: __mye153830, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153830, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153157, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149794/model/decoder/decoder/layers_2/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_100, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_104, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_100, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_100, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_101, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Softmax]
    Name: __mye157368_myl84_105, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_myl84_106, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_101, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149804/model/decoder/decoder/layers_2/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149805/model/decoder/decoder/layers_2/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_102, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_107, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_102, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_2/self_attn/Gemm_myl84_108, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156303dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149818/model/decoder/decoder/layers_2/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149819/model/decoder/decoder/layers_2/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_self_attn_out_proj_bias _ ONNXTRT_Broadcast_997_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_109, LayerType: kgen, Inputs: [ { Name: __mye152665_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152655_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157965_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_106, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157370_myl84_110, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157372_myl84_111, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/cross_attn/attention_weights/MatMul_myl84_112, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156308dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153667_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153674zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_1027_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add]
    Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/MatMul_myl84_113, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156313dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153678_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153685zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_1019_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add]
    Name: __myl_MaxSubExpSum_myl84_114, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_110, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_109, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax]
    Name: __mye157374_myl84_115, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl84_116, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_95, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19620, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19635, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19650, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19665, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19844, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19859, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19874, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19889, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20068, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20083, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20098, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20113, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150525_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150535_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150545_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157912_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150635, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150631, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye150190_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_113, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_112, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_111, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0x11b440b78a8fa74f33eb0bb614ceee80, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8]
    Name: __mye157376_myl84_117, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_118, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_110, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157978_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_112, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_111, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_113, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_109, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150966_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_119, LayerType: kgen, Inputs: [ { Name: __mye150966_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_115, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_120, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_106, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_115, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155246_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153689_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153696zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_1223_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_116, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_121, LayerType: kgen, Inputs: [ { Name: __mye152620_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152610_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157982_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_116, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_118, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_2/linear1/MatMul_myl84_122, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157436_xformed___mye156318dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153711_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153707zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153720_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_119, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.2/linear1/Add]
    Name: __myl_FcAdd_myl84_123, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_118, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_119, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156323dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153727_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153734zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_linear2_bias _ ONNXTRT_Broadcast_1245_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_120, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_124, LayerType: kgen, Inputs: [ { Name: __mye152584_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157986_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152578_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_120, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x3f53c92c8e85fb99f9934c06da28da1c, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157378_myl84_125, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157380_myl84_126, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_score_head_2/MatMul_myl84_127, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156328dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153738_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153745zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_dec_score_head_2_bias _ ONNXTRT_Broadcast_1293_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,300,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/dec_score_head.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/Add]
    Name: __myl_GatResNegExpAddDivRes_myl84_128, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,1,300,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_123, Dimensions: [1,24000], Format/Datatype: Float }], TacticName: __myl_GatResNegExpAddDivRes_0x1d563258c32f843400fb4233ccab3fa6, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/Sigmoid][ONNX Layer: /postprocessor/Flatten][ONNX Layer: /model/decoder/Gather_8]
    Name: __myl_Top_myl84_129, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_123, Dimensions: [1,24000], Format/Datatype: Float }], Outputs: [ { Name: scores, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_125, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x1c85ccd1fad109f046189f0d3e8dff44, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/TopK]
    Name: __mye157382_myl84_130, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/MatMul_myl84_131, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157440_xformed___mye156333dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153760_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153756zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153769_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_126, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_1/MatMul_myl84_132, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_126, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157444_xformed___mye156338dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153787_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153783zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153796_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_127, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/MatMul_myl84_133, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_127, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156343dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153803_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153810zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_2_layers_2_bias _ ONNXTRT_Broadcast_1275_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_128, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add]
    Name: __mye157384_myl84_134, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_myl84_135, LayerType: kgen, Inputs: [ { Name: orig_target_sizes, Dimensions: [1,2], Format/Datatype: Int64 }, { Name: __myln_k_arg__bb1_125, Dimensions: [1,300], Format/Datatype: Int32 }, { Name: __mye150647, Dimensions: [1,1], Format/Datatype: Int64 }, { Name: __mye150651, Dimensions: [1,1], Format/Datatype: Float }, { Name: __mye150655, Dimensions: [1,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_128, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_95, Dimensions: [1,300,4], Format/Datatype: Float }], Outputs: [ { Name: labels, Dimensions: [1,300], Format/Datatype: Int64 }, { Name: boxes, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_0x046287ea34a14bdbbd780dcf069cdb4a, StreamId: 0, Metadata: [ONNX Layer: Cast_3039][ONNX Layer: /model/decoder/decoder/Clip_6][ONNX Layer: /model/decoder/decoder/Sub_2][ONNX Layer: /model/decoder/decoder/Div_2][ONNX Layer: /model/decoder/decoder/Sigmoid_3][ONNX Layer: /model/decoder/Gather_9][ONNX Layer: /model/decoder/decoder/Unsqueeze_3][ONNX Layer: /model/decoder/decoder/Add_2][ONNX Layer: /model/decoder/decoder/Log_2][ONNX Layer: /postprocessor/Split][ONNX Layer: /postprocessor/Squeeze_1][ONNX Layer: /postprocessor/Squeeze_2][ONNX Layer: /postprocessor/Mul][ONNX Layer: /postprocessor/Add][ONNX Layer: /postprocessor/Unsqueeze_2][ONNX Layer: /postprocessor/Sub][ONNX Layer: /postprocessor/Unsqueeze][ONNX Layer: /postprocessor/Concat][ONNX Layer: /postprocessor/Mul_2][ONNX Layer: /postprocessor/GatherElements][ONNX Layer: /postprocessor/Unsqueeze_5][ONNX Layer: /postprocessor/Unsqueeze_3][ONNX Layer: /postprocessor/Add_1][ONNX Layer: /postprocessor/Squeeze][ONNX Layer: /postprocessor/Unsqueeze_1][ONNX Layer: /postprocessor/Sub_1][ONNX Layer: /postprocessor/Mul_1][ONNX Layer: /postprocessor/Squeeze_3][ONNX Layer: /postprocessor/Mul_3][ONNX Layer: /postprocessor/Sub_2][ONNX Layer: /postprocessor/Div][ONNX Layer: /postprocessor/Unsqueeze_4][ONNX Layer: /postprocessor/Tile]
    
    Bindings:
    images
    orig_target_sizes
    /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
    /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
    /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
    /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
    /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
    /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
    labels
    boxes
    scores
[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 67 MiB
[X] Adding 1 engine(s) to plan file.
[X] Adding 1 engine weights(s) to plan file.
[I] Finished engine building in 29.159 seconds
[V] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.
[X] Plugin creator already registered - ::ROIAlign_TRT version 2
[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1
[X] Plugin creator already registered - ::BatchedNMS_TRT version 1
[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1
[X] Plugin creator already registered - ::Clip_TRT version 1
[X] Plugin creator already registered - ::CoordConvAC version 1
[X] Plugin creator already registered - ::CropAndResizeDynamic version 1
[X] Plugin creator already registered - ::CropAndResize version 1
[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1
[X] Plugin creator already registered - ::DetectionLayer_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_TRT version 1
[X] Plugin creator already registered - ::FlattenConcat_TRT version 1
[X] Plugin creator already registered - ::GenerateDetection_TRT version 1
[X] Plugin creator already registered - ::GridAnchor_TRT version 1
[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 3
[X] Plugin creator already registered - ::LReLU_TRT version 1
[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1
[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1
[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1
[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1
[X] Plugin creator already registered - ::NMSDynamic_TRT version 1
[X] Plugin creator already registered - ::NMS_TRT version 1
[X] Plugin creator already registered - ::Normalize_TRT version 1
[X] Plugin creator already registered - ::PillarScatterPlugin version 1
[X] Plugin creator already registered - ::PriorBox_TRT version 1
[X] Plugin creator already registered - ::ProposalDynamic version 1
[X] Plugin creator already registered - ::ProposalLayer_TRT version 1
[X] Plugin creator already registered - ::Proposal version 1
[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1
[X] Plugin creator already registered - ::Region_TRT version 1
[X] Plugin creator already registered - ::Reorg_TRT version 2
[X] Plugin creator already registered - ::Reorg_TRT version 1
[X] Plugin creator already registered - ::ResizeNearest_TRT version 1
[X] Plugin creator already registered - ::ROIAlign_TRT version 1
[X] Plugin creator already registered - ::RPROI_TRT version 1
[X] Plugin creator already registered - ::ScatterElements version 1
[X] Plugin creator already registered - ::ScatterElements version 2
[X] Plugin creator already registered - ::ScatterND version 1
[X] Plugin creator already registered - ::SpecialSlice_TRT version 1
[X] Plugin creator already registered - ::Split version 1
[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1
[V] Loaded engine size: 28 MiB
[X] Deserialization required 7385 microseconds.
[X] Adding 1 engine(s) to plan file.
[X] Adding 1 engine weights(s) to plan file.
[I] Saving engine to default_mtq_int8_q_qint8break_fusion-output_modified.engine
[V] [MS] Running engine with multi stream info
[V] [MS] Number of aux streams is 1
[V] [MS] Number of total worker streams is 2
[V] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[X] Total per-runner device persistent memory is 0
[X] Total per-runner host persistent memory is 307312
[X] Allocated device scratch memory of size 63129600
[X] - Runner scratch: 63129600 bytes
[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +61, now: CPU 0, GPU 83 (MiB)
[X] CUDA lazy loading is enabled.
[V] Found candidate CUDA libraries: ['/usr/local/cuda/lib64/libcudart.so.12.1.55', '/usr/local/cuda/lib64/libcudart.so.12', '/usr/local/cuda/lib64/libcudart.so']
[V] Loading inputs from data loader
[V] Loaded Module: numpy | Version: 1.26.4 | Path: ['/root/workspace/development/RTDETR_TensorRT_Deployment/.venv/lib/python3.10/site-packages/numpy']
[V] Loaded Module: torch | Version: 2.5.0+cu124 | Path: ['/root/workspace/development/RTDETR_TensorRT_Deployment/.venv/lib/python3.10/site-packages/torch']
[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. 
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] trt-runner-N0-05/22/25-09:24:08    
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[X] trt-runner-N0-05/22/25-09:24:08     | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}
[V] trt-runner-N0-05/22/25-09:24:08     | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[X] Reallocated output tensor: labels to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[X] Reallocated output tensor: boxes to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[X] Reallocated output tensor: scores to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)
[I] trt-runner-N0-05/22/25-09:24:08    
    ---- Inference Output(s) ----
    {/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[X] trt-runner-N0-05/22/25-09:24:08     | Inference Time: 50.152 ms | Received outputs:
        {'/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0': tensor([[[ 4.2356,  2.5320,  3.0098,  ...,  1.4666, -1.4699,  0.4038],
                 [ 4.5288,  2.9112,  2.8116,  ...,  1.5363, -1.8686,  0.6656],
                 [ 2.3075,  1.2274,  1.4807,  ...,  1.0327, -0.9449,  0.2767],
                 ...,
                 [-2.0147, -1.6150, -1.3982,  ...,  0.3237, -0.5385, -0.2863],
                 [ 4.0195,  2.0774,  2.6026,  ...,  1.4409, -1.8702,  0.7291],
                 [ 5.3979,  2.7150,  2.9667,  ...,  1.9116, -2.2562,  1.1189]]]), '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0': tensor([[[ 2.6162,  2.5762,  1.8890,  ...,  0.6326, -3.2386,  0.3107],
                 [ 1.8620,  2.0987,  2.2594,  ...,  1.0211, -3.7011,  0.5785],
                 [ 1.2952,  1.0130,  1.7293,  ...,  0.8133, -2.8528,  1.1988],
                 ...,
                 [ 0.1365, -0.8366, -0.0760,  ...,  1.2268, -0.0768,  0.6488],
                 [ 3.0152,  3.2550,  1.4931,  ...,  1.3276, -2.4633,  1.0275],
                 [ 2.6256,  3.1985,  2.3334,  ...,  1.2383, -3.1302,  0.9921]]]), '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0': tensor([[[ 4.5454,  4.6249,  3.8835,  ...,  0.9147,  0.8356, -0.7903],
                 [ 3.3883,  3.4706,  2.9221,  ...,  0.0788,  0.2095, -0.2809],
                 [ 2.4338,  2.9204,  2.0296,  ...,  0.0473,  0.1271, -0.7722],
                 ...,
                 [-0.1303,  0.0265,  0.4036,  ...,  0.9066,  0.7460, -0.8565],
                 [ 3.5161,  3.4290,  2.8160,  ..., -0.5236,  1.1519, -0.3525],
                 [ 3.4524,  3.7785,  3.3043,  ..., -0.5757,  1.1171, -0.7011]]]), '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 0.1390,  0.7103,  1.4674,  ..., -1.3450,  2.6365, -5.7482],
                 [ 0.4255,  0.4941,  1.5431,  ..., -1.3107,  5.3255, -4.4489],
                 [ 0.2893,  0.2324,  1.5355,  ..., -2.9491,  5.7867, -5.4024],
                 ...,
                 [ 1.6220,  0.1315,  2.1223,  ..., -1.9954,  1.1090, -2.5507],
                 [ 0.6660,  0.3108,  1.4469,  ..., -2.1906,  7.1519, -4.1745],
                 [ 0.6948,  0.7508,  1.6715,  ..., -2.4627,  7.1783, -5.6823]]]), '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 0.5343,  1.3567,  1.0689,  ..., -1.8608,  2.2227, -3.0871],
                 [ 2.1295,  0.3512,  1.7654,  ..., -1.0354,  4.0582, -1.5561],
                 [ 2.6495,  1.0584,  2.2189,  ..., -1.0772,  2.3429, -2.4081],
                 ...,
                 [ 3.1354,  2.4881,  2.6348,  ..., -1.8398,  0.6319, -1.8554],
                 [ 1.0611,  1.1620,  1.5034,  ..., -1.4202,  2.8712, -2.6771],
                 [ 1.4880,  0.0166,  1.8558,  ..., -0.9512,  3.8181, -2.6678]]]), '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0': tensor([[[-1.2748e-01, -2.9534e-03,  1.8027e-01,  ..., -1.2945e+00,
                   4.8009e+00, -4.6217e+00],
                 [-2.4471e-01, -7.4293e-01,  2.8982e-01,  ..., -2.0361e+00,
                   3.6383e+00, -3.9905e+00],
                 [-2.4948e-01,  4.0067e-02,  2.8410e-01,  ..., -3.7427e+00,
                   2.3985e+00, -4.1059e+00],
                 ...,
                 [-1.5177e-01, -3.6113e-01,  8.8049e-01,  ..., -2.3994e+00,
                   3.2714e+00, -3.5415e+00],
                 [-1.7517e-01, -7.4239e-01,  9.0260e-01,  ..., -2.2391e+00,
                   3.9987e+00, -4.0388e+00],
                 [-4.4069e-01, -5.8864e-01,  8.6772e-02,  ..., -2.4258e+00,
                   3.7314e+00, -3.8026e+00]]]), 'labels': tensor([[ 0, 67, 26,  8, 67,  8,  8,  0,  8, 67, 67, 24,  8,  8,  8,  0,  0, 67,
                  8,  8, 28, 67, 24,  0,  8, 24,  8,  8,  0,  8, 26,  8, 24,  0,  8,  8,
                  0,  8,  0, 24, 26,  0,  8,  8,  8, 24,  8,  8,  0,  8,  0, 24,  8,  0,
                  0,  0,  8, 26,  8,  8,  0,  8, 24, 24,  8,  8, 26, 24,  0,  8, 24, 13,
                  8, 24, 26, 24, 26,  0, 13,  8, 28,  8,  0,  8, 28,  8, 24, 26,  0,  8,
                 13,  0,  8, 13, 26,  0,  8,  8, 13, 24, 24,  8,  8,  8,  8, 26,  8, 24,
                  8,  0, 24,  0, 28, 58, 67,  8,  8, 13, 28, 67,  2,  0,  8,  0,  8, 27,
                  0, 28, 67,  0, 26, 28, 58,  8, 76, 13, 13,  8, 28,  8, 67,  0, 24, 13,
                 26,  8,  8, 13,  0, 79, 24, 26, 28,  8, 56, 67,  0,  0,  8,  0, 26, 28,
                 79,  8,  8,  8,  0,  8,  0, 67, 28, 13,  8,  0, 26, 26, 67,  8,  1,  0,
                  8,  0,  8, 26,  0,  9,  0, 58,  0, 26, 24, 26, 58,  8,  2,  8,  0,  8,
                 13, 67, 26,  8, 56,  8, 26, 24,  0,  0, 24, 67, 13, 28, 26, 56, 10, 26,
                 67, 26,  0,  8,  0, 67, 67,  0,  3, 26,  8,  0,  0,  8,  8, 56, 67, 24,
                  0,  8, 26, 24,  8, 79, 56, 26, 24, 26, 13,  0, 73,  8,  8, 67,  1,  0,
                  2,  3,  0,  9, 76,  8,  0, 28,  0,  8, 24, 13, 24, 26, 58,  0, 26, 24,
                 39,  2, 26, 24, 26,  0,  8,  0, 24,  0, 26,  8,  1, 79,  0, 43, 43,  0,
                 28, 56, 25, 26,  2, 43,  2, 13,  0, 28, 59, 24]]), 'boxes': tensor([[[263.2104,  66.3191, 542.1085, 476.1717],
                 [360.7302, 136.5228, 373.2771, 168.9377],
                 [378.3781, 394.8648, 587.5635, 479.5542],
                 ...,
                 [188.7592, 237.3187, 223.2644, 258.2492],
                 [362.3384, 314.5605, 639.1819, 478.5569],
                 [361.2122, 399.1575, 463.3348, 479.4284]]]), 'scores': tensor([[0.6434, 0.5649, 0.4689, 0.4231, 0.4166, 0.4133, 0.4059, 0.3863, 0.3576,
                 0.3533, 0.3017, 0.2933, 0.2888, 0.2561, 0.2445, 0.2385, 0.2264, 0.2200,
                 0.1992, 0.1983, 0.1965, 0.1950, 0.1914, 0.1858, 0.1843, 0.1818, 0.1807,
                 0.1796, 0.1794, 0.1758, 0.1744, 0.1743, 0.1693, 0.1679, 0.1670, 0.1665,
                 0.1597, 0.1595, 0.1562, 0.1547, 0.1460, 0.1449, 0.1433, 0.1418, 0.1394,
                 0.1390, 0.1385, 0.1372, 0.1372, 0.1365, 0.1364, 0.1362, 0.1355, 0.1318,
                 0.1313, 0.1306, 0.1303, 0.1296, 0.1287, 0.1275, 0.1272, 0.1258, 0.1253,
                 0.1252, 0.1250, 0.1244, 0.1238, 0.1232, 0.1230, 0.1224, 0.1207, 0.1171,
                 0.1165, 0.1163, 0.1162, 0.1155, 0.1142, 0.1123, 0.1111, 0.1108, 0.1107,
                 0.1092, 0.1086, 0.1085, 0.1083, 0.1064, 0.1045, 0.1033, 0.1023, 0.1014,
                 0.0991, 0.0965, 0.0959, 0.0954, 0.0951, 0.0947, 0.0944, 0.0939, 0.0921,
                 0.0919, 0.0917, 0.0916, 0.0906, 0.0900, 0.0899, 0.0876, 0.0875, 0.0862,
                 0.0847, 0.0846, 0.0836, 0.0835, 0.0832, 0.0828, 0.0826, 0.0818, 0.0817,
                 0.0813, 0.0796, 0.0786, 0.0785, 0.0778, 0.0776, 0.0776, 0.0774, 0.0765,
                 0.0764, 0.0762, 0.0757, 0.0751, 0.0742, 0.0741, 0.0739, 0.0738, 0.0735,
                 0.0732, 0.0732, 0.0732, 0.0728, 0.0724, 0.0724, 0.0715, 0.0712, 0.0709,
                 0.0709, 0.0708, 0.0705, 0.0702, 0.0699, 0.0698, 0.0696, 0.0685, 0.0685,
                 0.0684, 0.0683, 0.0682, 0.0676, 0.0675, 0.0673, 0.0673, 0.0667, 0.0667,
                 0.0667, 0.0664, 0.0664, 0.0663, 0.0657, 0.0654, 0.0650, 0.0650, 0.0644,
                 0.0641, 0.0640, 0.0638, 0.0636, 0.0633, 0.0630, 0.0622, 0.0611, 0.0607,
                 0.0605, 0.0604, 0.0601, 0.0599, 0.0599, 0.0598, 0.0598, 0.0593, 0.0593,
                 0.0582, 0.0582, 0.0581, 0.0580, 0.0575, 0.0575, 0.0573, 0.0571, 0.0570,
                 0.0570, 0.0570, 0.0568, 0.0566, 0.0565, 0.0561, 0.0560, 0.0554, 0.0548,
                 0.0548, 0.0544, 0.0544, 0.0542, 0.0541, 0.0541, 0.0541, 0.0540, 0.0538,
                 0.0535, 0.0535, 0.0533, 0.0530, 0.0526, 0.0524, 0.0520, 0.0519, 0.0518,
                 0.0518, 0.0514, 0.0512, 0.0510, 0.0508, 0.0507, 0.0507, 0.0507, 0.0506,
                 0.0505, 0.0504, 0.0500, 0.0500, 0.0500, 0.0498, 0.0497, 0.0497, 0.0497,
                 0.0495, 0.0494, 0.0488, 0.0487, 0.0487, 0.0487, 0.0487, 0.0486, 0.0484,
                 0.0482, 0.0481, 0.0479, 0.0479, 0.0476, 0.0476, 0.0476, 0.0476, 0.0474,
                 0.0474, 0.0473, 0.0472, 0.0470, 0.0468, 0.0466, 0.0465, 0.0464, 0.0464,
                 0.0464, 0.0461, 0.0457, 0.0456, 0.0456, 0.0456, 0.0454, 0.0452, 0.0450,
                 0.0449, 0.0446, 0.0444, 0.0443, 0.0442, 0.0440, 0.0440, 0.0439, 0.0437,
                 0.0437, 0.0437, 0.0436, 0.0433, 0.0433, 0.0432, 0.0431, 0.0430, 0.0430,
                 0.0429, 0.0429, 0.0428]])}
[I] trt-runner-N0-05/22/25-09:24:08     | Completed 1 iteration(s) in 50.15 ms | Average inference time: 50.15 ms.
[I] onnxrt-runner-N0-05/22/25-09:24:08  | Activating and starting inference
[V] Loaded Module: onnxruntime | Version: 1.20.2 | Path: ['/root/workspace/development/RTDETR_TensorRT_Deployment/.venv/lib/python3.10/site-packages/onnxruntime']
[I] Creating ONNX-Runtime Inference Session with providers: ['CPUExecutionProvider']
[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. 
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] onnxrt-runner-N0-05/22/25-09:24:08 
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[X] onnxrt-runner-N0-05/22/25-09:24:08  | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}
[V] onnxrt-runner-N0-05/22/25-09:24:08  | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[I] onnxrt-runner-N0-05/22/25-09:24:08 
    ---- Inference Output(s) ----
    {/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[X] onnxrt-runner-N0-05/22/25-09:24:08  | Inference Time: 177.604 ms | Received outputs:
        {'/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0': tensor([[[ 3.8864,  2.6560,  3.0121,  ...,  1.4150, -1.3747,  0.5130],
                 [ 2.9682,  1.3912,  1.6830,  ...,  1.2557, -1.0860,  0.5815],
                 [-1.0341, -0.1668,  0.1541,  ...,  0.1303, -0.0785, -0.3616],
                 ...,
                 [-1.1155, -1.3151, -1.1665,  ...,  0.3689, -0.8365, -0.3330],
                 [ 3.7093,  2.3833,  2.7045,  ...,  1.2936, -1.3532,  0.7336],
                 [ 4.5051,  2.3447,  2.8511,  ...,  1.5855, -1.3272,  0.8068]]]), '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0': tensor([[[ 2.6173,  2.4923,  1.8669,  ...,  0.5285, -3.2315,  0.2703],
                 [ 2.3529,  1.0416,  1.9073,  ...,  0.9839, -2.8940,  1.2901],
                 [ 0.6215,  0.4753,  0.6065,  ...,  1.0875, -0.6423,  0.5404],
                 ...,
                 [-0.1215, -0.7094,  0.4629,  ...,  1.1607, -0.8799,  0.4742],
                 [ 2.4958,  2.7087,  1.6094,  ...,  1.0292, -2.2524,  0.7039],
                 [ 2.8415,  2.3623,  1.8292,  ...,  1.1610, -2.3385,  1.4377]]]), '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0': tensor([[[ 3.9151,  4.2549,  3.1362,  ...,  0.1553,  0.9190, -0.7568],
                 [ 2.7675,  2.5564,  2.1182,  ...,  0.9519,  0.2675, -1.2333],
                 [ 1.4859,  1.6941,  1.1975,  ...,  0.0775,  0.5086, -1.4401],
                 ...,
                 [ 0.4342, -0.0196,  0.7254,  ...,  0.0951,  0.9235, -0.7154],
                 [ 3.5248,  3.6790,  3.1350,  ..., -1.5470,  1.4541, -1.1836],
                 [ 2.8229,  3.3151,  1.8541,  ...,  2.3757,  0.4869, -1.0527]]]), '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0': tensor([[[-0.0296,  0.5031,  1.3804,  ..., -1.4008,  2.7708, -5.5250],
                 [-0.0953,  0.4430,  1.3873,  ..., -3.0156,  6.4843, -5.8382],
                 [ 0.8470, -0.4361,  1.5967,  ..., -2.5812,  4.8259, -2.7178],
                 ...,
                 [ 1.2395,  0.4135,  2.0213,  ..., -1.6325,  1.9428, -2.1817],
                 [ 0.6077,  0.1026,  1.4831,  ..., -2.4743,  6.6096, -4.1296],
                 [ 0.6205,  0.2294,  1.4172,  ..., -2.6687,  6.3413, -5.3056]]]), '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 0.6574,  1.0883,  0.8281,  ..., -1.8779,  2.2116, -3.0860],
                 [ 2.1510,  1.7804,  1.6356,  ..., -0.4797,  2.5973, -2.3041],
                 [ 2.5166,  1.3435,  2.0963,  ..., -1.4547,  1.1467, -1.8441],
                 ...,
                 [ 3.3671,  2.3563,  3.2236,  ..., -0.8749,  1.1280, -1.8341],
                 [ 1.4733,  0.6677,  1.5430,  ..., -1.6876,  2.8583, -3.4443],
                 [ 2.0375,  1.7558,  1.9177,  ..., -0.7307,  3.4448, -1.8215]]]), '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0': tensor([[[-0.4735,  0.0336, -0.1407,  ..., -1.7204,  4.3141, -4.5292],
                 [-1.5946,  0.1056, -0.2197,  ..., -3.4163,  3.0090, -4.3650],
                 [-0.5148, -1.6492,  0.3677,  ..., -2.4444,  3.6438, -3.7387],
                 ...,
                 [-0.5150, -0.6700,  0.4484,  ..., -2.2795,  3.0152, -4.1140],
                 [-0.6301, -0.8608,  0.4201,  ..., -1.7217,  3.7461, -4.2863],
                 [ 0.1090,  0.9420,  1.4409,  ..., -2.2724,  3.3272, -3.5136]]]), 'labels': tensor([[ 0, 67,  8,  8, 26, 67,  8,  0,  0,  0, 28,  8,  8,  0,  8,  8,  0,  0,
                 67,  8,  8,  8,  8,  8,  8,  8,  8,  0, 24,  8, 67, 24, 13,  8,  0,  8,
                  8, 67,  0,  8,  8,  8, 67, 26,  8,  0,  0, 26,  8,  0, 67,  8,  8, 26,
                  0,  0,  8,  0,  8, 13,  8,  8,  8, 28, 13,  8,  0, 28, 24,  8,  0, 26,
                 28, 26,  8, 24,  0, 26,  0,  8, 24,  0, 24,  0,  0, 67,  0, 67, 67,  8,
                 24,  0, 26, 67, 24,  0, 67, 79, 58,  8,  8, 24,  0,  0,  0,  8, 28,  8,
                  8, 56,  0, 58, 28, 67, 67,  0, 76,  0, 26, 24, 73,  8, 60, 24, 28,  8,
                 24,  8, 13,  0,  0, 24,  0,  8, 28, 24, 11,  8, 43,  2, 26,  0, 26,  8,
                  8, 28,  8,  8, 24,  0, 24, 28, 24,  2, 24, 26, 26,  0, 26, 28, 39, 26,
                 26, 13, 56, 24,  8,  0, 26, 13, 67, 24,  8, 24, 24, 26,  8, 28,  0, 28,
                  2, 24, 24, 13,  8, 56, 67, 26, 24,  0, 26,  0, 76, 24,  9,  8,  8, 34,
                  0, 58, 67, 28, 13, 24, 13,  8,  8, 24, 56, 79, 56, 24,  0, 12, 25, 28,
                 43, 26,  3,  0, 27, 58, 74,  3,  8,  8,  2, 26, 58,  0,  8, 74, 60,  9,
                  0, 24,  0, 26, 24, 24,  0, 11,  8,  8,  0, 13,  0,  0,  0,  1, 79,  8,
                 28, 24,  0,  1,  2, 59,  0, 58, 24, 28,  0,  9, 26, 24,  8, 26, 79,  8,
                 26, 26, 24, 24,  8, 24, 13, 67,  2,  8,  0, 26,  0, 13, 26, 24,  0, 58,
                  0,  8, 67,  8, 11, 25,  2,  0, 56,  2, 28, 24]]), 'boxes': tensor([[[262.9576,  71.5327, 547.7870, 476.1678],
                 [361.2971, 137.0405, 373.3970, 169.7942],
                 [ -1.0678, 216.7084,  27.7456, 234.6881],
                 ...,
                 [100.0009, 178.9919, 123.2679, 196.2153],
                 [175.3711, 198.3185, 231.4542, 280.3434],
                 [368.4505, 398.3175, 518.1219, 478.9788]]]), 'scores': tensor([[0.6030, 0.5362, 0.5060, 0.4716, 0.4550, 0.4172, 0.4056, 0.3967, 0.3598,
                 0.3229, 0.2707, 0.2680, 0.2555, 0.2454, 0.2324, 0.2316, 0.2067, 0.2027,
                 0.2022, 0.2010, 0.1952, 0.1934, 0.1921, 0.1915, 0.1900, 0.1865, 0.1848,
                 0.1837, 0.1807, 0.1792, 0.1760, 0.1732, 0.1698, 0.1648, 0.1636, 0.1600,
                 0.1595, 0.1585, 0.1555, 0.1547, 0.1529, 0.1523, 0.1518, 0.1502, 0.1478,
                 0.1432, 0.1393, 0.1369, 0.1360, 0.1356, 0.1341, 0.1318, 0.1289, 0.1285,
                 0.1260, 0.1236, 0.1187, 0.1185, 0.1182, 0.1180, 0.1179, 0.1175, 0.1174,
                 0.1169, 0.1167, 0.1138, 0.1123, 0.1119, 0.1114, 0.1112, 0.1103, 0.1103,
                 0.1102, 0.1101, 0.1092, 0.1092, 0.1083, 0.1081, 0.1059, 0.1050, 0.1040,
                 0.1031, 0.1025, 0.1021, 0.1009, 0.0988, 0.0972, 0.0961, 0.0952, 0.0948,
                 0.0943, 0.0942, 0.0942, 0.0942, 0.0940, 0.0934, 0.0934, 0.0932, 0.0930,
                 0.0925, 0.0924, 0.0922, 0.0921, 0.0915, 0.0913, 0.0910, 0.0907, 0.0899,
                 0.0898, 0.0883, 0.0872, 0.0868, 0.0864, 0.0864, 0.0859, 0.0849, 0.0835,
                 0.0833, 0.0830, 0.0819, 0.0814, 0.0810, 0.0804, 0.0800, 0.0790, 0.0788,
                 0.0786, 0.0780, 0.0779, 0.0776, 0.0776, 0.0773, 0.0771, 0.0770, 0.0766,
                 0.0765, 0.0760, 0.0759, 0.0752, 0.0751, 0.0749, 0.0748, 0.0744, 0.0742,
                 0.0740, 0.0736, 0.0732, 0.0732, 0.0731, 0.0730, 0.0722, 0.0718, 0.0717,
                 0.0717, 0.0710, 0.0706, 0.0705, 0.0693, 0.0692, 0.0691, 0.0686, 0.0684,
                 0.0680, 0.0678, 0.0677, 0.0677, 0.0672, 0.0670, 0.0656, 0.0656, 0.0655,
                 0.0655, 0.0654, 0.0654, 0.0652, 0.0645, 0.0640, 0.0639, 0.0638, 0.0631,
                 0.0628, 0.0626, 0.0619, 0.0617, 0.0612, 0.0612, 0.0611, 0.0610, 0.0609,
                 0.0606, 0.0606, 0.0604, 0.0604, 0.0604, 0.0604, 0.0601, 0.0597, 0.0597,
                 0.0597, 0.0596, 0.0595, 0.0593, 0.0589, 0.0586, 0.0586, 0.0586, 0.0585,
                 0.0578, 0.0574, 0.0573, 0.0573, 0.0571, 0.0570, 0.0568, 0.0567, 0.0562,
                 0.0557, 0.0556, 0.0555, 0.0555, 0.0555, 0.0544, 0.0542, 0.0535, 0.0533,
                 0.0533, 0.0531, 0.0530, 0.0527, 0.0525, 0.0522, 0.0521, 0.0519, 0.0518,
                 0.0517, 0.0515, 0.0514, 0.0512, 0.0507, 0.0507, 0.0507, 0.0505, 0.0504,
                 0.0503, 0.0502, 0.0502, 0.0499, 0.0496, 0.0495, 0.0487, 0.0487, 0.0485,
                 0.0484, 0.0482, 0.0480, 0.0480, 0.0480, 0.0476, 0.0472, 0.0467, 0.0466,
                 0.0462, 0.0460, 0.0460, 0.0460, 0.0459, 0.0456, 0.0455, 0.0455, 0.0455,
                 0.0454, 0.0451, 0.0451, 0.0445, 0.0444, 0.0444, 0.0443, 0.0441, 0.0441,
                 0.0440, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437, 0.0436, 0.0435,
                 0.0432, 0.0430, 0.0429, 0.0426, 0.0426, 0.0425, 0.0423, 0.0419, 0.0419,
                 0.0419, 0.0417, 0.0417]])}
[I] onnxrt-runner-N0-05/22/25-09:24:08  | Completed 1 iteration(s) in 177.6 ms | Average inference time: 177.6 ms.
[V] Successfully ran: ['trt-runner-N0-05/22/25-09:24:08', 'onnxrt-runner-N0-05/22/25-09:24:08']
[I] Accuracy Comparison | trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) with '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([ 4.2356,  2.5320,  3.0098,  ...,  1.9116, -2.2562,  1.1189])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([ 3.8864,  2.6560,  3.0121,  ...,  1.5855, -1.3272,  0.8068])
[I]         trt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 | Stats: mean=0.1901, std-dev=1.8582, var=3.4529, median=0.42605, min=-6.2469 at (0, 150, 82), max=6.6511 at (0, 38, 0), avg-magnitude=1.4616, p90=2.4413, p95=3.0014, p99=3.8832
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.69 , -5.33 ) |       51.0 | 
                (-5.33 , -3.98 ) |      617.0 | ##
                (-3.98 , -2.62 ) |     2126.0 | #######
                (-2.62 , -1.27 ) |     3078.0 | ###########
                (-1.27 , 0.0886) |     5583.0 | ####################
                (0.0886, 1.44  ) |    10798.0 | ########################################
                (1.44  , 2.8   ) |     4641.0 | #################
                (2.8   , 4.15  ) |     1752.0 | ######
                (4.15  , 5.51  ) |      126.0 | 
                (5.51  , 6.86  ) |       28.0 | 
[I]         onnxrt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 | Stats: mean=0.1808, std-dev=1.8751, var=3.5161, median=0.42876, min=-6.6851 at (0, 123, 82), max=6.8622 at (0, 170, 0), avg-magnitude=1.4761, p90=2.4666, p95=3.0216, p99=3.855
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.69 , -5.33 ) |       61.0 | 
                (-5.33 , -3.98 ) |      580.0 | ##
                (-3.98 , -2.62 ) |     2320.0 | ########
                (-2.62 , -1.27 ) |     3046.0 | ###########
                (-1.27 , 0.0886) |     5462.0 | ####################
                (0.0886, 1.44  ) |    10882.0 | ########################################
                (1.44  , 2.8   ) |     4514.0 | ################
                (2.8   , 4.15  ) |     1781.0 | ######
                (4.15  , 5.51  ) |      124.0 | 
                (5.51  , 6.86  ) |       30.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=9.2013] OR [rel=15474] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.96761, std-dev=1.0679, var=1.1405, median=0.58349, min=0 at (0, 115, 33), max=9.2013 at (0, 90, 0), avg-magnitude=0.96761, p90=2.4632, p95=3.3032, p99=4.8229
[I]                 ---- Histogram ----
                    Bin Range    |  Num Elems | Visualization
                    (0   , 0.92) |    18807.0 | ########################################
                    (0.92, 1.84) |     5424.0 | ###########
                    (1.84, 2.76) |     2288.0 | ####
                    (2.76, 3.68) |     1257.0 | ##
                    (3.68, 4.6 ) |      650.0 | #
                    (4.6 , 5.52) |      260.0 | 
                    (5.52, 6.44) |       76.0 | 
                    (6.44, 7.36) |       26.0 | 
                    (7.36, 8.28) |        9.0 | 
                    (8.28, 9.2 ) |        3.0 | 
[I]             Relative Difference | Stats: mean=4.306, std-dev=117.9, var=13899, median=0.56261, min=0 at (0, 115, 33), max=15474 at (0, 76, 14), avg-magnitude=4.306, p90=4.0824, p95=8.1097, p99=41.376
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0       , 1.55e+03) |    28795.0 | ########################################
                    (1.55e+03, 3.09e+03) |        2.0 | 
                    (3.09e+03, 4.64e+03) |        1.0 | 
                    (4.64e+03, 6.19e+03) |        0.0 | 
                    (6.19e+03, 7.74e+03) |        0.0 | 
                    (7.74e+03, 9.28e+03) |        0.0 | 
                    (9.28e+03, 1.08e+04) |        0.0 | 
                    (1.08e+04, 1.24e+04) |        1.0 | 
                    (1.24e+04, 1.39e+04) |        0.0 | 
                    (1.39e+04, 1.55e+04) |        1.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [trt-runner-N0-05/22/25-09:24:08] and '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) with '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([ 2.6162,  2.5762,  1.8890,  ...,  1.2383, -3.1302,  0.9921])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([ 2.6173,  2.4923,  1.8669,  ...,  1.1610, -2.3385,  1.4377])
[I]         trt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 | Stats: mean=-0.008702, std-dev=1.8757, var=3.5183, median=0.3032, min=-7.198 at (0, 52, 59), max=5.0999 at (0, 167, 73), avg-magnitude=1.4781, p90=2.2009, p95=2.7388, p99=3.7136
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-7.2 , -5.96) |       12.0 | 
                (-5.96, -4.72) |      345.0 | #
                (-4.72, -3.49) |     1269.0 | #####
                (-3.49, -2.25) |     2338.0 | ##########
                (-2.25, -1.02) |     3586.0 | ###############
                (-1.02, 0.221) |     6236.0 | ##########################
                (0.221, 1.46 ) |     9252.0 | ########################################
                (1.46 , 2.69 ) |     4227.0 | ##################
                (2.69 , 3.93 ) |     1365.0 | #####
                (3.93 , 5.17 ) |      170.0 | 
[I]         onnxrt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 | Stats: mean=-0.01428, std-dev=1.8962, var=3.5957, median=0.30371, min=-6.7511 at (0, 50, 59), max=5.1675 at (0, 222, 73), avg-magnitude=1.5028, p90=2.2252, p95=2.7625, p99=3.6673
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-7.2 , -5.96) |       15.0 | 
                (-5.96, -4.72) |      334.0 | #
                (-4.72, -3.49) |     1336.0 | #####
                (-3.49, -2.25) |     2352.0 | ##########
                (-2.25, -1.02) |     3669.0 | ################
                (-1.02, 0.221) |     6112.0 | ##########################
                (0.221, 1.46 ) |     9094.0 | ########################################
                (1.46 , 2.69 ) |     4313.0 | ##################
                (2.69 , 3.93 ) |     1417.0 | ######
                (3.93 , 5.17 ) |      158.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=7.3638] OR [rel=14086] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=1.0231, std-dev=0.96209, var=0.92561, median=0.73368, min=2.9802e-08 at (0, 162, 20), max=7.3638 at (0, 166, 10), avg-magnitude=1.0231, p90=2.3226, p95=2.9907, p99=4.3985
[I]                 ---- Histogram ----
                    Bin Range         |  Num Elems | Visualization
                    (2.98e-08, 0.736) |    14445.0 | ########################################
                    (0.736   , 1.47 ) |     7437.0 | ####################
                    (1.47    , 2.21 ) |     3682.0 | ##########
                    (2.21    , 2.95 ) |     1733.0 | ####
                    (2.95    , 3.68 ) |      824.0 | ##
                    (3.68    , 4.42 ) |      395.0 | #
                    (4.42    , 5.15 ) |      179.0 | 
                    (5.15    , 5.89 ) |       74.0 | 
                    (5.89    , 6.63 ) |       25.0 | 
                    (6.63    , 7.36 ) |        6.0 | 
[I]             Relative Difference | Stats: mean=4.3245, std-dev=118.65, var=14079, median=0.62131, min=8.7979e-08 at (0, 162, 20), max=14086 at (0, 82, 92), avg-magnitude=4.3245, p90=3.9601, p95=8.0823, p99=35.23
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (8.8e-08 , 1.41e+03) |    28793.0 | ########################################
                    (1.41e+03, 2.82e+03) |        2.0 | 
                    (2.82e+03, 4.23e+03) |        3.0 | 
                    (4.23e+03, 5.63e+03) |        0.0 | 
                    (5.63e+03, 7.04e+03) |        0.0 | 
                    (7.04e+03, 8.45e+03) |        0.0 | 
                    (8.45e+03, 9.86e+03) |        0.0 | 
                    (9.86e+03, 1.13e+04) |        0.0 | 
                    (1.13e+04, 1.27e+04) |        1.0 | 
                    (1.27e+04, 1.41e+04) |        1.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [trt-runner-N0-05/22/25-09:24:08] and '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) with '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([ 4.5454,  4.6249,  3.8835,  ..., -0.5757,  1.1171, -0.7011])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([ 3.9151,  4.2549,  3.1362,  ...,  2.3757,  0.4869, -1.0527])
[I]         trt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 | Stats: mean=0.30115, std-dev=1.788, var=3.1968, median=0.48497, min=-7.1703 at (0, 12, 23), max=11.901 at (0, 100, 93), avg-magnitude=1.4369, p90=2.4942, p95=3.0908, p99=4.0952
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-7.23, -5.32) |      104.0 | 
                (-5.32, -3.41) |      717.0 | ##
                (-3.41, -1.49) |     3801.0 | #############
                (-1.49, 0.42 ) |     9258.0 | ################################
                (0.42 , 2.33 ) |    11540.0 | ########################################
                (2.33 , 4.25 ) |     3169.0 | ##########
                (4.25 , 6.16 ) |      203.0 | 
                (6.16 , 8.07 ) |        5.0 | 
                (8.07 , 9.99 ) |        2.0 | 
                (9.99 , 11.9 ) |        1.0 | 
[I]         onnxrt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 | Stats: mean=0.29836, std-dev=1.7936, var=3.2169, median=0.486, min=-7.2331 at (0, 151, 23), max=10.866 at (0, 104, 93), avg-magnitude=1.4456, p90=2.4812, p95=3.0639, p99=4.11
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-7.23, -5.32) |      111.0 | 
                (-5.32, -3.41) |      683.0 | ##
                (-3.41, -1.49) |     3924.0 | #############
                (-1.49, 0.42 ) |     9135.0 | ###############################
                (0.42 , 2.33 ) |    11597.0 | ########################################
                (2.33 , 4.25 ) |     3134.0 | ##########
                (4.25 , 6.16 ) |      208.0 | 
                (6.16 , 8.07 ) |        3.0 | 
                (8.07 , 9.99 ) |        3.0 | 
                (9.99 , 11.9 ) |        2.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=12.43] OR [rel=3191] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=1.0642, std-dev=1.0104, var=1.021, median=0.76037, min=0 at (0, 242, 22), max=12.43 at (0, 100, 93), avg-magnitude=1.0642, p90=2.4268, p95=3.1088, p99=4.4795
[I]                 ---- Histogram ----
                    Bin Range    |  Num Elems | Visualization
                    (0   , 1.24) |    19704.0 | ########################################
                    (1.24, 2.49) |     6383.0 | ############
                    (2.49, 3.73) |     1992.0 | ####
                    (3.73, 4.97) |      565.0 | #
                    (4.97, 6.21) |      109.0 | 
                    (6.21, 7.46) |       26.0 | 
                    (7.46, 8.7 ) |       11.0 | 
                    (8.7 , 9.94) |        7.0 | 
                    (9.94, 11.2) |        2.0 | 
                    (11.2, 12.4) |        1.0 | 
[I]             Relative Difference | Stats: mean=3.4354, std-dev=34.843, var=1214.1, median=0.6661, min=0 at (0, 242, 22), max=3191 at (0, 54, 70), avg-magnitude=3.4354, p90=4.178, p95=8.3349, p99=42.505
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0       , 319     ) |    28772.0 | ########################################
                    (319     , 638     ) |       18.0 | 
                    (638     , 957     ) |        3.0 | 
                    (957     , 1.28e+03) |        3.0 | 
                    (1.28e+03, 1.6e+03 ) |        1.0 | 
                    (1.6e+03 , 1.91e+03) |        1.0 | 
                    (1.91e+03, 2.23e+03) |        0.0 | 
                    (2.23e+03, 2.55e+03) |        0.0 | 
                    (2.55e+03, 2.87e+03) |        1.0 | 
                    (2.87e+03, 3.19e+03) |        1.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [trt-runner-N0-05/22/25-09:24:08] and '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([ 0.1390,  0.7103,  1.4674,  ..., -2.4627,  7.1783, -5.6823])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([-0.0296,  0.5031,  1.3804,  ..., -2.6687,  6.3413, -5.3056])
[I]         trt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.19266, std-dev=2.0674, var=4.2743, median=0.18831, min=-7.9978 at (0, 109, 191), max=7.9049 at (0, 109, 24), avg-magnitude=1.6845, p90=2.899, p95=3.6905, p99=4.3681
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-8     , -6.41  ) |       22.0 | 
                (-6.41  , -4.82  ) |      170.0 | 
                (-4.82  , -3.23  ) |     2654.0 | ######
                (-3.23  , -1.64  ) |     8783.0 | #####################
                (-1.64  , -0.0464) |    14454.0 | ###################################
                (-0.0464, 1.54   ) |    16123.0 | ########################################
                (1.54   , 3.13   ) |    10481.0 | ##########################
                (3.13   , 4.72   ) |     4524.0 | ###########
                (4.72   , 6.31   ) |      273.0 | 
                (6.31   , 7.9    ) |      116.0 | 
[I]         onnxrt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.15685, std-dev=2.0738, var=4.3007, median=0.14499, min=-7.8389 at (0, 123, 96), max=7.7039 at (0, 215, 190), avg-magnitude=1.6873, p90=2.8771, p95=3.6862, p99=4.361
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-8     , -6.41  ) |       26.0 | 
                (-6.41  , -4.82  ) |      209.0 | 
                (-4.82  , -3.23  ) |     2744.0 | ######
                (-3.23  , -1.64  ) |     8979.0 | ######################
                (-1.64  , -0.0464) |    14587.0 | ####################################
                (-0.0464, 1.54   ) |    16117.0 | ########################################
                (1.54   , 3.13   ) |    10047.0 | ########################
                (3.13   , 4.72   ) |     4508.0 | ###########
                (4.72   , 6.31   ) |      253.0 | 
                (6.31   , 7.9    ) |      130.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=8.1654] OR [rel=29852] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.64126, std-dev=0.69136, var=0.47798, median=0.41542, min=0 at (0, 38, 2), max=8.1654 at (0, 109, 24), avg-magnitude=0.64126, p90=1.5033, p95=1.9988, p99=3.2715
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (0    , 0.817) |    42124.0 | ########################################
                    (0.817, 1.63 ) |    10649.0 | ##########
                    (1.63 , 2.45 ) |     3220.0 | ###
                    (2.45 , 3.27 ) |     1029.0 | 
                    (3.27 , 4.08 ) |      350.0 | 
                    (4.08 , 4.9  ) |      152.0 | 
                    (4.9  , 5.72 ) |       58.0 | 
                    (5.72 , 6.53 ) |       14.0 | 
                    (6.53 , 7.35 ) |        3.0 | 
                    (7.35 , 8.17 ) |        1.0 | 
[I]             Relative Difference | Stats: mean=3.0514, std-dev=143.32, var=20541, median=0.33438, min=0 at (0, 38, 2), max=29852 at (0, 120, 104), avg-magnitude=3.0514, p90=2.201, p95=4.4983, p99=24.102
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0       , 2.99e+03) |    57594.0 | ########################################
                    (2.99e+03, 5.97e+03) |        4.0 | 
                    (5.97e+03, 8.96e+03) |        0.0 | 
                    (8.96e+03, 1.19e+04) |        0.0 | 
                    (1.19e+04, 1.49e+04) |        1.0 | 
                    (1.49e+04, 1.79e+04) |        0.0 | 
                    (1.79e+04, 2.09e+04) |        0.0 | 
                    (2.09e+04, 2.39e+04) |        0.0 | 
                    (2.39e+04, 2.69e+04) |        0.0 | 
                    (2.69e+04, 2.99e+04) |        1.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N0-05/22/25-09:24:08] and '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([ 0.5343,  1.3567,  1.0689,  ..., -0.9512,  3.8181, -2.6678])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([ 0.6574,  1.0883,  0.8281,  ..., -0.7307,  3.4448, -1.8215])
[I]         trt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.072437, std-dev=2.5346, var=6.4241, median=0.14731, min=-35.783 at (0, 23, 183), max=9.295 at (0, 166, 46), avg-magnitude=1.8412, p90=3.0812, p95=3.6259, p99=4.7161
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-35.8, -31.2) |        5.0 | 
                (-31.2, -26.6) |       15.0 | 
                (-26.6, -22  ) |       47.0 | 
                (-22  , -17.4) |       88.0 | 
                (-17.4, -12.8) |       61.0 | 
                (-12.8, -8.23) |       44.0 | 
                (-8.23, -3.64) |     2066.0 | ##
                (-3.64, 0.953) |    34601.0 | ########################################
                (0.953, 5.54 ) |    20281.0 | #######################
                (5.54 , 10.1 ) |      392.0 | 
[I]         onnxrt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.062898, std-dev=2.5249, var=6.3749, median=0.12922, min=-33.608 at (0, 49, 183), max=10.137 at (0, 272, 47), avg-magnitude=1.8338, p90=3.0449, p95=3.621, p99=4.9222
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-35.8, -31.2) |        5.0 | 
                (-31.2, -26.6) |       20.0 | 
                (-26.6, -22  ) |       38.0 | 
                (-22  , -17.4) |       72.0 | 
                (-17.4, -12.8) |       83.0 | 
                (-12.8, -8.23) |       51.0 | 
                (-8.23, -3.64) |     1942.0 | ##
                (-3.64, 0.953) |    34951.0 | ########################################
                (0.953, 5.54 ) |    20024.0 | ######################
                (5.54 , 10.1 ) |      414.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=28.07] OR [rel=5.5626e+05] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.8019, std-dev=0.97327, var=0.94726, median=0.55333, min=0 at (0, 209, 141), max=28.07 at (0, 19, 183), avg-magnitude=0.8019, p90=1.7577, p95=2.274, p99=3.664
[I]                 ---- Histogram ----
                    Bin Range    |  Num Elems | Visualization
                    (0   , 2.81) |    56129.0 | ########################################
                    (2.81, 5.61) |     1269.0 | 
                    (5.61, 8.42) |       81.0 | 
                    (8.42, 11.2) |       42.0 | 
                    (11.2, 14  ) |       40.0 | 
                    (14  , 16.8) |       18.0 | 
                    (16.8, 19.6) |       14.0 | 
                    (19.6, 22.5) |        3.0 | 
                    (22.5, 25.3) |        2.0 | 
                    (25.3, 28.1) |        2.0 | 
[I]             Relative Difference | Stats: mean=22.192, std-dev=3160.5, var=9.9887e+06, median=0.40359, min=0 at (0, 209, 141), max=5.5626e+05 at (0, 79, 106), avg-magnitude=22.192, p90=2.834, p95=5.5357, p99=27.904
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0       , 5.56e+04) |    57598.0 | ########################################
                    (5.56e+04, 1.11e+05) |        0.0 | 
                    (1.11e+05, 1.67e+05) |        0.0 | 
                    (1.67e+05, 2.23e+05) |        0.0 | 
                    (2.23e+05, 2.78e+05) |        0.0 | 
                    (2.78e+05, 3.34e+05) |        0.0 | 
                    (3.34e+05, 3.89e+05) |        0.0 | 
                    (3.89e+05, 4.45e+05) |        0.0 | 
                    (4.45e+05, 5.01e+05) |        0.0 | 
                    (5.01e+05, 5.56e+05) |        2.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N0-05/22/25-09:24:08] and '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([-1.2748e-01, -2.9534e-03,  1.8027e-01,  ..., -2.4258e+00,
                         3.7314e+00, -3.8026e+00])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([-0.4735,  0.0336, -0.1407,  ..., -2.2724,  3.3272, -3.5136])
[I]         trt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.12017, std-dev=2.2312, var=4.9783, median=0.18659, min=-9.2136 at (0, 151, 183), max=8.0296 at (0, 147, 104), avg-magnitude=1.7989, p90=3.0779, p95=3.8439, p99=4.4853
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.32 , -7.59 ) |       91.0 | 
                (-7.59 , -5.85 ) |      166.0 | 
                (-5.85 , -4.12 ) |      558.0 | #
                (-4.12 , -2.38 ) |     7794.0 | #################
                (-2.38 , -0.647) |    11554.0 | #########################
                (-0.647, 1.09  ) |    18135.0 | ########################################
                (1.09  , 2.82  ) |    12214.0 | ##########################
                (2.82  , 4.56  ) |     6565.0 | ##############
                (4.56  , 6.29  ) |      492.0 | #
                (6.29  , 8.03  ) |       31.0 | 
[I]         onnxrt-runner-N0-05/22/25-09:24:08: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.11493, std-dev=2.2279, var=4.9633, median=0.1752, min=-9.3242 at (0, 208, 183), max=7.8344 at (0, 100, 104), avg-magnitude=1.7968, p90=3.0641, p95=3.8392, p99=4.463
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.32 , -7.59 ) |      103.0 | 
                (-7.59 , -5.85 ) |      142.0 | 
                (-5.85 , -4.12 ) |      542.0 | #
                (-4.12 , -2.38 ) |     7761.0 | #################
                (-2.38 , -0.647) |    11809.0 | ##########################
                (-0.647, 1.09  ) |    17904.0 | ########################################
                (1.09  , 2.82  ) |    12289.0 | ###########################
                (2.82  , 4.56  ) |     6550.0 | ##############
                (4.56  , 6.29  ) |      472.0 | #
                (6.29  , 8.03  ) |       28.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=8.43] OR [rel=62355] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.79023, std-dev=0.75625, var=0.57191, median=0.56766, min=0 at (0, 148, 176), max=8.43 at (0, 8, 184), avg-magnitude=0.79023, p90=1.8214, p95=2.3052, p99=3.3571
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (0    , 0.843) |    36989.0 | ########################################
                    (0.843, 1.69 ) |    13661.0 | ##############
                    (1.69 , 2.53 ) |     4891.0 | #####
                    (2.53 , 3.37 ) |     1496.0 | #
                    (3.37 , 4.22 ) |      414.0 | 
                    (4.22 , 5.06 ) |      115.0 | 
                    (5.06 , 5.9  ) |       22.0 | 
                    (5.9  , 6.74 ) |        7.0 | 
                    (6.74 , 7.59 ) |        3.0 | 
                    (7.59 , 8.43 ) |        2.0 | 
[I]             Relative Difference | Stats: mean=4.3713, std-dev=270.56, var=73202, median=0.42857, min=0 at (0, 148, 176), max=62355 at (0, 195, 103), avg-magnitude=4.3713, p90=3.179, p95=6.4338, p99=32.752
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0       , 6.24e+03) |    57597.0 | ########################################
                    (6.24e+03, 1.25e+04) |        1.0 | 
                    (1.25e+04, 1.87e+04) |        1.0 | 
                    (1.87e+04, 2.49e+04) |        0.0 | 
                    (2.49e+04, 3.12e+04) |        0.0 | 
                    (3.12e+04, 3.74e+04) |        0.0 | 
                    (3.74e+04, 4.36e+04) |        0.0 | 
                    (4.36e+04, 4.99e+04) |        0.0 | 
                    (4.99e+04, 5.61e+04) |        0.0 | 
                    (5.61e+04, 6.24e+04) |        1.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N0-05/22/25-09:24:08] and '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: 'labels' (dtype=int64, shape=torch.Size([1, 300])) with 'labels' (dtype=int64, shape=torch.Size([1, 300]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([26, 67,  8,  8, 67, 67, 24,  8,  0, 67,  8, 28, 67, 24,  0, 24,  8,  0,
                        26,  8, 24,  0,  8,  0,  8, 24, 26,  0,  8,  8, 24,  8,  8,  0,  8,  0,
                        24,  0, 26,  8,  0, 24, 24,  8, 26, 24,  0, 24, 13,  8, 24, 26, 26,  0,
                        13, 28,  8,  0,  8, 28,  8, 24, 26,  0, 13,  8, 13, 26,  8,  8, 13, 24,
                        24,  8,  8,  8,  8, 26,  8, 24,  0, 24,  0, 58,  8,  8, 13, 28, 67,  2,
                         0,  8,  0,  8, 27,  0, 28, 67, 26, 28, 58, 76, 13, 13, 28,  8, 67, 24,
                        13, 26,  8, 13,  0, 79, 26, 28,  8, 56, 67,  0,  8,  0, 26, 28, 79,  8,
                         8,  8,  0,  8,  0, 67, 28, 13,  0, 26, 67,  8,  1,  0,  8,  0,  8, 26,
                         0,  9,  0, 58,  0, 26, 24, 26, 58,  8,  2,  0,  8, 13, 67, 26,  8, 56,
                         8, 26, 24,  0,  0, 24, 67, 13, 28, 26, 56, 10, 26, 67,  0,  8,  0, 67,
                        67,  0,  3, 26,  8,  0,  0,  8, 56, 67, 24,  8, 26, 24,  8, 79, 56, 26,
                        24, 26, 13,  0, 73,  8,  8, 67,  1,  0,  2,  3,  9, 76,  8, 28,  0,  8,
                        24, 13, 24, 26, 58,  0, 26, 24, 39,  2, 26, 26,  0,  8,  0, 24,  0, 26,
                         8,  1, 79,  0, 43, 43,  0, 28, 56, 25, 26,  2, 43, 13,  0, 28, 59])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([ 8, 26, 67,  0,  0, 28,  8,  0,  8,  0, 67,  8,  8,  8,  8,  8,  0, 24,
                        67, 24, 13,  8,  0,  8, 67,  8,  8,  8, 67, 26,  0,  0, 26,  8,  0, 67,
                         8, 26,  0, 13,  8,  8, 28, 13,  0, 28, 24,  0, 26, 28, 26,  8,  0, 26,
                         0, 24,  0, 24,  0,  0, 67,  0, 67, 67, 24, 26, 67, 24, 67, 79, 58,  8,
                         8, 24,  0,  0,  0,  8, 28,  8, 56,  0, 58, 67,  0, 76,  0, 26, 24, 73,
                         8, 60, 24, 28,  8, 24,  8, 13,  0, 24,  0, 28, 24, 11, 43,  2, 26, 26,
                         8,  8, 28,  8, 24,  0, 28, 24,  2, 24, 26, 26, 26, 28, 39, 26, 26, 13,
                        56, 24,  8,  0, 26, 13, 67, 24, 24, 24,  8, 28,  0, 28,  2, 24, 24, 13,
                         8, 56, 67, 26, 24,  0, 26,  0, 76, 24,  9,  8, 34,  0, 58, 67, 28, 13,
                        24, 13,  8,  8, 24, 56, 79, 56, 24,  0, 12, 25, 28, 43,  3,  0, 27, 58,
                        74,  3,  8,  8,  2, 26, 58,  0, 74, 60,  9, 24,  0, 26, 24, 24,  0, 11,
                         8,  8,  0, 13,  0,  0,  0,  1, 79,  8, 28, 24,  1,  2, 59, 58, 24, 28,
                         0,  9, 26, 24,  8, 26, 79,  8, 26, 26, 24,  8, 24, 13, 67,  2,  8,  0,
                        26,  0, 13, 26, 24,  0, 58,  0,  8, 67,  8, 11, 25,  0, 56,  2, 28])
[I]         trt-runner-N0-05/22/25-09:24:08: labels | Stats: mean=19.237, std-dev=20.783, var=431.95, median=8, min=0 at (0, 0), max=79 at (0, 149), avg-magnitude=19.237, p90=58, p95=67, p99=79
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         73 | ############################
                (7 , 15) |        101 | ########################################
                (15, 23) |          0 | 
                (23, 31) |         83 | ################################
                (31, 39) |          1 | 
                (39, 47) |          3 | #
                (47, 55) |          0 | 
                (55, 63) |         12 | ####
                (63, 71) |         20 | #######
                (71, 79) |          7 | ##
[I]         onnxrt-runner-N0-05/22/25-09:24:08: labels | Stats: mean=20.497, std-dev=21.41, var=458.4, median=12, min=0 at (0, 0), max=79 at (0, 97), avg-magnitude=20.497, p90=59, p95=67, p99=79
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         73 | ################################
                (7 , 15) |         89 | #######################################
                (15, 23) |          0 | 
                (23, 31) |         90 | ########################################
                (31, 39) |          2 | 
                (39, 47) |          2 | 
                (47, 55) |          0 | 
                (55, 63) |         16 | #######
                (63, 71) |         19 | ########
                (71, 79) |          9 | ####
[I]         Error Metrics: labels
[I]             Minimum Required Tolerance: elemwise error | [abs=79] OR [rel=nan] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=20.14, std-dev=19.828, var=393.16, median=16, min=0 at (0, 0), max=79 at (0, 149), avg-magnitude=20.14, p90=55, p95=59, p99=73
[I]                 ---- Histogram ----
                    Bin Range|  Num Elems | Visualization
                    (0 , 7 ) |         88 | ########################################
                    (7 , 15) |         56 | #########################
                    (15, 23) |         50 | ######################
                    (23, 31) |         45 | ####################
                    (31, 39) |          5 | ##
                    (39, 47) |         14 | ######
                    (47, 55) |         12 | #####
                    (55, 63) |         15 | ######
                    (63, 71) |         11 | #####
                    (71, 79) |          4 | #
[I]             Relative Difference | Stats: mean=nan, std-dev=nan, var=nan, median=nan, min=nan at (0, 0), max=nan at (0, 0), avg-magnitude=nan, p90=nan, p95=nan, p99=nan
[V]                 Could not generate histogram. Note: Error was: torch.histogramdd: dimension 0's range [-nan, -nan] is not finite
[I]                 
[X]         Finished comparing: 'labels' (dtype=int64, shape=torch.Size([1, 300])) [trt-runner-N0-05/22/25-09:24:08] and 'labels' (dtype=int64, shape=torch.Size([1, 300])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: 'labels' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) with 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([263.2104,  66.3191, 542.1085,  ..., 399.1575, 463.3348, 479.4284])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([262.9576,  71.5327, 547.7870,  ..., 398.3175, 518.1219, 478.9788])
[I]         trt-runner-N0-05/22/25-09:24:08: boxes | Stats: mean=265.63, std-dev=146.41, var=21436, median=225.59, min=-1.7347 at (0, 5, 0), max=642.1 at (0, 93, 2), avg-magnitude=265.64, p90=479.8, p95=587.85, p99=624.1
[I]             ---- Histogram ----
                Bin Range     |  Num Elems | Visualization
                (-1.73, 63.3) |       72.0 | ########
                (63.3 , 128 ) |       73.0 | ########
                (128  , 193 ) |      245.0 | ############################
                (193  , 258 ) |      342.0 | ########################################
                (258  , 323 ) |      131.0 | ###############
                (323  , 388 ) |      113.0 | #############
                (388  , 453 ) |       53.0 | ######
                (453  , 518 ) |       68.0 | #######
                (518  , 583 ) |       33.0 | ###
                (583  , 649 ) |       70.0 | ########
[I]         onnxrt-runner-N0-05/22/25-09:24:08: boxes | Stats: mean=260.78, std-dev=140.6, var=19769, median=224.33, min=-1.3738 at (0, 105, 0), max=648.52 at (0, 276, 2), avg-magnitude=260.79, p90=479.46, p95=594.98, p99=622.91
[I]             ---- Histogram ----
                Bin Range     |  Num Elems | Visualization
                (-1.73, 63.3) |       62.0 | ######
                (63.3 , 128 ) |       83.0 | ########
                (128  , 193 ) |      240.0 | #########################
                (193  , 258 ) |      374.0 | ########################################
                (258  , 323 ) |      132.0 | ##############
                (323  , 388 ) |      121.0 | ############
                (388  , 453 ) |       40.0 | ####
                (453  , 518 ) |       53.0 | #####
                (518  , 583 ) |       25.0 | ##
                (583  , 649 ) |       70.0 | #######
[I]         Error Metrics: boxes
[I]             Minimum Required Tolerance: elemwise error | [abs=611.99] OR [rel=511.88] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=140.2, std-dev=131.71, var=17348, median=97.637, min=0.0038757 at (0, 0, 3), max=611.99 at (0, 276, 2), avg-magnitude=140.2, p90=313.6, p95=405.55, p99=525.04
[I]                 ---- Histogram ----
                    Bin Range       |  Num Elems | Visualization
                    (0.00388, 61.2) |      471.0 | ########################################
                    (61.2   , 122 ) |      192.0 | ################
                    (122    , 184 ) |      126.0 | ##########
                    (184    , 245 ) |      146.0 | ############
                    (245    , 306 ) |      136.0 | ###########
                    (306    , 367 ) |       45.0 | ###
                    (367    , 428 ) |       34.0 | ##
                    (428    , 490 ) |       23.0 | #
                    (490    , 551 ) |       20.0 | #
                    (551    , 612 ) |        7.0 | 
[I]             Relative Difference | Stats: mean=2.9973, std-dev=26.636, var=709.46, median=0.42143, min=8.1394e-06 at (0, 0, 3), max=511.88 at (0, 127, 0), avg-magnitude=2.9973, p90=1.7255, p95=3.386, p99=20.206
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (8.14e-06, 51.2) |     1190.0 | ########################################
                    (51.2    , 102 ) |        1.0 | 
                    (102     , 154 ) |        4.0 | 
                    (154     , 205 ) |        0.0 | 
                    (205     , 256 ) |        0.0 | 
                    (256     , 307 ) |        0.0 | 
                    (307     , 358 ) |        2.0 | 
                    (358     , 410 ) |        2.0 | 
                    (410     , 461 ) |        0.0 | 
                    (461     , 512 ) |        1.0 | 
[X]         Finished comparing: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [trt-runner-N0-05/22/25-09:24:08] and 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: 'boxes' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: 'scores' (dtype=float32, shape=torch.Size([1, 300])) with 'scores' (dtype=float32, shape=torch.Size([1, 300]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08
[X]             trt-runner-N0-05/22/25-09:24:08     | Mismatched values:
                tensor([0.6434, 0.5649, 0.4689, 0.4231, 0.4166, 0.4133, 0.4059, 0.3863, 0.3576,
                        0.3533, 0.3017, 0.2933, 0.2888, 0.2561, 0.2445, 0.2385, 0.2264, 0.2200,
                        0.1992, 0.1983, 0.1965, 0.1950, 0.1914, 0.1858, 0.1843, 0.1818, 0.1807,
                        0.1796, 0.1794, 0.1758, 0.1744, 0.1743, 0.1693, 0.1679, 0.1670, 0.1665,
                        0.1597, 0.1595, 0.1562, 0.1547, 0.1460, 0.1449, 0.1433, 0.1418, 0.1394,
                        0.1390, 0.1385, 0.1372, 0.1372, 0.1365, 0.1364, 0.1362, 0.1355, 0.1318,
                        0.1313, 0.1306, 0.1303, 0.1296, 0.1287, 0.1275, 0.1272, 0.1258, 0.1253,
                        0.1252, 0.1250, 0.1244, 0.1238, 0.1232, 0.1230, 0.1224, 0.1207, 0.1171,
                        0.1165, 0.1163, 0.1162, 0.1155, 0.1142, 0.1123, 0.1111, 0.1108, 0.1107,
                        0.1092, 0.1086, 0.1085, 0.1083, 0.1064, 0.1045, 0.1033, 0.1023, 0.1014,
                        0.0991, 0.0965, 0.0959, 0.0954, 0.0951, 0.0947, 0.0944, 0.0939, 0.0921,
                        0.0919, 0.0917, 0.0916, 0.0906, 0.0900, 0.0899, 0.0876, 0.0875, 0.0862,
                        0.0847, 0.0846, 0.0836, 0.0835, 0.0832, 0.0828, 0.0826, 0.0818, 0.0817,
                        0.0813, 0.0796, 0.0786, 0.0785, 0.0778, 0.0776, 0.0776, 0.0774, 0.0765,
                        0.0764, 0.0762, 0.0757, 0.0751, 0.0742, 0.0741, 0.0739, 0.0738, 0.0735,
                        0.0732, 0.0732, 0.0732, 0.0728, 0.0724, 0.0724, 0.0715, 0.0712, 0.0709,
                        0.0709, 0.0708, 0.0705, 0.0702, 0.0699, 0.0698, 0.0696, 0.0685, 0.0685,
                        0.0684, 0.0683, 0.0682, 0.0676, 0.0675, 0.0673, 0.0673, 0.0667, 0.0667,
                        0.0667, 0.0664, 0.0664, 0.0663, 0.0657, 0.0654, 0.0650, 0.0650, 0.0644,
                        0.0641, 0.0640, 0.0638, 0.0636, 0.0633, 0.0630, 0.0622, 0.0611, 0.0607,
                        0.0605, 0.0604, 0.0601, 0.0599, 0.0599, 0.0598, 0.0598, 0.0593, 0.0593,
                        0.0582, 0.0582, 0.0581, 0.0580, 0.0575, 0.0575, 0.0573, 0.0571, 0.0570,
                        0.0570, 0.0570, 0.0568, 0.0566, 0.0565, 0.0561, 0.0560, 0.0554, 0.0548,
                        0.0548, 0.0544, 0.0544, 0.0542, 0.0541, 0.0541, 0.0541, 0.0540, 0.0538,
                        0.0535, 0.0535, 0.0533, 0.0530, 0.0526, 0.0524, 0.0520, 0.0519, 0.0518,
                        0.0518, 0.0514, 0.0512, 0.0510, 0.0508, 0.0507, 0.0507, 0.0507, 0.0506,
                        0.0505, 0.0504, 0.0500, 0.0500, 0.0500, 0.0498, 0.0497, 0.0497, 0.0497,
                        0.0495, 0.0494, 0.0488, 0.0487, 0.0487, 0.0487, 0.0487, 0.0486, 0.0484,
                        0.0482, 0.0481, 0.0479, 0.0479, 0.0476, 0.0476, 0.0476, 0.0476, 0.0474,
                        0.0474, 0.0473, 0.0472, 0.0470, 0.0468, 0.0466, 0.0465, 0.0464, 0.0464,
                        0.0464, 0.0461, 0.0457, 0.0456, 0.0456, 0.0456, 0.0454, 0.0452, 0.0450,
                        0.0449, 0.0446, 0.0444, 0.0443, 0.0442, 0.0440, 0.0440, 0.0439, 0.0437,
                        0.0437, 0.0437, 0.0436, 0.0433, 0.0433, 0.0432, 0.0431, 0.0430, 0.0430,
                        0.0429, 0.0429, 0.0428])
[X]             onnxrt-runner-N0-05/22/25-09:24:08  | Mismatched values:
                tensor([0.6030, 0.5362, 0.5060, 0.4716, 0.4550, 0.4172, 0.4056, 0.3967, 0.3598,
                        0.3229, 0.2707, 0.2680, 0.2555, 0.2454, 0.2324, 0.2316, 0.2067, 0.2027,
                        0.2022, 0.2010, 0.1952, 0.1934, 0.1921, 0.1915, 0.1900, 0.1865, 0.1848,
                        0.1837, 0.1807, 0.1792, 0.1760, 0.1732, 0.1698, 0.1648, 0.1636, 0.1600,
                        0.1595, 0.1585, 0.1555, 0.1547, 0.1529, 0.1523, 0.1518, 0.1502, 0.1478,
                        0.1432, 0.1393, 0.1369, 0.1360, 0.1356, 0.1341, 0.1318, 0.1289, 0.1285,
                        0.1260, 0.1236, 0.1187, 0.1185, 0.1182, 0.1180, 0.1179, 0.1175, 0.1174,
                        0.1169, 0.1167, 0.1138, 0.1123, 0.1119, 0.1114, 0.1112, 0.1103, 0.1103,
                        0.1102, 0.1101, 0.1092, 0.1092, 0.1083, 0.1081, 0.1059, 0.1050, 0.1040,
                        0.1031, 0.1025, 0.1021, 0.1009, 0.0988, 0.0972, 0.0961, 0.0952, 0.0948,
                        0.0943, 0.0942, 0.0942, 0.0942, 0.0940, 0.0934, 0.0934, 0.0932, 0.0930,
                        0.0925, 0.0924, 0.0922, 0.0921, 0.0915, 0.0913, 0.0910, 0.0907, 0.0899,
                        0.0898, 0.0883, 0.0872, 0.0868, 0.0864, 0.0864, 0.0859, 0.0849, 0.0835,
                        0.0833, 0.0830, 0.0819, 0.0814, 0.0810, 0.0804, 0.0800, 0.0790, 0.0788,
                        0.0786, 0.0780, 0.0779, 0.0776, 0.0776, 0.0773, 0.0771, 0.0770, 0.0766,
                        0.0765, 0.0760, 0.0759, 0.0752, 0.0751, 0.0749, 0.0748, 0.0744, 0.0742,
                        0.0740, 0.0736, 0.0732, 0.0732, 0.0731, 0.0730, 0.0722, 0.0718, 0.0717,
                        0.0717, 0.0710, 0.0706, 0.0705, 0.0693, 0.0692, 0.0691, 0.0686, 0.0684,
                        0.0680, 0.0678, 0.0677, 0.0677, 0.0672, 0.0670, 0.0656, 0.0656, 0.0655,
                        0.0655, 0.0654, 0.0654, 0.0652, 0.0645, 0.0640, 0.0639, 0.0638, 0.0631,
                        0.0628, 0.0626, 0.0619, 0.0617, 0.0612, 0.0612, 0.0611, 0.0610, 0.0609,
                        0.0606, 0.0606, 0.0604, 0.0604, 0.0604, 0.0604, 0.0601, 0.0597, 0.0597,
                        0.0597, 0.0596, 0.0595, 0.0593, 0.0589, 0.0586, 0.0586, 0.0586, 0.0585,
                        0.0578, 0.0574, 0.0573, 0.0573, 0.0571, 0.0570, 0.0568, 0.0567, 0.0562,
                        0.0557, 0.0556, 0.0555, 0.0555, 0.0555, 0.0544, 0.0542, 0.0535, 0.0533,
                        0.0533, 0.0531, 0.0530, 0.0527, 0.0525, 0.0522, 0.0521, 0.0519, 0.0518,
                        0.0517, 0.0515, 0.0514, 0.0512, 0.0507, 0.0507, 0.0507, 0.0505, 0.0504,
                        0.0503, 0.0502, 0.0502, 0.0499, 0.0496, 0.0495, 0.0487, 0.0487, 0.0485,
                        0.0484, 0.0482, 0.0480, 0.0480, 0.0480, 0.0476, 0.0472, 0.0467, 0.0466,
                        0.0462, 0.0460, 0.0460, 0.0460, 0.0459, 0.0456, 0.0455, 0.0455, 0.0455,
                        0.0454, 0.0451, 0.0451, 0.0445, 0.0444, 0.0444, 0.0443, 0.0441, 0.0441,
                        0.0440, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437, 0.0436, 0.0435,
                        0.0432, 0.0430, 0.0429, 0.0426, 0.0426, 0.0425, 0.0423, 0.0419, 0.0419,
                        0.0419, 0.0417, 0.0417])
[I]         trt-runner-N0-05/22/25-09:24:08: scores | Stats: mean=0.098229, std-dev=0.081519, var=0.0066453, median=0.069714, min=0.0428 at (0, 299), max=0.64337 at (0, 0), avg-magnitude=0.098229, p90=0.17452, p95=0.23882, p99=0.42361
[I]             ---- Histogram ----
                Bin Range       |  Num Elems | Visualization
                (0.0417, 0.102) |      211.0 | ########################################
                (0.102 , 0.162) |       53.0 | ##########
                (0.162 , 0.222) |       19.0 | ###
                (0.222 , 0.282) |        4.0 | 
                (0.282 , 0.343) |        3.0 | 
                (0.343 , 0.403) |        3.0 | 
                (0.403 , 0.463) |        4.0 | 
                (0.463 , 0.523) |        1.0 | 
                (0.523 , 0.583) |        1.0 | 
                (0.583 , 0.643) |        1.0 | 
[I]         onnxrt-runner-N0-05/22/25-09:24:08: scores | Stats: mean=0.097993, std-dev=0.080413, var=0.0064662, median=0.072622, min=0.041714 at (0, 299), max=0.60297 at (0, 0), avg-magnitude=0.097993, p90=0.17633, p95=0.23168, p99=0.47193
[I]             ---- Histogram ----
                Bin Range       |  Num Elems | Visualization
                (0.0417, 0.102) |      216.0 | ########################################
                (0.102 , 0.162) |       49.0 | #########
                (0.162 , 0.222) |       19.0 | ###
                (0.222 , 0.282) |        6.0 | #
                (0.282 , 0.343) |        1.0 | 
                (0.343 , 0.403) |        2.0 | 
                (0.403 , 0.463) |        3.0 | 
                (0.463 , 0.523) |        2.0 | 
                (0.523 , 0.583) |        1.0 | 
                (0.583 , 0.643) |        1.0 | 
[I]         Error Metrics: scores
[I]             Minimum Required Tolerance: elemwise error | [abs=0.048436] OR [rel=0.13029] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.0039556, std-dev=0.0062498, var=3.9061e-05, median=0.0022789, min=1.6134e-05 at (0, 257), max=0.048436 at (0, 3), avg-magnitude=0.0039556, p90=0.0082962, p95=0.011316, p99=0.037096
[I]                 ---- Histogram ----
                    Bin Range           |  Num Elems | Visualization
                    (1.61e-05, 0.00486) |      240.0 | ########################################
                    (0.00486 , 0.0097 ) |       37.0 | ######
                    (0.0097  , 0.0145 ) |       12.0 | ##
                    (0.0145  , 0.0194 ) |        1.0 | 
                    (0.0194  , 0.0242 ) |        1.0 | 
                    (0.0242  , 0.0291 ) |        2.0 | 
                    (0.0291  , 0.0339 ) |        3.0 | 
                    (0.0339  , 0.0388 ) |        2.0 | 
                    (0.0388  , 0.0436 ) |        1.0 | 
                    (0.0436  , 0.0484 ) |        1.0 | 
[I]             Relative Difference | Stats: mean=0.034892, std-dev=0.023652, var=0.0005594, median=0.029125, min=0.00017935 at (0, 39), max=0.13029 at (0, 12), avg-magnitude=0.034892, p90=0.067039, p95=0.085707, p99=0.10272
[I]                 ---- Histogram ----
                    Bin Range          |  Num Elems | Visualization
                    (0.000179, 0.0132) |       47.0 | #######################
                    (0.0132  , 0.0262) |       80.0 | ########################################
                    (0.0262  , 0.0392) |       63.0 | ###############################
                    (0.0392  , 0.0522) |       57.0 | ############################
                    (0.0522  , 0.0652) |       22.0 | ###########
                    (0.0652  , 0.0782) |       12.0 | ######
                    (0.0782  , 0.0913) |        5.0 | ##
                    (0.0913  , 0.104 ) |       12.0 | ######
                    (0.104   , 0.117 ) |        1.0 | 
                    (0.117   , 0.13  ) |        1.0 | 
[X]         Finished comparing: 'scores' (dtype=float32, shape=torch.Size([1, 300])) [trt-runner-N0-05/22/25-09:24:08] and 'scores' (dtype=float32, shape=torch.Size([1, 300])) [onnxrt-runner-N0-05/22/25-09:24:08]
[E]         FAILED | Output: 'scores' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[E]     FAILED | Mismatched outputs: ['/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0', '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0', '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0', '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0', '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0', '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0', 'labels', 'boxes', 'scores']
[X]     Finished comparing trt-runner-N0-05/22/25-09:24:08 with onnxrt-runner-N0-05/22/25-09:24:08
[E] Accuracy Summary | trt-runner-N0-05/22/25-09:24:08 vs. onnxrt-runner-N0-05/22/25-09:24:08 | Passed: 0/1 iterations | Pass Rate: 0.0%
