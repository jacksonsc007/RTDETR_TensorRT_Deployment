[96m[INFO] Origianl output [0m
['labels', 'boxes', 'scores']
[38;5;13m[V] Marking all ONNX tensors as outputs[0m
[96m[INFO] output to be compared [0m
['/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0',
 '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0',
 'labels',
 'boxes',
 'scores']
[38;5;14m[I] trt-runner-N1-05/19/25-15:35:51     | Activating and starting inference[0m
[38;5;13m[V] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.[0m
[38;5;104m[X] CUDA lazy loading is enabled.[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Clip_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CoordConvAC version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResizeDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResize version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DetectionLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::FlattenConcat_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GenerateDetection_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchor_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 3[0m
[38;5;104m[X] Plugin creator already registered - ::LReLU_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Normalize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PillarScatterPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PriorBox_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Proposal version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Region_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ResizeNearest_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::RPROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 2[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterND version 1[0m
[38;5;104m[X] Plugin creator already registered - ::SpecialSlice_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Split version 1[0m
[38;5;104m[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1[0m
[38;5;13m[V] ----------------------------------------------------------------[0m
[38;5;13m[V] Input filename:   default_mtq_int8_q_qint8mark_outputs_of_fused_nodes-output_modified.onnx[0m
[38;5;13m[V] ONNX IR version:  0.0.8[0m
[38;5;13m[V] Opset version:    17[0m
[38;5;13m[V] Producer name:    pytorch[0m
[38;5;13m[V] Producer version: 2.5.0[0m
[38;5;13m[V] Domain:           [0m
[38;5;13m[V] Model version:    0[0m
[38;5;13m[V] Doc string:       [0m
[38;5;13m[V] ----------------------------------------------------------------[0m
[38;5;104m[X] Adding network input: images with dtype: float32, dimensions: (1, 3, 640, 640)[0m
[38;5;104m[X] Registering tensor: images for ONNX tensor: images[0m
[38;5;104m[X] Adding network input: orig_target_sizes with dtype: int64, dimensions: (1, 2)[0m
[38;5;104m[X] Registering tensor: orig_target_sizes for ONNX tensor: orig_target_sizes[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.anchors[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_score_head.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_score_head.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_score_head.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: onnx::Add_3614[0m
[38;5;104m[X] Importing initializer: onnx::Add_3616[0m
[38;5;104m[X] Importing initializer: onnx::Add_3618[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3619[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3620[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3621[0m
[38;5;104m[X] Importing initializer: onnx::Mul_3692[0m
[38;5;104m[X] Importing initializer: onnx::Add_3731[0m
[38;5;104m[X] Importing initializer: onnx::Add_3733[0m
[38;5;104m[X] Importing initializer: onnx::Add_3735[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3736[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3737[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3738[0m
[38;5;104m[X] Importing initializer: onnx::Mul_3755[0m
[38;5;104m[X] Importing initializer: onnx::Add_3803[0m
[38;5;104m[X] Importing initializer: onnx::Add_3805[0m
[38;5;104m[X] Importing initializer: onnx::Add_3807[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3808[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3809[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3810[0m
[38;5;104m[X] Importing initializer: onnx::Add_3875[0m
[38;5;104m[X] Importing initializer: onnx::Add_3877[0m
[38;5;104m[X] Importing initializer: onnx::Add_3879[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3880[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3881[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3882[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/Constant_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Split_2305[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /postprocessor/Constant_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Tile_3498[0m
[38;5;104m[X] Importing initializer: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] Importing initializer: _v_4326[0m
[38;5;104m[X] Importing initializer: _v_1997[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Concat_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Concat_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Importing initializer: _v_1846[0m
[38;5;104m[X] Importing initializer: _v_1848[0m
[38;5;104m[X] Importing initializer: _v_1850[0m
[38;5;104m[X] Importing initializer: _v_1749[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] Importing initializer: _v_1663[0m
[38;5;104m[X] Importing initializer: _v_1665[0m
[38;5;104m[X] Importing initializer: _v_1669[0m
[38;5;104m[X] Importing initializer: _v_1675[0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: images[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [images -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight for ONNX node: tmp_weight[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_0 for ONNX node: tmp_weight_0[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_1.conv.weight -> (32, 3, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1 for ONNX node: tmp_weight_1[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_2 for ONNX node: tmp_weight_2[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/Conv for ONNX node: /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_1.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_var -> (32)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/act/Relu for ONNX node: /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_3 for ONNX node: tmp_weight_3[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_4 for ONNX node: tmp_weight_4[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_2.conv.weight -> (32, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_5 for ONNX node: tmp_weight_5[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_6 for ONNX node: tmp_weight_6[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/Conv for ONNX node: /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_2.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_var -> (32)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/act/Relu for ONNX node: /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_7 for ONNX node: tmp_weight_7[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_8 for ONNX node: tmp_weight_8[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_3.conv.weight -> (64, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_9 for ONNX node: tmp_weight_9[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_10 for ONNX node: tmp_weight_10[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/Conv for ONNX node: /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_3.norm.weight -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.bias -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_mean -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/act/Relu for ONNX node: /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/MaxPool [MaxPool][0m
[38;5;104m[X] Parsing node: /model/backbone/MaxPool [MaxPool][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/MaxPool [MaxPool] inputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/MaxPool for ONNX node: /model/backbone/MaxPool[0m
[38;5;104m[X] Registering tensor: /model/backbone/MaxPool_output_0 for ONNX tensor: /model/backbone/MaxPool_output_0[0m
[38;5;104m[X] /model/backbone/MaxPool [MaxPool] outputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/MaxPool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_11 for ONNX node: tmp_weight_11[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_12 for ONNX node: tmp_weight_12[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_13 for ONNX node: tmp_weight_13[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_14 for ONNX node: tmp_weight_14[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_15 for ONNX node: tmp_weight_15[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_16 for ONNX node: tmp_weight_16[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_17 for ONNX node: tmp_weight_17[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_18 for ONNX node: tmp_weight_18[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.short.conv.weight -> (64, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.short.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_19 for ONNX node: tmp_weight_19[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_20 for ONNX node: tmp_weight_20[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/Add for ONNX node: /model/backbone/res_layers.0/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_21 for ONNX node: tmp_weight_21[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_22 for ONNX node: tmp_weight_22[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_23 for ONNX node: tmp_weight_23[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_24 for ONNX node: tmp_weight_24[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_25 for ONNX node: tmp_weight_25[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_26 for ONNX node: tmp_weight_26[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_27 for ONNX node: tmp_weight_27[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_28 for ONNX node: tmp_weight_28[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/Add for ONNX node: /model/backbone/res_layers.0/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_29 for ONNX node: tmp_weight_29[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_30 for ONNX node: tmp_weight_30[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2a.conv.weight -> (128, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_31 for ONNX node: tmp_weight_31[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_32 for ONNX node: tmp_weight_32[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_33 for ONNX node: tmp_weight_33[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_34 for ONNX node: tmp_weight_34[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_35 for ONNX node: tmp_weight_35[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_36 for ONNX node: tmp_weight_36[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_37 for ONNX node: tmp_weight_37[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_38 for ONNX node: tmp_weight_38[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.short.conv.conv.weight -> (128, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_39 for ONNX node: tmp_weight_39[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_40 for ONNX node: tmp_weight_40[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/Add for ONNX node: /model/backbone/res_layers.1/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_41 for ONNX node: tmp_weight_41[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_42 for ONNX node: tmp_weight_42[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2a.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_43 for ONNX node: tmp_weight_43[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_44 for ONNX node: tmp_weight_44[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_45 for ONNX node: tmp_weight_45[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_46 for ONNX node: tmp_weight_46[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_47 for ONNX node: tmp_weight_47[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_48 for ONNX node: tmp_weight_48[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/Add for ONNX node: /model/backbone/res_layers.1/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_49 for ONNX node: tmp_weight_49[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_50 for ONNX node: tmp_weight_50[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2a.conv.weight -> (256, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_51 for ONNX node: tmp_weight_51[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_52 for ONNX node: tmp_weight_52[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_53 for ONNX node: tmp_weight_53[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_54 for ONNX node: tmp_weight_54[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_55 for ONNX node: tmp_weight_55[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_56 for ONNX node: tmp_weight_56[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_57 for ONNX node: tmp_weight_57[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_58 for ONNX node: tmp_weight_58[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.short.conv.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_59 for ONNX node: tmp_weight_59[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_60 for ONNX node: tmp_weight_60[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/Add for ONNX node: /model/backbone/res_layers.2/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_61 for ONNX node: tmp_weight_61[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_62 for ONNX node: tmp_weight_62[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2a.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_63 for ONNX node: tmp_weight_63[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_64 for ONNX node: tmp_weight_64[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_65 for ONNX node: tmp_weight_65[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_66 for ONNX node: tmp_weight_66[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_67 for ONNX node: tmp_weight_67[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_68 for ONNX node: tmp_weight_68[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/Add for ONNX node: /model/backbone/res_layers.2/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_69 for ONNX node: tmp_weight_69[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_70 for ONNX node: tmp_weight_70[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2a.conv.weight -> (512, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_71 for ONNX node: tmp_weight_71[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_72 for ONNX node: tmp_weight_72[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_73 for ONNX node: tmp_weight_73[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_74 for ONNX node: tmp_weight_74[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_75 for ONNX node: tmp_weight_75[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_76 for ONNX node: tmp_weight_76[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_77 for ONNX node: tmp_weight_77[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_78 for ONNX node: tmp_weight_78[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.short.conv.conv.weight -> (512, 256, 1, 1)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_79 for ONNX node: tmp_weight_79[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_80 for ONNX node: tmp_weight_80[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/Add for ONNX node: /model/backbone/res_layers.3/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_81 for ONNX node: tmp_weight_81[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_82 for ONNX node: tmp_weight_82[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2a.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_83 for ONNX node: tmp_weight_83[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_84 for ONNX node: tmp_weight_84[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_85 for ONNX node: tmp_weight_85[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_86 for ONNX node: tmp_weight_86[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_87 for ONNX node: tmp_weight_87[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_88 for ONNX node: tmp_weight_88[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/Add for ONNX node: /model/backbone/res_layers.3/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.0.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_89 for ONNX node: tmp_weight_89[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_90 for ONNX node: tmp_weight_90[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/Conv for ONNX node: /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/Conv [Conv] outputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_91 for ONNX node: tmp_weight_91[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_92 for ONNX node: tmp_weight_92[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/Conv for ONNX node: /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/Conv [Conv] outputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_93 for ONNX node: tmp_weight_93[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_94 for ONNX node: tmp_weight_94[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.2.conv.weight -> (256, 512, 1, 1)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_95 for ONNX node: tmp_weight_95[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_96 for ONNX node: tmp_weight_96[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/Conv [Conv] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/Conv for ONNX node: /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/Conv [Conv] outputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/encoder/Reshape [Reshape] inputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/Reshape for ONNX node: /model/encoder/Reshape[0m
[38;5;104m[X] Registering tensor: /model/encoder/Reshape_output_0 for ONNX tensor: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/Reshape [Reshape] outputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/Transpose [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Transpose for ONNX node: /model/encoder/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/Transpose_output_0 for ONNX tensor: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/Transpose [Transpose] outputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/Constant_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add for ONNX node: /model/encoder/encoder.0/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3619[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3619 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3619 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_97 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3614[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3614 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3614 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_98 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_99 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3620[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3620 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3620 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_100 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_101 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3616[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3616 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3616 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_102 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_103 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3621[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3621 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3621 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_104 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_105 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3618[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3618 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3618 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_106 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_107 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_108 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_109 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_110 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_111 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_112 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_113 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_114 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_115 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_116 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_117 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/Add_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_120 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_121 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_122 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_123 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_124 for ONNX node: tmp_weight_124[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_125 for ONNX node: tmp_weight_125[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_126 for ONNX node: tmp_weight_126[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_127 for ONNX node: tmp_weight_127[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_128 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_129 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_130 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_131 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_132 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_133 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Div for ONNX node: /model/encoder/encoder.0/layers.0/activation/Div[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] outputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Div_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] inputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Erf for ONNX node: /model/encoder/encoder.0/layers.0/activation/Erf[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] outputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Erf_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] inputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_134 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_135 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Add for ONNX node: /model/encoder/encoder.0/layers.0/activation/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_136 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_137 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_138 for ONNX node: tmp_weight_138[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_139 for ONNX node: tmp_weight_139[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_140 for ONNX node: tmp_weight_140[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_141 for ONNX node: tmp_weight_141[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_142 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_143 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear2.bias -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_144 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_145 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/Add_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_2_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_148 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_149 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_150 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_151 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Transpose_1 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Transpose_1 for ONNX node: /model/encoder/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/Transpose_1_output_0 for ONNX tensor: /model/encoder/Transpose_1_output_0[0m
[38;5;104m[X] /model/encoder/Transpose_1 [Transpose] outputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_1_output_0[0m
[38;5;104m[X] /model/encoder/Reshape_1 [Reshape] inputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], [/model/encoder/Concat_1_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_152 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/Reshape_1 for ONNX node: /model/encoder/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/Reshape_1_output_0 for ONNX tensor: /model/encoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/Reshape_1 [Reshape] outputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_153 for ONNX node: tmp_weight_153[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_154 for ONNX node: tmp_weight_154[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.lateral_convs.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_155 for ONNX node: tmp_weight_155[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_156 for ONNX node: tmp_weight_156[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/Conv for ONNX node: /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.lateral_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/act/Mul for ONNX node: /model/encoder/lateral_convs.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Resize [Resize][0m
[38;5;104m[X] Parsing node: /model/encoder/Resize [Resize][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] /model/encoder/Resize [Resize] inputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Resize for ONNX node: /model/encoder/Resize[0m
[38;5;104m[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest[0m
[38;5;104m[X] Registering tensor: /model/encoder/Resize_output_0 for ONNX tensor: /model/encoder/Resize_output_0[0m
[38;5;104m[X] /model/encoder/Resize [Resize] outputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_2 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_2 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/Resize_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Concat_2 [Concat] inputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_2 for ONNX node: /model/encoder/Concat_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_2_output_0 for ONNX tensor: /model/encoder/Concat_2_output_0[0m
[38;5;104m[X] /model/encoder/Concat_2 [Concat] outputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_157 for ONNX node: tmp_weight_157[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_158 for ONNX node: tmp_weight_158[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_159 for ONNX node: tmp_weight_159[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_160 for ONNX node: tmp_weight_160[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_161 for ONNX node: tmp_weight_161[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_162 for ONNX node: tmp_weight_162[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_163 for ONNX node: tmp_weight_163[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_164 for ONNX node: tmp_weight_164[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_165 for ONNX node: tmp_weight_165[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_166 for ONNX node: tmp_weight_166[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_167 for ONNX node: tmp_weight_167[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_168 for ONNX node: tmp_weight_168[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_169 for ONNX node: tmp_weight_169[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_170 for ONNX node: tmp_weight_170[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_171 for ONNX node: tmp_weight_171[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_172 for ONNX node: tmp_weight_172[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_173 for ONNX node: tmp_weight_173[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_174 for ONNX node: tmp_weight_174[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/Add [Add] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/Add for ONNX node: /model/encoder/fpn_blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/Add [Add] outputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_175 for ONNX node: tmp_weight_175[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_176 for ONNX node: tmp_weight_176[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_177 for ONNX node: tmp_weight_177[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_178 for ONNX node: tmp_weight_178[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_179 for ONNX node: tmp_weight_179[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_180 for ONNX node: tmp_weight_180[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.lateral_convs.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_181 for ONNX node: tmp_weight_181[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_182 for ONNX node: tmp_weight_182[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/Conv for ONNX node: /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.lateral_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/act/Mul for ONNX node: /model/encoder/lateral_convs.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Resize_1 [Resize][0m
[38;5;104m[X] Parsing node: /model/encoder/Resize_1 [Resize][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] /model/encoder/Resize_1 [Resize] inputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Resize_1 for ONNX node: /model/encoder/Resize_1[0m
[38;5;104m[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest[0m
[38;5;104m[X] Registering tensor: /model/encoder/Resize_1_output_0 for ONNX tensor: /model/encoder/Resize_1_output_0[0m
[38;5;104m[X] /model/encoder/Resize_1 [Resize] outputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_3 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_3 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/Resize_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Concat_3 [Concat] inputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_3 for ONNX node: /model/encoder/Concat_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_3_output_0 for ONNX tensor: /model/encoder/Concat_3_output_0[0m
[38;5;104m[X] /model/encoder/Concat_3 [Concat] outputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_183 for ONNX node: tmp_weight_183[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_184 for ONNX node: tmp_weight_184[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_185 for ONNX node: tmp_weight_185[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_186 for ONNX node: tmp_weight_186[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_187 for ONNX node: tmp_weight_187[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_188 for ONNX node: tmp_weight_188[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_189 for ONNX node: tmp_weight_189[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_190 for ONNX node: tmp_weight_190[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_191 for ONNX node: tmp_weight_191[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_192 for ONNX node: tmp_weight_192[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_193 for ONNX node: tmp_weight_193[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_194 for ONNX node: tmp_weight_194[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_195 for ONNX node: tmp_weight_195[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_196 for ONNX node: tmp_weight_196[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_197 for ONNX node: tmp_weight_197[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_198 for ONNX node: tmp_weight_198[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_199 for ONNX node: tmp_weight_199[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_200 for ONNX node: tmp_weight_200[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/Add [Add] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/Add for ONNX node: /model/encoder/fpn_blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/Add_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/Add [Add] outputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_201 for ONNX node: tmp_weight_201[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_202 for ONNX node: tmp_weight_202[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_203 for ONNX node: tmp_weight_203[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_204 for ONNX node: tmp_weight_204[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_205 for ONNX node: tmp_weight_205[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_206 for ONNX node: tmp_weight_206[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.0.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.downsample_convs.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_207 for ONNX node: tmp_weight_207[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_208 for ONNX node: tmp_weight_208[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/Conv for ONNX node: /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.downsample_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/act/Mul for ONNX node: /model/encoder/downsample_convs.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_4 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_4 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/Concat_4 [Concat] inputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_4 for ONNX node: /model/encoder/Concat_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_4_output_0 for ONNX tensor: /model/encoder/Concat_4_output_0[0m
[38;5;104m[X] /model/encoder/Concat_4 [Concat] outputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_4_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_209 for ONNX node: tmp_weight_209[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_210 for ONNX node: tmp_weight_210[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_211 for ONNX node: tmp_weight_211[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_212 for ONNX node: tmp_weight_212[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_213 for ONNX node: tmp_weight_213[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_214 for ONNX node: tmp_weight_214[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_215 for ONNX node: tmp_weight_215[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_216 for ONNX node: tmp_weight_216[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_217 for ONNX node: tmp_weight_217[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_218 for ONNX node: tmp_weight_218[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_219 for ONNX node: tmp_weight_219[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_220 for ONNX node: tmp_weight_220[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_221 for ONNX node: tmp_weight_221[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_222 for ONNX node: tmp_weight_222[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_223 for ONNX node: tmp_weight_223[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_224 for ONNX node: tmp_weight_224[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_225 for ONNX node: tmp_weight_225[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_226 for ONNX node: tmp_weight_226[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/Add [Add] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/Add for ONNX node: /model/encoder/pan_blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/Add [Add] outputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_227 for ONNX node: tmp_weight_227[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_228 for ONNX node: tmp_weight_228[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_229 for ONNX node: tmp_weight_229[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_230 for ONNX node: tmp_weight_230[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_231 for ONNX node: tmp_weight_231[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_232 for ONNX node: tmp_weight_232[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.1.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.downsample_convs.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_233 for ONNX node: tmp_weight_233[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_234 for ONNX node: tmp_weight_234[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/Conv for ONNX node: /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.downsample_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/act/Mul for ONNX node: /model/encoder/downsample_convs.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_5 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_5 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/Concat_5 [Concat] inputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_5 for ONNX node: /model/encoder/Concat_5[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_5_output_0 for ONNX tensor: /model/encoder/Concat_5_output_0[0m
[38;5;104m[X] /model/encoder/Concat_5 [Concat] outputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_235 for ONNX node: tmp_weight_235[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_236 for ONNX node: tmp_weight_236[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_237 for ONNX node: tmp_weight_237[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_238 for ONNX node: tmp_weight_238[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_239 for ONNX node: tmp_weight_239[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_240 for ONNX node: tmp_weight_240[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_241 for ONNX node: tmp_weight_241[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_242 for ONNX node: tmp_weight_242[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_243 for ONNX node: tmp_weight_243[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_244 for ONNX node: tmp_weight_244[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_245 for ONNX node: tmp_weight_245[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_246 for ONNX node: tmp_weight_246[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_247 for ONNX node: tmp_weight_247[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_248 for ONNX node: tmp_weight_248[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_249 for ONNX node: tmp_weight_249[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_250 for ONNX node: tmp_weight_250[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_251 for ONNX node: tmp_weight_251[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_252 for ONNX node: tmp_weight_252[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/Add [Add] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/Add for ONNX node: /model/encoder/pan_blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/Add_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/Add [Add] outputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_253 for ONNX node: tmp_weight_253[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_254 for ONNX node: tmp_weight_254[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_255 for ONNX node: tmp_weight_255[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_256 for ONNX node: tmp_weight_256[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_257 for ONNX node: tmp_weight_257[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_258 for ONNX node: tmp_weight_258[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/Conv for ONNX node: /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/Conv [Conv] outputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.decoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_259 for ONNX node: tmp_weight_259[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_260 for ONNX node: tmp_weight_260[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/Conv for ONNX node: /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/Conv [Conv] outputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.decoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_261 for ONNX node: tmp_weight_261[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_262 for ONNX node: tmp_weight_262[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.2.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_263 for ONNX node: tmp_weight_263[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_264 for ONNX node: tmp_weight_264[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/Conv [Conv] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/Conv for ONNX node: /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/Conv [Conv] outputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.decoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape [Reshape] inputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_265 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape for ONNX node: /model/decoder/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_output_0 for ONNX tensor: /model/decoder/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/Reshape [Reshape] outputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/Transpose [Transpose] inputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose for ONNX node: /model/decoder/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_output_0 for ONNX tensor: /model/decoder/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/Transpose [Transpose] outputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape_1 [Reshape] inputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_266 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape_1 for ONNX node: /model/decoder/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_1_output_0 for ONNX tensor: /model/decoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/Reshape_1 [Reshape] outputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_1 [Transpose] inputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose_1 for ONNX node: /model/decoder/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_1_output_0 for ONNX tensor: /model/decoder/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_1 [Transpose] outputs: [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape_2 [Reshape] inputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_267 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape_2 for ONNX node: /model/decoder/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_2_output_0 for ONNX tensor: /model/decoder/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/Reshape_2 [Reshape] outputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_2 [Transpose] inputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose_2 for ONNX node: /model/decoder/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_2_output_0 for ONNX tensor: /model/decoder/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_2 [Transpose] outputs: [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Concat_3 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/Concat_3 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/Concat_3 [Concat] inputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Concat_3 for ONNX node: /model/decoder/Concat_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/Concat_3_output_0 for ONNX tensor: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] /model/decoder/Concat_3 [Concat] outputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/Mul [Mul][0m
[38;5;104m[X] Searching for input: onnx::Mul_3692[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] /model/decoder/Mul [Mul] inputs: [onnx::Mul_3692 -> (1, 8400, 1)[FLOAT]], [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Mul_3692 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Mul for ONNX node: /model/decoder/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/Mul_output_0 for ONNX tensor: /model/decoder/Mul_output_0[0m
[38;5;104m[X] /model/decoder/Mul [Mul] outputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_268 for ONNX node: tmp_weight_268[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_269 for ONNX node: tmp_weight_269[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_output.proj.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_270 for ONNX node: tmp_weight_270[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_271 for ONNX node: tmp_weight_271[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Transpose [Transpose] inputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/Transpose for ONNX node: /model/decoder/enc_output/proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/Transpose_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Transpose [Transpose] outputs: [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/MatMul [MatMul] inputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_272 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_273 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/MatMul for ONNX node: /model/decoder/enc_output/proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/MatMul_output_0 for ONNX tensor: /model/decoder/enc_output/proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/MatMul [MatMul] outputs: [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Add [Add] inputs: [model.decoder.enc_output.proj.bias -> (256)[FLOAT]], [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_274 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_275 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/Add for ONNX node: /model/decoder/enc_output/proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/Add_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Add [Add] outputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/Add_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] inputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [model.decoder.enc_output.norm.weight -> (256)[FLOAT]], [model.decoder.enc_output.norm.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.norm.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.norm.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_278 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_279 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_280 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_281 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/norm/LayerNormalization for ONNX node: /model/decoder/enc_output/norm/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0 for ONNX tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] outputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_282 for ONNX node: tmp_weight_282[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_283 for ONNX node: tmp_weight_283[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_score_head.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_score_head.weight -> (80, 256)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_score_head.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_284 for ONNX node: tmp_weight_284[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_285 for ONNX node: tmp_weight_285[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Transpose [Transpose] inputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/Transpose for ONNX node: /model/decoder/enc_score_head/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/Transpose_output_0 for ONNX tensor: /model/decoder/enc_score_head/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Transpose [Transpose] outputs: [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_286 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_287 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/MatMul for ONNX node: /model/decoder/enc_score_head/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/MatMul_output_0 for ONNX tensor: /model/decoder/enc_score_head/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/MatMul [MatMul] outputs: [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_score_head.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Add [Add] inputs: [model.decoder.enc_score_head.bias -> (80)[FLOAT]], [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_score_head.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_288 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_289 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/Add for ONNX node: /model/decoder/enc_score_head/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/Add_output_0 for ONNX tensor: /model/decoder/enc_score_head/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Add [Add] outputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_290 for ONNX node: tmp_weight_290[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_291 for ONNX node: tmp_weight_291[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_292 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_293 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.0.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_294 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_295 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Add for ONNX node: /model/decoder/enc_bbox_head/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/act/Relu for ONNX node: /model/decoder/enc_bbox_head/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/act/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_296 for ONNX node: tmp_weight_296[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_297 for ONNX node: tmp_weight_297[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_298 for ONNX node: tmp_weight_298[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_299 for ONNX node: tmp_weight_299[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_300 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_301 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_302 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_303 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Add for ONNX node: /model/decoder/enc_bbox_head/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/act_1/Relu for ONNX node: /model/decoder/enc_bbox_head/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_304 for ONNX node: tmp_weight_304[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_305 for ONNX node: tmp_weight_305[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_306 for ONNX node: tmp_weight_306[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_307 for ONNX node: tmp_weight_307[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_308 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_309 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.2.bias -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_310 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_311 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Add for ONNX node: /model/decoder/enc_bbox_head/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.anchors[0m
[38;5;104m[X] /model/decoder/Add [Add] inputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], [model.decoder.anchors -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.anchors required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Add for ONNX node: /model/decoder/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/Add_output_0 for ONNX tensor: /model/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/Add [Add] outputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/ReduceMax [ReduceMax][0m
[38;5;104m[X] Parsing node: /model/decoder/ReduceMax [ReduceMax][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/Add_output_0[0m
[38;5;104m[X] /model/decoder/ReduceMax [ReduceMax] inputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/ReduceMax for ONNX node: /model/decoder/ReduceMax[0m
[38;5;104m[X] Registering tensor: /model/decoder/ReduceMax_output_0 for ONNX tensor: /model/decoder/ReduceMax_output_0[0m
[38;5;104m[X] /model/decoder/ReduceMax [ReduceMax] outputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/TopK [TopK][0m
[38;5;104m[X] Parsing node: /model/decoder/TopK [TopK][0m
[38;5;104m[X] Searching for input: /model/decoder/ReduceMax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] /model/decoder/TopK [TopK] inputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Constant_18_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_convertToScalar required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/TopK for ONNX node: /model/decoder/TopK[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/TopK_output_0 for ONNX tensor: /model/decoder/TopK_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/TopK_output_1 for ONNX tensor: /model/decoder/TopK_output_1[0m
[38;5;104m[X] /model/decoder/TopK [TopK] outputs: [/model/decoder/TopK_output_0 -> (1, 300)[FLOAT]], [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/TopK_output_1[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/Unsqueeze [Unsqueeze] inputs: [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Constant_7_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Unsqueeze for ONNX node: /model/decoder/Unsqueeze[0m
[38;5;104m[X] Registering tensor: /model/decoder/Unsqueeze_output_0 for ONNX tensor: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] /model/decoder/Unsqueeze [Unsqueeze] outputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Tile [Tile][0m
[38;5;104m[X] Parsing node: /model/decoder/Tile [Tile][0m
[38;5;104m[X] Searching for input: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] /model/decoder/Tile [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Tile for ONNX node: /model/decoder/Tile[0m
[38;5;104m[X] Registering tensor: /model/decoder/Tile_output_0 for ONNX tensor: /model/decoder/Tile_output_0[0m
[38;5;104m[X] /model/decoder/Tile [Tile] outputs: [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/GatherElements [GatherElements][0m
[38;5;104m[X] Parsing node: /model/decoder/GatherElements [GatherElements][0m
[38;5;104m[X] Searching for input: /model/decoder/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Tile_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements [GatherElements] inputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_312 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/GatherElements for ONNX node: /model/decoder/GatherElements[0m
[38;5;104m[X] Registering tensor: /model/decoder/GatherElements_output_0 for ONNX tensor: /model/decoder/GatherElements_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements [GatherElements] outputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Tile_1 [Tile][0m
[38;5;104m[X] Parsing node: /model/decoder/Tile_1 [Tile][0m
[38;5;104m[X] Searching for input: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/Tile_1 [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_7_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_313 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Tile_1 for ONNX node: /model/decoder/Tile_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Tile_1_output_0 for ONNX tensor: /model/decoder/Tile_1_output_0[0m
[38;5;104m[X] /model/decoder/Tile_1 [Tile] outputs: [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/GatherElements_1 [GatherElements][0m
[38;5;104m[X] Parsing node: /model/decoder/GatherElements_1 [GatherElements][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Tile_1_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements_1 [GatherElements] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_314 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/GatherElements_1 for ONNX node: /model/decoder/GatherElements_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/GatherElements_1_output_0 for ONNX tensor: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements_1 [GatherElements] outputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid [Sigmoid] inputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid for ONNX node: /model/decoder/decoder/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_315 for ONNX node: tmp_weight_315[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_316 for ONNX node: tmp_weight_316[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.0.weight -> (512, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_317 for ONNX node: tmp_weight_317[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_318 for ONNX node: tmp_weight_318[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_319 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_320 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_321 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_322 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_323 for ONNX node: tmp_weight_323[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_324 for ONNX node: tmp_weight_324[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.1.weight -> (256, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_325 for ONNX node: tmp_weight_325[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_326 for ONNX node: tmp_weight_326[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_327 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_328 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_329 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_330 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add for ONNX node: /model/decoder/decoder/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add [Add] outputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3736[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3736 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3736 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_331 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_332 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3731[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3731 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3731 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_333 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_334 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3737[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3737 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3737 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_335 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_336 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3733[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3733 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3733 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_337 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_338 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3738[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3738 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3738 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_339 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_340 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3735[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3735 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3735 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_341 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_342 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_343 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_344 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_345 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_346 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_347 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_348 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_349 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.0/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_350 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_351 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_352 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_1 [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_1 for ONNX node: /model/decoder/decoder/layers.0/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_355 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_356 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_357 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_358 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_2 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_2 for ONNX node: /model/decoder/decoder/layers.0/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_359 for ONNX node: tmp_weight_359[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_360 for ONNX node: tmp_weight_360[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_361 for ONNX node: tmp_weight_361[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_362 for ONNX node: tmp_weight_362[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_363 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_364 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_365 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_366 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_367 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_368 for ONNX node: tmp_weight_368[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_369 for ONNX node: tmp_weight_369[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_370 for ONNX node: tmp_weight_370[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_371 for ONNX node: tmp_weight_371[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_372 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_373 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_374 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_375 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_376 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0_377 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_378 for ONNX node: tmp_weight_378[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_379 for ONNX node: tmp_weight_379[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_380 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_381 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_382 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_383 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_384 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0_385 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_386 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Mul_3755 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_387 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_388 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: _v_1997 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_389 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_390 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_393 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_394 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_396 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_397 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_398 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_400 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_401 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_402 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_404 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_405 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_407 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_408 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_409 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_410 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_412 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_413 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_414 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_415 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_417 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_418 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_419 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_420 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_421 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: onnx::Unsqueeze_1255 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_422 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_423 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_425 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_426 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_428 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_429 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_431 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_432 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_433 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_435 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_436 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_437 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_439 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_440 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_442 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_443 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_444 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_445 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_447 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_448 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_449 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_450 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_452 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_453 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_454 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_455 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_456 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_457 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_459 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_460 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_462 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_463 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_465 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_466 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_467 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_469 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_470 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_471 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_473 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_474 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_476 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_477 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_478 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_479 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_481 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_482 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_483 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_484 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_486 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_487 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_488 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_489 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_490 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_492 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_493 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_495 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_496 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_498 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_499 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_500 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_502 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_503 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_504 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_506 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_507 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_509 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_510 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_511 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_512 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_514 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_515 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_516 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_517 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_519 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_520 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_521 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_522 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_523 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_525 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_526 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_528 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_529 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_531 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_532 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_533 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_535 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_536 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_537 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_539 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_540 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_542 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_543 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_544 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_545 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_547 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_548 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_549 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_550 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_552 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_553 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_554 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_555 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_556 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_557 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_558 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_559 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_560 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_561 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_562 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_563 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_564 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_565 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_566 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_567 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_568 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_569 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_570 for ONNX node: tmp_weight_570[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_571 for ONNX node: tmp_weight_571[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_572 for ONNX node: tmp_weight_572[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_573 for ONNX node: tmp_weight_573[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_574 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_575 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_576 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_577 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_3 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_3 for ONNX node: /model/decoder/decoder/layers.0/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_3 [Add] outputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_580 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_581 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_582 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_583 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_584 for ONNX node: tmp_weight_584[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_585 for ONNX node: tmp_weight_585[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_586 for ONNX node: tmp_weight_586[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_587 for ONNX node: tmp_weight_587[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_588 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_589 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Add [Add] inputs: [model.decoder.decoder.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_590 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_591 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Add for ONNX node: /model/decoder/decoder/layers.0/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/activation/Relu for ONNX node: /model/decoder/decoder/layers.0/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_592 for ONNX node: tmp_weight_592[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_593 for ONNX node: tmp_weight_593[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_594 for ONNX node: tmp_weight_594[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_595 for ONNX node: tmp_weight_595[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_596 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_597 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Add [Add] inputs: [model.decoder.decoder.layers.0.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_598 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_599 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Add for ONNX node: /model/decoder/decoder/layers.0/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_4 [Add] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_4 for ONNX node: /model/decoder/decoder/layers.0/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_4 [Add] outputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_602 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_603 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_604 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_605 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_606 for ONNX node: tmp_weight_606[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_607 for ONNX node: tmp_weight_607[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_608 for ONNX node: tmp_weight_608[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_609 for ONNX node: tmp_weight_609[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_610 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_611 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_612 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_613 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_614 for ONNX node: tmp_weight_614[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_615 for ONNX node: tmp_weight_615[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_616 for ONNX node: tmp_weight_616[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_617 for ONNX node: tmp_weight_617[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_618 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_619 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_620 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_621 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_622 for ONNX node: tmp_weight_622[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_623 for ONNX node: tmp_weight_623[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_624 for ONNX node: tmp_weight_624[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_625 for ONNX node: tmp_weight_625[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_626 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_627 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_628 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_629 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip [Clip] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip for ONNX node: /model/decoder/decoder/Clip[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_output_0 for ONNX tensor: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip [Clip] outputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_1 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_1 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_1 [Clip] inputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Constant_3_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_631 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_632 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_633 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_634 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_1_output_0 for ONNX tensor: /model/decoder/decoder/Clip_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_1 [Clip] outputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_635 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_636 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub for ONNX node: /model/decoder/decoder/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_output_0 for ONNX tensor: /model/decoder/decoder/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub [Sub] outputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_2 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_2 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_2 [Clip] inputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_638 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_639 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_640 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_641 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_2_output_0 for ONNX tensor: /model/decoder/decoder/Clip_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_2 [Clip] outputs: [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div [Div] inputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div for ONNX node: /model/decoder/decoder/Div[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_output_0 for ONNX tensor: /model/decoder/decoder/Div_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div [Div] outputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log [Log] inputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log for ONNX node: /model/decoder/decoder/Log[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_output_0 for ONNX tensor: /model/decoder/decoder/Log_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log [Log] outputs: [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add [Add] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add for ONNX node: /model/decoder/decoder/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_output_0 for ONNX tensor: /model/decoder/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add [Add] outputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] inputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_1 for ONNX node: /model/decoder/decoder/Sigmoid_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_1_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_642 for ONNX node: tmp_weight_642[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_643 for ONNX node: tmp_weight_643[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_644 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_645 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_646 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_647 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act_1/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_648 for ONNX node: tmp_weight_648[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_649 for ONNX node: tmp_weight_649[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_650 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_651 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_652 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_653 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add for ONNX node: /model/decoder/decoder/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add [Add] outputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3808[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3808 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3808 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_654 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_655 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3803[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] inputs: [onnx::Add_3803 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3803 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_656 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_657 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3809[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3809 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3809 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_658 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_659 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3805[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] inputs: [onnx::Add_3805 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3805 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_660 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_661 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3810[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3810 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3810 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_662 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_663 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3807[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] inputs: [onnx::Add_3807 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3807 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_664 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_665 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_666 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_667 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_668 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_669 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_670 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_671 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_672 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.1/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_673 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_674 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: _v_1675[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1675 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_675 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_1 [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_1 for ONNX node: /model/decoder/decoder/layers.1/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_678 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_679 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_680 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_681 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_2 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_2 for ONNX node: /model/decoder/decoder/layers.1/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_682 for ONNX node: tmp_weight_682[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_683 for ONNX node: tmp_weight_683[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_684 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_685 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_686 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_687 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_688 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_689 for ONNX node: tmp_weight_689[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_690 for ONNX node: tmp_weight_690[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_691 for ONNX node: tmp_weight_691[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_692 for ONNX node: tmp_weight_692[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_693 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_694 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_695 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_696 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_697 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0_698 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_699 for ONNX node: tmp_weight_699[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_700 for ONNX node: tmp_weight_700[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_701 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_702 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_703 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_704 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_705 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0_706 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_707 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_708 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_709 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_710 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_711 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_713 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_714 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_716 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_717 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_719 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_720 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_721 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_723 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_724 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_725 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_727 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_728 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_730 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_731 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_732 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_733 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_735 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_736 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_737 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_738 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_740 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_741 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_742 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_743 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_744 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_745 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_746 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_748 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_749 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_751 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_752 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_754 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_755 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_756 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_758 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_759 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_760 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_762 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_763 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_765 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_766 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_767 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_768 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_770 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_771 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_772 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_773 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_775 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_776 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_777 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_778 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_779 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_780 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_782 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_783 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_785 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_786 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_788 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_789 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_790 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_792 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_793 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_794 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_796 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_797 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_799 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_800 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_801 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_802 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_804 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_805 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_806 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_807 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_809 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_810 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_811 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_812 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_813 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_815 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_816 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_818 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_819 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_821 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_822 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_823 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_825 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_826 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_827 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_829 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_830 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_832 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_833 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_834 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_835 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_837 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_838 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_839 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_840 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_842 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_843 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_844 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_845 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_846 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_848 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_849 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_851 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_852 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_854 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_855 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_856 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_858 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_859 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_860 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_862 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_863 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_865 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_866 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_867 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_868 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_870 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_871 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_872 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_873 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_875 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_876 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_877 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_878 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_879 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_880 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_881 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_882 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_883 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_884 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_885 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_886 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_887 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_888 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_889 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_890 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_891 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_892 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_893 for ONNX node: tmp_weight_893[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_894 for ONNX node: tmp_weight_894[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_895 for ONNX node: tmp_weight_895[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_896 for ONNX node: tmp_weight_896[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_897 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_898 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_899 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_900 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_3 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_3 for ONNX node: /model/decoder/decoder/layers.1/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_3 [Add] outputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_903 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_904 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_905 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_906 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_907 for ONNX node: tmp_weight_907[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_908 for ONNX node: tmp_weight_908[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_909 for ONNX node: tmp_weight_909[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_910 for ONNX node: tmp_weight_910[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_911 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_912 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Add [Add] inputs: [model.decoder.decoder.layers.1.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_913 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_914 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Add for ONNX node: /model/decoder/decoder/layers.1/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/activation/Relu for ONNX node: /model/decoder/decoder/layers.1/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_915 for ONNX node: tmp_weight_915[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_916 for ONNX node: tmp_weight_916[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_917 for ONNX node: tmp_weight_917[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_918 for ONNX node: tmp_weight_918[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_919 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_920 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Add [Add] inputs: [model.decoder.decoder.layers.1.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_921 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_922 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Add for ONNX node: /model/decoder/decoder/layers.1/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_4 [Add] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_4 for ONNX node: /model/decoder/decoder/layers.1/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_4 [Add] outputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_925 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_926 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_927 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_928 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_929 for ONNX node: tmp_weight_929[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_930 for ONNX node: tmp_weight_930[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_931 for ONNX node: tmp_weight_931[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_932 for ONNX node: tmp_weight_932[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_933 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_934 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_935 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_936 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_937 for ONNX node: tmp_weight_937[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_938 for ONNX node: tmp_weight_938[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_939 for ONNX node: tmp_weight_939[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_940 for ONNX node: tmp_weight_940[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_941 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_942 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_943 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_944 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_945 for ONNX node: tmp_weight_945[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_946 for ONNX node: tmp_weight_946[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_947 for ONNX node: tmp_weight_947[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_948 for ONNX node: tmp_weight_948[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_949 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_950 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_951 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_952 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_3 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_3 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_3 [Clip] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip_3 for ONNX node: /model/decoder/decoder/Clip_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_3_output_0 for ONNX tensor: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_3 [Clip] outputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_4 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_4 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_4 [Clip] inputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_954 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_955 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_956 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_957 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_4_output_0 for ONNX tensor: /model/decoder/decoder/Clip_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_4 [Clip] outputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub_1 [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub_1 [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_1 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_958 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_959 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub_1 for ONNX node: /model/decoder/decoder/Sub_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_1_output_0 for ONNX tensor: /model/decoder/decoder/Sub_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_1 [Sub] outputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_5 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_5 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_5 [Clip] inputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_961 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_962 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_963 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_964 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_5_output_0 for ONNX tensor: /model/decoder/decoder/Clip_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_5 [Clip] outputs: [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div_1 [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div_1 [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_1 [Div] inputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div_1 for ONNX node: /model/decoder/decoder/Div_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_1_output_0 for ONNX tensor: /model/decoder/decoder/Div_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_1 [Div] outputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log_1 [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log_1 [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_1 [Log] inputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log_1 for ONNX node: /model/decoder/decoder/Log_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_1_output_0 for ONNX tensor: /model/decoder/decoder/Log_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_1 [Log] outputs: [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_1 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add_1 for ONNX node: /model/decoder/decoder/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_1 [Add] outputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] inputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_2 for ONNX node: /model/decoder/decoder/Sigmoid_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_2_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_965 for ONNX node: tmp_weight_965[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_966 for ONNX node: tmp_weight_966[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_967 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_968 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_969 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_970 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act_2/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_2/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_971 for ONNX node: tmp_weight_971[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_972 for ONNX node: tmp_weight_972[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_973 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_974 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_975 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_976 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add for ONNX node: /model/decoder/decoder/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add [Add] outputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3880[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3880 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3880 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_977 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_978 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3875[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] inputs: [onnx::Add_3875 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3875 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_979 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_980 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3881[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3881 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3881 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_981 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_982 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3877[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] inputs: [onnx::Add_3877 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3877 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_983 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_984 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3882[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3882 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3882 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_985 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_986 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3879[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] inputs: [onnx::Add_3879 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3879 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_987 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_988 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_989 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_990 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_991 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_992 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_993 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_994 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_995 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.2/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_996 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_997 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: _v_1675[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1675 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_998 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_1 [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_1 for ONNX node: /model/decoder/decoder/layers.2/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1001 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1002 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1003 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1004 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_2 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_2 for ONNX node: /model/decoder/decoder/layers.2/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1005 for ONNX node: tmp_weight_1005[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1006 for ONNX node: tmp_weight_1006[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1007 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1008 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1009 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1010 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1011 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1012 for ONNX node: tmp_weight_1012[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1013 for ONNX node: tmp_weight_1013[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1014 for ONNX node: tmp_weight_1014[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1015 for ONNX node: tmp_weight_1015[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1016 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1017 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1018 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1019 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1020 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0_1021 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1022 for ONNX node: tmp_weight_1022[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1023 for ONNX node: tmp_weight_1023[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1024 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1025 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1026 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1027 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1028 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0_1029 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1030 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1031 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1032 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1033 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1034 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1036 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1037 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1039 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1040 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1042 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1043 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1044 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1046 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1047 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1048 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1050 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1051 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1053 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1054 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1055 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1056 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1058 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1059 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1060 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1061 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1063 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1064 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1065 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1066 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1067 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1068 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1069 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1071 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1072 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1074 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1075 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1077 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1078 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1079 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1081 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1082 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1083 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1085 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1086 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1088 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1089 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1090 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1091 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1093 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1094 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1095 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1096 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1098 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1099 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1100 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1101 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1102 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1103 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1105 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1106 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1108 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1109 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1111 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1112 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1113 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1115 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1116 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1117 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1119 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1120 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1122 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1123 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1124 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1125 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1127 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1128 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1129 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1130 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1132 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1133 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1134 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1135 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1136 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1138 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1139 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1141 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1142 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1144 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1145 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1146 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1148 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1149 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1150 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1152 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1153 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1155 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1156 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1157 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1158 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1160 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1161 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1162 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1163 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1165 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1166 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1167 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1168 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1169 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1171 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1172 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1174 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1175 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1177 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1178 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1179 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1181 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1182 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1183 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1185 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1186 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1188 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1189 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1190 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1191 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1193 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1194 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1195 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1196 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1198 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1199 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1200 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1201 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1202 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1203 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1204 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1205 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1206 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1207 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1208 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1209 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1210 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1211 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1212 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1213 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1214 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1215 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1216 for ONNX node: tmp_weight_1216[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1217 for ONNX node: tmp_weight_1217[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1218 for ONNX node: tmp_weight_1218[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1219 for ONNX node: tmp_weight_1219[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1220 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1221 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1222 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1223 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_3 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_3 for ONNX node: /model/decoder/decoder/layers.2/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_3 [Add] outputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1226 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1227 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1228 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1229 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1230 for ONNX node: tmp_weight_1230[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1231 for ONNX node: tmp_weight_1231[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1232 for ONNX node: tmp_weight_1232[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1233 for ONNX node: tmp_weight_1233[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1234 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1235 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Add [Add] inputs: [model.decoder.decoder.layers.2.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1236 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1237 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Add for ONNX node: /model/decoder/decoder/layers.2/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/activation/Relu for ONNX node: /model/decoder/decoder/layers.2/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1238 for ONNX node: tmp_weight_1238[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1239 for ONNX node: tmp_weight_1239[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1240 for ONNX node: tmp_weight_1240[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1241 for ONNX node: tmp_weight_1241[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1242 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1243 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Add [Add] inputs: [model.decoder.decoder.layers.2.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1244 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1245 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Add for ONNX node: /model/decoder/decoder/layers.2/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_4 [Add] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_4 for ONNX node: /model/decoder/decoder/layers.2/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_4 [Add] outputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1248 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1249 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1250 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1251 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1252 for ONNX node: tmp_weight_1252[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1253 for ONNX node: tmp_weight_1253[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1254 for ONNX node: tmp_weight_1254[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1255 for ONNX node: tmp_weight_1255[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1256 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1257 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1258 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1259 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1260 for ONNX node: tmp_weight_1260[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1261 for ONNX node: tmp_weight_1261[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1262 for ONNX node: tmp_weight_1262[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1263 for ONNX node: tmp_weight_1263[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1264 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1265 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1266 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1267 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1268 for ONNX node: tmp_weight_1268[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1269 for ONNX node: tmp_weight_1269[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1270 for ONNX node: tmp_weight_1270[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1271 for ONNX node: tmp_weight_1271[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1272 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1273 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1274 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1275 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_6 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_6 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_6 [Clip] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip_6 for ONNX node: /model/decoder/decoder/Clip_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_6_output_0 for ONNX tensor: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_6 [Clip] outputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_7 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_7 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_7 [Clip] inputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1277 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1278 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1279 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1280 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_7_output_0 for ONNX tensor: /model/decoder/decoder/Clip_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_7 [Clip] outputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub_2 [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub_2 [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_2 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1281 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1282 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub_2 for ONNX node: /model/decoder/decoder/Sub_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_2_output_0 for ONNX tensor: /model/decoder/decoder/Sub_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_2 [Sub] outputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_8 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_8 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_8 [Clip] inputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1284 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1285 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1286 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1287 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_8_output_0 for ONNX tensor: /model/decoder/decoder/Clip_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_8 [Clip] outputs: [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div_2 [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div_2 [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_2 [Div] inputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div_2 for ONNX node: /model/decoder/decoder/Div_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_2_output_0 for ONNX tensor: /model/decoder/decoder/Div_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_2 [Div] outputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log_2 [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log_2 [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_2 [Log] inputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log_2 for ONNX node: /model/decoder/decoder/Log_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_2_output_0 for ONNX tensor: /model/decoder/decoder/Log_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_2 [Log] outputs: [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_2 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add_2 for ONNX node: /model/decoder/decoder/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_2 [Add] outputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] inputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_3 for ONNX node: /model/decoder/decoder/Sigmoid_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_3_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_score_head.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_score_head.2.weight -> (80, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_score_head.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1288 for ONNX node: tmp_weight_1288[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1289 for ONNX node: tmp_weight_1289[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Transpose for ONNX node: /model/decoder/decoder/dec_score_head.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1290 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1291 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/MatMul for ONNX node: /model/decoder/decoder/dec_score_head.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Add [Add] inputs: [model.decoder.dec_score_head.2.bias -> (80)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_score_head.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1292 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1293 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Add for ONNX node: /model/decoder/decoder/dec_score_head.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Add [Add] outputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_3_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Unsqueeze_3 for ONNX node: /model/decoder/decoder/Unsqueeze_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Unsqueeze_3_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Add_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] inputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Unsqueeze_4 for ONNX node: /model/decoder/decoder/Unsqueeze_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Unsqueeze_4_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Gather_8 [Gather][0m
[38;5;104m[X] Parsing node: /model/decoder/Gather_8 [Gather][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Unsqueeze_4_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/Gather_8 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Constant_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Using Gather axis: 0[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1294 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Gather_8 for ONNX node: /model/decoder/Gather_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/Gather_8_output_0 for ONNX tensor: /model/decoder/Gather_8_output_0[0m
[38;5;104m[X] /model/decoder/Gather_8 [Gather] outputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Gather_9 [Gather][0m
[38;5;104m[X] Parsing node: /model/decoder/Gather_9 [Gather][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Unsqueeze_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/Gather_9 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Using Gather axis: 0[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1295 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Gather_9 for ONNX node: /model/decoder/Gather_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/Gather_9_output_0 for ONNX tensor: /model/decoder/Gather_9_output_0[0m
[38;5;104m[X] /model/decoder/Gather_9 [Gather] outputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Split [Split][0m
[38;5;104m[X] Parsing node: /postprocessor/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/Gather_9_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_output_0[0m
[38;5;104m[X] /postprocessor/Split [Split] inputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Constant_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1296 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1297 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1298 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1299 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1300 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1301 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1302 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_0 for ONNX tensor: /postprocessor/Split_output_0[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_1 for ONNX tensor: /postprocessor/Split_output_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_2 for ONNX tensor: /postprocessor/Split_output_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_3 for ONNX tensor: /postprocessor/Split_output_3[0m
[38;5;104m[X] /postprocessor/Split [Split] outputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze [Squeeze] inputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze for ONNX node: /postprocessor/Squeeze[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_output_0 for ONNX tensor: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze [Squeeze] outputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_1 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_1 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_1[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_1 [Squeeze] inputs: [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_1 for ONNX node: /postprocessor/Squeeze_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_1_output_0 for ONNX tensor: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_1 [Squeeze] outputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_2 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_2 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_2[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_2 [Squeeze] inputs: [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_2 for ONNX node: /postprocessor/Squeeze_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_2_output_0 for ONNX tensor: /postprocessor/Squeeze_2_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_2 [Squeeze] outputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_3 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_3 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_3[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_3 [Squeeze] inputs: [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_3 for ONNX node: /postprocessor/Squeeze_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_3_output_0 for ONNX tensor: /postprocessor/Squeeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_3 [Squeeze] outputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul [Mul] inputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1303 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1304 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul for ONNX node: /postprocessor/Mul[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_output_0 for ONNX tensor: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Mul [Mul] outputs: [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Sub [Sub] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub for ONNX node: /postprocessor/Sub[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sub_output_0 for ONNX tensor: /postprocessor/Sub_output_0[0m
[38;5;104m[X] /postprocessor/Sub [Sub] outputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul_1 [Mul] inputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1305 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1306 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_1 for ONNX node: /postprocessor/Mul_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_1_output_0 for ONNX tensor: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Mul_1 [Mul] outputs: [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub_1 [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub_1 [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Sub_1 [Sub] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub_1 for ONNX node: /postprocessor/Sub_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sub_1_output_0 for ONNX tensor: /postprocessor/Sub_1_output_0[0m
[38;5;104m[X] /postprocessor/Sub_1 [Sub] outputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Add [Add][0m
[38;5;104m[X] Parsing node: /postprocessor/Add [Add][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Add [Add] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Add for ONNX node: /postprocessor/Add[0m
[38;5;104m[X] Registering tensor: /postprocessor/Add_output_0 for ONNX tensor: /postprocessor/Add_output_0[0m
[38;5;104m[X] /postprocessor/Add [Add] outputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /postprocessor/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Add_1 [Add] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Add_1 for ONNX node: /postprocessor/Add_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Add_1_output_0 for ONNX tensor: /postprocessor/Add_1_output_0[0m
[38;5;104m[X] /postprocessor/Add_1 [Add] outputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Sub_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze [Unsqueeze] inputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze for ONNX node: /postprocessor/Unsqueeze[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_output_0 for ONNX tensor: /postprocessor/Unsqueeze_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze [Unsqueeze] outputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Sub_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_1 [Unsqueeze] inputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_1 for ONNX node: /postprocessor/Unsqueeze_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_1_output_0 for ONNX tensor: /postprocessor/Unsqueeze_1_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_1 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_2 [Unsqueeze] inputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_2 for ONNX node: /postprocessor/Unsqueeze_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_2_output_0 for ONNX tensor: /postprocessor/Unsqueeze_2_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_2 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_3 [Unsqueeze] inputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_3 for ONNX node: /postprocessor/Unsqueeze_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_3_output_0 for ONNX tensor: /postprocessor/Unsqueeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_3 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Concat [Concat][0m
[38;5;104m[X] Parsing node: /postprocessor/Concat [Concat][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_2_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Concat [Concat] inputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Concat for ONNX node: /postprocessor/Concat[0m
[38;5;104m[X] Registering tensor: /postprocessor/Concat_output_0 for ONNX tensor: /postprocessor/Concat_output_0[0m
[38;5;104m[X] /postprocessor/Concat [Concat] outputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Tile [Tile][0m
[38;5;104m[X] Parsing node: /postprocessor/Tile [Tile][0m
[38;5;104m[X] Searching for input: orig_target_sizes[0m
[38;5;104m[X] Searching for input: onnx::Tile_3498[0m
[38;5;104m[X] /postprocessor/Tile [Tile] inputs: [orig_target_sizes -> (1, 2)[INT64]], [onnx::Tile_3498 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1307 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Tile for ONNX node: /postprocessor/Tile[0m
[38;5;104m[X] Registering tensor: /postprocessor/Tile_output_0 for ONNX tensor: /postprocessor/Tile_output_0[0m
[38;5;104m[X] /postprocessor/Tile [Tile] outputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Tile_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_4 [Unsqueeze] inputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Constant_21_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_4 for ONNX node: /postprocessor/Unsqueeze_4[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_4_output_0 for ONNX tensor: /postprocessor/Unsqueeze_4_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_4 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: Cast_3039 [Cast][0m
[38;5;104m[X] Parsing node: Cast_3039 [Cast][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_4_output_0[0m
[38;5;104m[X] Cast_3039 [Cast] inputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], [0m
[38;5;104m[X] Casting to type: float32[0m
[38;5;104m[X] Registering layer: Cast_3039 for ONNX node: Cast_3039[0m
[38;5;104m[X] Registering tensor: onnx::Mul_3505 for ONNX tensor: onnx::Mul_3505[0m
[38;5;104m[X] Cast_3039 [Cast] outputs: [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Concat_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3505[0m
[38;5;104m[X] /postprocessor/Mul_2 [Mul] inputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_2 for ONNX node: /postprocessor/Mul_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_2_output_0 for ONNX tensor: /postprocessor/Mul_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul_2 [Mul] outputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /postprocessor/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/Gather_8_output_0[0m
[38;5;104m[X] /postprocessor/Sigmoid [Sigmoid] inputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sigmoid for ONNX node: /postprocessor/Sigmoid[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sigmoid_output_0 for ONNX tensor: /postprocessor/Sigmoid_output_0[0m
[38;5;104m[X] /postprocessor/Sigmoid [Sigmoid] outputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Flatten [Flatten][0m
[38;5;104m[X] Parsing node: /postprocessor/Flatten [Flatten][0m
[38;5;104m[X] Searching for input: /postprocessor/Sigmoid_output_0[0m
[38;5;104m[X] /postprocessor/Flatten [Flatten] inputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1308 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Flatten for ONNX node: /postprocessor/Flatten[0m
[38;5;104m[X] Registering tensor: /postprocessor/Flatten_output_0 for ONNX tensor: /postprocessor/Flatten_output_0[0m
[38;5;104m[X] /postprocessor/Flatten [Flatten] outputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/TopK [TopK][0m
[38;5;104m[X] Parsing node: /postprocessor/TopK [TopK][0m
[38;5;104m[X] Searching for input: /postprocessor/Flatten_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] /postprocessor/TopK [TopK] inputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_convertToScalar_1309 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/TopK for ONNX node: /postprocessor/TopK[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1310 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: scores_1311 for ONNX tensor: scores[0m
[38;5;104m[X] Registering tensor: /postprocessor/TopK_output_1 for ONNX tensor: /postprocessor/TopK_output_1[0m
[38;5;104m[X] /postprocessor/TopK [TopK] outputs: [scores -> (1, 300)[FLOAT]], [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Div [Div][0m
[38;5;104m[X] Parsing node: /postprocessor/Div [Div][0m
[38;5;104m[X] Searching for input: /postprocessor/TopK_output_1[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] /postprocessor/Div [Div] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Constant_14_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1312 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1313 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Div for ONNX node: /postprocessor/Div[0m
[38;5;104m[X] Registering tensor: /postprocessor/Div_output_0 for ONNX tensor: /postprocessor/Div_output_0[0m
[38;5;104m[X] /postprocessor/Div [Div] outputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Div_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] /postprocessor/Mul_3 [Mul] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1314 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1315 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_3 for ONNX node: /postprocessor/Mul_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_3_output_0 for ONNX tensor: /postprocessor/Mul_3_output_0[0m
[38;5;104m[X] /postprocessor/Mul_3 [Mul] outputs: [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub_2 [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub_2 [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/TopK_output_1[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_3_output_0[0m
[38;5;104m[X] /postprocessor/Sub_2 [Sub] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub_2 for ONNX node: /postprocessor/Sub_2[0m
[38;5;104m[X] Registering tensor: labels_1316 for ONNX tensor: labels[0m
[38;5;104m[X] /postprocessor/Sub_2 [Sub] outputs: [labels -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Div_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_5 [Unsqueeze] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_5 for ONNX node: /postprocessor/Unsqueeze_5[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_5_output_0 for ONNX tensor: /postprocessor/Unsqueeze_5_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_5 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Tile_1 [Tile][0m
[38;5;104m[X] Parsing node: /postprocessor/Tile_1 [Tile][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] /postprocessor/Tile_1 [Tile] inputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1317 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Tile_1 for ONNX node: /postprocessor/Tile_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Tile_1_output_0 for ONNX tensor: /postprocessor/Tile_1_output_0[0m
[38;5;104m[X] /postprocessor/Tile_1 [Tile] outputs: [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/GatherElements [GatherElements][0m
[38;5;104m[X] Parsing node: /postprocessor/GatherElements [GatherElements][0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_2_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Tile_1_output_0[0m
[38;5;104m[X] /postprocessor/GatherElements [GatherElements] inputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1318 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/GatherElements for ONNX node: /postprocessor/GatherElements[0m
[38;5;104m[X] Registering tensor: boxes_1319 for ONNX tensor: boxes[0m
[38;5;104m[X] /postprocessor/GatherElements [GatherElements] outputs: [boxes -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0_377 as output: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0_698 as output: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0_1021 as output: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0_385 as output: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0_706 as output: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0_1029 as output: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] Marking labels_1316 as output: labels[0m
[38;5;104m[X] Marking boxes_1319 as output: boxes[0m
[38;5;104m[X] Marking scores_1311 as output: scores[0m
[38;5;14m[I] Building engine with configuration:
    Flags                  | [TF32]
    Engine Capability      | EngineCapability.STANDARD
    Memory Pools           | [WORKSPACE: 1024.00 MiB, TACTIC_DRAM: 24105.06 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]
    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]
    Profiling Verbosity    | ProfilingVerbosity.DETAILED
    Preview Features       | [PROFILE_SHARING_0806][0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Original: 1920 layers[0m
[38;5;104m[X] After dead-layer removal: 1920 layers[0m
[38;5;104m[X] Graph construction completed in 0.0184849 seconds.[0m
[38;5;104m[X] After adding DebugOutput nodes: 1920 layers[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3619[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3619 with ONNXTRT_Broadcast[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3614[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3614 with ONNXTRT_Broadcast_99[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3620[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3620 with ONNXTRT_Broadcast_101[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3616[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3616 with ONNXTRT_Broadcast_103[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3621[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3621 with ONNXTRT_Broadcast_105[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3618[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3618 with ONNXTRT_Broadcast_107[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_116[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.weight with ONNXTRT_Broadcast_121[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.bias with ONNXTRT_Broadcast_123[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear1.bias with ONNXTRT_Broadcast_131[0m
[38;5;104m[X] Running: ConstShuffleFusion on /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] ConstShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/activation/Constant_output_0 with ONNXTRT_Broadcast_133[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear2.bias with ONNXTRT_Broadcast_145[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.weight with ONNXTRT_Broadcast_149[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.bias with ONNXTRT_Broadcast_151[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.proj.bias with ONNXTRT_Broadcast_275[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.weight with ONNXTRT_Broadcast_279[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.bias with ONNXTRT_Broadcast_281[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_score_head.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_score_head.bias with ONNXTRT_Broadcast_289[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.0.bias with ONNXTRT_Broadcast_295[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.1.bias with ONNXTRT_Broadcast_303[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.2.bias with ONNXTRT_Broadcast_311[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3736[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3736 with ONNXTRT_Broadcast_332[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3731[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3731 with ONNXTRT_Broadcast_334[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3737[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3737 with ONNXTRT_Broadcast_336[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3733[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3733 with ONNXTRT_Broadcast_338[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3738[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3738 with ONNXTRT_Broadcast_340[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3735[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3735 with ONNXTRT_Broadcast_342[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_351[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.weight with ONNXTRT_Broadcast_356[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.bias with ONNXTRT_Broadcast_358[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.value_proj.bias with ONNXTRT_Broadcast_366[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_375[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_383[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.output_proj.bias with ONNXTRT_Broadcast_577[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.weight with ONNXTRT_Broadcast_581[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.bias with ONNXTRT_Broadcast_583[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear1.bias with ONNXTRT_Broadcast_591[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear2.bias with ONNXTRT_Broadcast_599[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.weight with ONNXTRT_Broadcast_603[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.bias with ONNXTRT_Broadcast_605[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.0.bias with ONNXTRT_Broadcast_613[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.1.bias with ONNXTRT_Broadcast_621[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.2.bias with ONNXTRT_Broadcast_629[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1426) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1426) [Constant] with ONNXTRT_Broadcast_634[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1433) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1433) [Constant] with ONNXTRT_Broadcast_641[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3808[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3808 with ONNXTRT_Broadcast_655[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3803[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3803 with ONNXTRT_Broadcast_657[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3809[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3809 with ONNXTRT_Broadcast_659[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3805[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3805 with ONNXTRT_Broadcast_661[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3810[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3810 with ONNXTRT_Broadcast_663[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3807[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3807 with ONNXTRT_Broadcast_665[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.self_attn.out_proj.bias with ONNXTRT_Broadcast_674[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.weight with ONNXTRT_Broadcast_679[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.bias with ONNXTRT_Broadcast_681[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.value_proj.bias with ONNXTRT_Broadcast_687[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_696[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_704[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.output_proj.bias with ONNXTRT_Broadcast_900[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.weight with ONNXTRT_Broadcast_904[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.bias with ONNXTRT_Broadcast_906[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear1.bias with ONNXTRT_Broadcast_914[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear2.bias with ONNXTRT_Broadcast_922[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.weight with ONNXTRT_Broadcast_926[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.bias with ONNXTRT_Broadcast_928[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.0.bias with ONNXTRT_Broadcast_936[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.1.bias with ONNXTRT_Broadcast_944[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.2.bias with ONNXTRT_Broadcast_952[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1834) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1834) [Constant] with ONNXTRT_Broadcast_957[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1841) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1841) [Constant] with ONNXTRT_Broadcast_964[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3880[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3880 with ONNXTRT_Broadcast_978[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3875[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3875 with ONNXTRT_Broadcast_980[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3881[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3881 with ONNXTRT_Broadcast_982[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3877[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3877 with ONNXTRT_Broadcast_984[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3882[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3882 with ONNXTRT_Broadcast_986[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3879[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3879 with ONNXTRT_Broadcast_988[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.self_attn.out_proj.bias with ONNXTRT_Broadcast_997[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.weight with ONNXTRT_Broadcast_1002[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.bias with ONNXTRT_Broadcast_1004[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.value_proj.bias with ONNXTRT_Broadcast_1010[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_1019[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_1027[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.output_proj.bias with ONNXTRT_Broadcast_1223[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.weight with ONNXTRT_Broadcast_1227[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.bias with ONNXTRT_Broadcast_1229[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear1.bias with ONNXTRT_Broadcast_1237[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear2.bias with ONNXTRT_Broadcast_1245[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.weight with ONNXTRT_Broadcast_1249[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.bias with ONNXTRT_Broadcast_1251[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.0.bias with ONNXTRT_Broadcast_1259[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.1.bias with ONNXTRT_Broadcast_1267[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.2.bias with ONNXTRT_Broadcast_1275[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 2242) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2242) [Constant] with ONNXTRT_Broadcast_1280[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 2249) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2249) [Constant] with ONNXTRT_Broadcast_1287[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_score_head.2.bias with ONNXTRT_Broadcast_1293[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear1/Transpose with ONNXTRT_Broadcast_129[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear2/Transpose with ONNXTRT_Broadcast_143[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_output/proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_output/proj/Transpose with ONNXTRT_Broadcast_273[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_score_head/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_score_head/Transpose with ONNXTRT_Broadcast_287[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.0/Transpose with ONNXTRT_Broadcast_293[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.1/Transpose with ONNXTRT_Broadcast_301[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.2/Transpose with ONNXTRT_Broadcast_309[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_364[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_373[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_381[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_575[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear1/Transpose with ONNXTRT_Broadcast_589[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear2/Transpose with ONNXTRT_Broadcast_597[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose with ONNXTRT_Broadcast_611[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose with ONNXTRT_Broadcast_619[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose with ONNXTRT_Broadcast_627[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_685[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_694[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_702[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_898[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear1/Transpose with ONNXTRT_Broadcast_912[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear2/Transpose with ONNXTRT_Broadcast_920[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose with ONNXTRT_Broadcast_934[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose with ONNXTRT_Broadcast_942[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose with ONNXTRT_Broadcast_950[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_1008[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_1017[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_1025[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_1221[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear1/Transpose with ONNXTRT_Broadcast_1235[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear2/Transpose with ONNXTRT_Broadcast_1243[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose with ONNXTRT_Broadcast_1257[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose with ONNXTRT_Broadcast_1265[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose with ONNXTRT_Broadcast_1273[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_score_head.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_score_head.2/Transpose with ONNXTRT_Broadcast_1291[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape with /model/encoder/encoder.0/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_113[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_113[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 with /model/encoder/encoder.0/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/Transpose_1 with /model/encoder/Reshape_1[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape with /model/decoder/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_1 with /model/decoder/Transpose_1[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_2 with /model/decoder/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape with /model/decoder/decoder/layers.0/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape with /model/decoder/decoder/layers.1/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape with /model/decoder/decoder/layers.2/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_2 with /model/decoder/decoder/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape with /model/decoder/decoder/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_1 with /model/decoder/decoder/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_348[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_348[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Transpose_5 with /model/decoder/decoder/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_4 with /model/decoder/decoder/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_386[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_386 with /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_386 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_386 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2 with /model/decoder/decoder/layers.0/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Transpose_1 with /model/decoder/decoder/layers.0/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape_10 with /model/decoder/decoder/layers.0/cross_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_2 with /model/decoder/decoder/layers.1/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape with /model/decoder/decoder/layers.1/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_1 with /model/decoder/decoder/layers.1/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_671[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_671[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Transpose_5 with /model/decoder/decoder/layers.1/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_4 with /model/decoder/decoder/layers.1/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_707[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_707 with /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_707 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_707 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2 with /model/decoder/decoder/layers.1/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Transpose_1 with /model/decoder/decoder/layers.1/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape_10 with /model/decoder/decoder/layers.1/cross_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_2 with /model/decoder/decoder/layers.2/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape with /model/decoder/decoder/layers.2/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_1 with /model/decoder/decoder/layers.2/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_994[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_994[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Transpose_5 with /model/decoder/decoder/layers.2/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_4 with /model/decoder/decoder/layers.2/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1030[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1030 with /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1030 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1030 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2 with /model/decoder/decoder/layers.2/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Transpose_1 with /model/decoder/decoder/layers.2/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape_10 with /model/decoder/decoder/layers.2/cross_attn/Transpose_3[0m
[38;5;104m[X] QDQ graph optimizer - constant folding of Q/DQ initializers[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_5[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_9[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_13[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_17[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_19[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_23[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_27[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_31[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_35[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_39[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_43[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_47[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_51[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_55[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_59[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_63[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_67[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_71[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_75[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_79[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_83[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_87[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_89[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_91[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_95[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_126[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_140[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_155[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_159[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_163[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_167[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_171[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_173[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_177[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_181[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_185[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_189[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_193[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_197[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_199[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_203[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_207[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_211[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_215[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_219[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_223[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_225[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_229[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_233[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_237[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_241[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_245[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_249[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_251[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_255[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_257[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_259[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_263[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_270[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_284[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_290[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_298[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_306[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_317[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_325[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_361[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_370[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_378[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_572[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_586[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_594[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_608[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_616[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_624[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_682[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_691[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_699[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_895[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_909[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_917[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_931[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_939[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_947[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1005[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1014[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1022[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1218[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1232[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1240[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1254[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1262[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1270[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1288[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_2[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_10[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_18[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_24[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_32[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_40[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_48[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_56[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_64[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_72[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_80[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_88[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_92[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_127[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_156[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_164[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_172[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_178[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_186[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_194[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_200[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_208[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_216[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_224[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_230[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_238[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_246[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_252[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_258[0m
[38;5;104m[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_264[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_285[0m
[38;5;104m[X] Removing /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_299[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_318[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_362[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_379[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_587[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_609[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_625[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_692[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_896[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_918[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_940[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1006[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1023[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1233[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1255[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1271[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_3[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_4[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_7[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_8[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_11[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_12[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_15[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_16[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_21[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_22[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_25[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_26[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_29[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_30[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_37[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_38[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_34[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_41[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_42[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_45[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_46[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_49[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_50[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_57[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_58[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_54[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_61[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_62[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_65[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_66[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_69[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_70[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_77[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_78[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_74[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_81[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_82[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_85[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_86[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_93[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_94[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_124[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_125[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_138[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_139[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_153[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_154[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_157[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_158[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_161[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_162[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_165[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_166[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_169[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_170[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_175[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_176[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_179[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_180[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_183[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_184[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_187[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_188[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_191[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_192[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_195[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_196[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_201[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_202[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_205[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_206[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_209[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_210[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_213[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_214[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_217[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_218[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_221[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_222[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_227[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_228[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_231[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_232[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_235[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_236[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_239[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_240[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_243[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_244[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_247[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_248[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_253[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_254[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_261[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_262[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_359[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_268[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_360[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_269[0m
[38;5;104m[X] Removing /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_282[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_283[0m
[38;5;104m[X] Removing /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_296[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_297[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_304[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_305[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_315[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_316[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_323[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_324[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_368[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_369[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_570[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_571[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_584[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_585[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_592[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_593[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_606[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_607[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_614[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_615[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_622[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_623[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_642[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_643[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_648[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_649[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_689[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_690[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_893[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_894[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_907[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_908[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_915[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_916[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_929[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_930[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_937[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_938[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_945[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_946[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_965[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_966[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_971[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_972[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1012[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1013[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1216[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1217[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1230[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1231[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1238[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1239[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1252[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1253[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1260[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1261[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1268[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1269[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_6[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_14[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_20[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_28[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_36[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_44[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_52[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_60[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_68[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_76[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_84[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_90[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_96[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_141[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_160[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_168[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_174[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_182[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_190[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_198[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_204[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_212[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_220[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_226[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_234[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_242[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_250[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_256[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_260[0m
[38;5;104m[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_271[0m
[38;5;104m[X] Removing /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_291[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_307[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_326[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_371[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_573[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_595[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_617[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_683[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_700[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_910[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_932[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_948[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1015[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1219[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1241[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1263[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1289[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_33[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_53[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_73[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found and reassigned Myelin backends for Self-Attention nodes[0m
[38;5;104m[X] After Myelin optimization: 464 layers[0m
[38;5;104m[X] QDQ graph optimizer - constant folding of Q/DQ initializers[0m
[38;5;104m[X] QDQ graph optimizer forward pass - DQ motions and fusions[0m
[38;5;104m[X] QDQ graph optimizer backward pass[0m
[38;5;104m[X] QDQ graph optimizer quantization pass - Generate quantized ops[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.0/Add with /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.1/Add with /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.0/Add with /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.1/Add with /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.0/Add with /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.1/Add with /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.0/Add with /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.1/Add with /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_1/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_1/norm/BatchNormalization with /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_2/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_2/norm/BatchNormalization with /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_3/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_3/norm/BatchNormalization with /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_1.conv.weight with /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_2.conv.weight with /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_3.conv.weight with /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight with /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.2.conv.weight with /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.0.conv.weight with /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.1.conv.weight with /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.0.conv.weight with /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight with /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight with /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight with /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.1.conv.weight with /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight with /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight with /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight with /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.0.conv.weight with /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight with /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight with /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight with /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.1.conv.weight with /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight with /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight with /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight with /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.0.conv.weight with /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.1.conv.weight with /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.2.conv.weight with /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/backbone/MaxPool[0m
[38;5;104m[X] Swapping /model/backbone/MaxPool with /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_2[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_3[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_4[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_5[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize[0m
[38;5;104m[X] Swapping /model/encoder/Resize with /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize_1[0m
[38;5;104m[X] Swapping /model/encoder/Resize_1 with /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Running: HorizontalMergeQNodes on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Eliminating /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 which duplicates (Q) /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/lateral_convs.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.0/act/Sigmoid with /model/encoder/lateral_convs.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv1/act/Sigmoid with /model/encoder/fpn_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv2/act/Sigmoid with /model/encoder/fpn_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul) with /model/encoder/fpn_blocks.0/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv3/act/Sigmoid with /model/encoder/fpn_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/lateral_convs.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.1/act/Sigmoid with /model/encoder/lateral_convs.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv1/act/Sigmoid with /model/encoder/fpn_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv2/act/Sigmoid with /model/encoder/fpn_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul) with /model/encoder/fpn_blocks.1/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv3/act/Sigmoid with /model/encoder/fpn_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/downsample_convs.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.0/act/Sigmoid with /model/encoder/downsample_convs.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv1/act/Sigmoid with /model/encoder/pan_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv2/act/Sigmoid with /model/encoder/pan_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul) with /model/encoder/pan_blocks.0/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv3/act/Sigmoid with /model/encoder/pan_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/downsample_convs.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.1/act/Sigmoid with /model/encoder/downsample_convs.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv1/act/Sigmoid with /model/encoder/pan_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv2/act/Sigmoid with /model/encoder/pan_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul) with /model/encoder/pan_blocks.1/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv3/act/Sigmoid with /model/encoder/pan_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/norm/BatchNormalization + /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/norm/BatchNormalization + /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/norm/BatchNormalization + /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/lateral_convs.1/conv/Conv with PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.0/conv/Conv with PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.1/conv/Conv with PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1 and /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2 and /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2 and /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv with /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] QDQ graph optimizer quantization epilogue pass[0m
[38;5;104m[X] QDQ optimization pass[0m
[38;5;104m[X] QDQ graph optimizer constant fold dangling QDQ pass[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Swap the layer type of /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Swap the layer type of /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 from QUANTIZE to kQDQ[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] After vertical fusions: 86 layers[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] After slice removal: 86 layers[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_5[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_5_/model/encoder/lateral_convs.0/act/Mul_output_0_clone_1 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_4[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Generating copy for /model/encoder/Resize_1_output_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_3[0m
[38;5;104m[X] Generating copy for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_2[0m
[38;5;104m[X] Generating copy for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] After concat removal: 85 layers[0m
[38;5;104m[X] Trying to split Reshape and strided tensor[0m
[38;5;104m[X] Graph optimization time: 0.08142 seconds.[0m
[38;5;104m[X] Building graph using backend strategy 2[0m
[38;5;13m[V] Local timing cache in use. Profiling results in this builder pass will not be stored.[0m
[38;5;104m[X] Constructing optimization profile number 0 [1/1].[0m
[38;5;104m[X] Applying generic optimizations to the graph for inference.[0m
[38;5;104m[X] Reserving memory for host IO tensors. Host: 0 bytes[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(3276800,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_chw_int8int8_tilesize16x16_k32_fltsteps1_threadspercta256_r3s3_u2v2_scalebias_relu Tactic: 0x11764d94950382f8 Time: 0.00587734[0m
[38;5;104m[X] Tactic Name: ampere_first_layer_filter3x3_imma_fwd Tactic: 0x9ae0c0d2fb3a01e5 Time: 0.00654787[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00623568 seconds. Fastest Tactic: 0x11764d94950382f8 Time: 0.00587734[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x11764d94950382f8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,409600:4,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize16x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x3d988d07a78b0918 Time: 0.00508703[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x5cc792a989a1d1a6 Time: 0.00504374[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00975482[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00965851[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0117351 seconds. Fastest Tactic: 0x5cc792a989a1d1a6 Time: 0.00504374[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5cc792a989a1d1a6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1:16,640,1) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0391277[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0179615[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.014745[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0150975[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0302116[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0255962[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0376569[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0141649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.044736[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0268841[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0291058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0159975[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0301227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0158138[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0308528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0308393[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0340171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0405843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0113525[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0152616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0142516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0112274[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.043644[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0142053[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0191283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0120682[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0172405[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0234937[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.037075[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0307171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0303089[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0310216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0451933[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.02629[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0144844[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0140191[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0186501[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0112676[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0434067[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0141227[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.014256[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0449413[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.043444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0172635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0111605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0162159[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0113148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0149004[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0315772[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0155971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0182007[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0197384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.015046[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0122011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.045816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.01446[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0157479[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0144511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0151309[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0263015[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0316761[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0440693[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0143782[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0319253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0415799[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0158938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0158095[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0118944[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0357621[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0328902[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0347019[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0133577[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0359819[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0313581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0446133[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0123824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0322366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0149023[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0116785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0185618[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.041325[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0203576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0136546[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0137562[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0407644[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.047108[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0172032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0311457[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0170043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0191704[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0188996[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0264254[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0525943[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0275791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0121642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0195698[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0308073[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0181418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0555246[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0123887[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0282053[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0431373[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0110675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0164785[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0152039[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0274453[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0173829[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0184556[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0408047[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0150762[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.029024[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.0284684[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0292311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0138893[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0133054[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0110923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0144129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.0394086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0181541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0132583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0127534[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0281404[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0112551[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0221367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0166258[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0151627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0147845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0138956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0259848[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0167035[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.00997615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0495695[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0135411[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0284302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.040307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.02828[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0164343[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0369248[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0282338[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0487634[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0142467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0119733[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0106983[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0166512[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0178751[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0129128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0139338[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0111886[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.446451 seconds. Fastest Tactic: 0x2958c78a91e58cda Time: 0.00997615[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2958c78a91e58cda[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,409600:32,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0260611[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0197973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0344128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0356885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0141218[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.017016[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0279787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0176056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0262539[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0203169[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0295182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0194939[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0163997[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0245242[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0273485[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.023821[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0275832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0138906[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0233152[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0163906[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0198387[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0156761[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0150664[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0212753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0275126[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.04336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0295742[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.018525[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0196298[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0143484[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0232818[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.027776[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0119097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.045212[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0129506[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0271811[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0154754[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0485272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.019395[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0274109[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0146523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0287289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0260718[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0277243[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0301102[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0121821[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0198594[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0356747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0249295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.021136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.026839[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0132078[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.019882[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0196135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0383111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0438387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0161854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0197791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0267602[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0195662[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0158274[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.049504[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0171061[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.012476[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0128558[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0188124[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0143702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0137374[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0443227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0261645[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0314686[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0189049[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0265255[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0180508[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0272386[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0172944[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0449467[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0218787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0150706[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0270318[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0154531[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0169899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0270663[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0346539[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0161488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0264418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0203639[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.243936 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.0119097[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(3276800,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0206237[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0202573[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00577222 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0202573[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0348811[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0217827[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0167024[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0167365[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0403046[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0323627[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0511238[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0153076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0380919[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0224612[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.037581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0205321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0411556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0196819[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0223652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.041988[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0298791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0359125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0132792[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0162448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0138125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0125724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0284738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0183006[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.025993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0127111[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0227399[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0298107[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0507474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0237035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0404693[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0427653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0391822[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0219007[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0148712[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0134182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.021004[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0114962[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0280036[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0135454[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.014452[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0305968[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0283618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0199561[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0126013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0189197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0133901[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0169248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0433213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0168683[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0260324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0259791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0160411[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0137391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0640942[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.017815[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0173648[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0175663[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0164185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0341205[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0471307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0602453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0140142[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.041312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0375087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0163139[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0194684[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0141311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0508206[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.027904[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0303079[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0143431[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0509562[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0226624[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0293964[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0140329[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0269128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0166359[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0116094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0257305[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0372302[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0263992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0152005[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0152184[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.037056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0647058[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0231225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0453307[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0189908[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0260628[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0224512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0319147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.073504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0238301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0133054[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0264238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.044892[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0206808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0759104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0132981[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0347445[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0280916[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0152436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0190916[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0180379[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0369141[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.021632[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0268792[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0306337[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.018062[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0466747[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.046324[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0481478[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0185428[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0157188[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0137143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0209487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.0599609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0265272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0161681[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0153421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0412148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0141916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0318613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0207184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0200433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.01851[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0166965[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0251611[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.025664[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.0117959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0747712[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0164688[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.043668[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0291138[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0247505[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0260554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0369696[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0433213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0740629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0159589[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.014428[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0144293[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0221193[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0265313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0164744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0167808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0133807[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.423854 seconds. Fastest Tactic: 0xa60c3259c62a72b2 Time: 0.0114962[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa60c3259c62a72b2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xde3a6a3727f31f34 Time: 0.946859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0255337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0190181[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbbff0ceb48c87bac Time: 0.9216[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x9c9fd7d74c020c9d Time: 0.489472[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0340096[0m
[38;5;104m[X] Fast skip Tactic:0xb97409e537081e4c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb97409e537081e4c Time: 3.58605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0348192[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe5f40c565f9c8a09 Time: 0.0481768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0128714[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x4e679e1c8dcfbe3c Time: 0.946155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0162865[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0275036[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x645c57c8d2bdcafa Time: 0.0526629[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x13cb22041bcdf2f5 Time: 0.859819[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0163586[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0256274[0m
[38;5;104m[X] Fast skip Tactic:0xa1513318dd2f5314 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa1513318dd2f5314 Time: 2.37056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.019968[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0289742[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.017528[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0156078[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0238392[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2d0a836ca3b48b55 Time: 0.61136[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x307c1c762709b00e Time: 0.834688[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0267192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0231111[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9826a9122a4e1bac Time: 0.408576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0267134[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0128476[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.022068[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.015939[0m
[38;5;104m[X] Fast skip Tactic:0x9c391ea4731a473c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9c391ea4731a473c Time: 1.86573[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe6fb49f176c8ac20 Time: 0.241632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0196367[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x136deb7724d5b954 Time: 0.997717[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x718a86dfcb201f10 Time: 0.20992[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0148508[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x415d0d459f475d7a Time: 0.0706091[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0143982[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x7f9cac2d273e24da Time: 0.039949[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x730183d1b4e07af0 Time: 0.0603413[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.021072[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xab1e201ca705d0dd Time: 0.613376[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x2c80f3b4623a1878 Time: 0.213845[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4dc022b90c990350 Time: 0.037459[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0269842[0m
[38;5;104m[X] Fast skip Tactic:0x8286a69028b3e3f0 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x8286a69028b3e3f0 Time: 4.04448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.043064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0292053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0187081[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0138859[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4480741a5fe7a6b0 Time: 0.0309673[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x8c7efb20a3cfa7ec Time: 0.591531[0m
[38;5;104m[X] Fast skip Tactic:0xb7622317db162586 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb7622317db162586 Time: 1.06179[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0226894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0271015[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00994384[0m
[38;5;104m[X] Fast skip Tactic:0x89a3827f636f5c26 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x89a3827f636f5c26 Time: 1.04515[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xdb35ad3ee7e5f3a9 Time: 0.0636462[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xfe34f7b3aa1f6429 Time: 0.0414566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0448387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.011333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0268488[0m
[38;5;104m[X] Fast skip Tactic:0x3836d6c48cd9dbee which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3836d6c48cd9dbee Time: 1.2073[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x0f57663c97a6a89c Time: 0.477867[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5801f8b0fa1b5d5c Time: 0.807936[0m
[38;5;104m[X] Fast skip Tactic:0x48fd1cd53c4157a8 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x48fd1cd53c4157a8 Time: 4.67555[0m
[38;5;104m[X] Fast skip Tactic:0x31768067dfa77e0e which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x31768067dfa77e0e Time: 1.11549[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x0a6a5850a77efc64 Time: 0.949824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0138112[0m
[38;5;104m[X] Fast skip Tactic:0xe47e7c8e9e121924 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xe47e7c8e9e121924 Time: 3.70995[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x09cde4f526284108 Time: 0.111029[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x244ad5cff0ca2eb5 Time: 0.436565[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x853ead83f0b1020c Time: 0.752843[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0481036[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.01888[0m
[38;5;104m[X] Fast skip Tactic:0x3620fc3660c7e024 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3620fc3660c7e024 Time: 1.9159[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xaca2d8f22e95cba6 Time: 0.594603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0267742[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x23f62d21795a35ce Time: 0.896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0134084[0m
[38;5;104m[X] Fast skip Tactic:0x6b2a895dc9dde74c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x6b2a895dc9dde74c Time: 2.05312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0269054[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5600d95cf91aed70 Time: 0.0370726[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x032a0ef3f4005984 Time: 0.760832[0m
[38;5;104m[X] Fast skip Tactic:0xdf8cd3fb81a9e498 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xdf8cd3fb81a9e498 Time: 3.94957[0m
[38;5;104m[X] Fast skip Tactic:0x2e05c6cb8ae0ad7c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2e05c6cb8ae0ad7c Time: 4.19226[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0253989[0m
[38;5;104m[X] Fast skip Tactic:0xd916513230270d0c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xd916513230270d0c Time: 1.16019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0273772[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0297102[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0113717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0196373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0352224[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.023952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0207768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0267733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0117521[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xecb45af50ce22fe9 Time: 0.0351925[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0196317[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0193541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0373902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0433267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0156771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0177437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0261079[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0192877[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0150034[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.049152[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0165349[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.010441[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0118069[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0171952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0128067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.011995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0439947[0m
[38;5;104m[X] Fast skip Tactic:0x5642a4e167e8f364 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x5642a4e167e8f364 Time: 2.10454[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0253196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0309527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0182877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0258921[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0167915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0264295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0162164[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9efe56660532ec2c Time: 0.508043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.044612[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.021488[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9ebc2bdb9bc0f238 Time: 0.123762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0138956[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa25e76bff47b753d Time: 0.768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0263303[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x11aaa3b552fd1244 Time: 0.613035[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xb99e8d5a01f89b1d Time: 0.50928[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbe2275b488688066 Time: 0.867328[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x34abf9381f0785c4 Time: 0.515979[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0144733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0162738[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xc52cdc7983541cc4 Time: 0.421547[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x06f777ac34a0a24e Time: 0.654336[0m
[38;5;104m[X] Fast skip Tactic:0xc1336bcfda004054 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xc1336bcfda004054 Time: 1.73466[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0262498[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x36ca788956376575 Time: 0.448853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0339733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.014376[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x76dcfa8e7440813a Time: 0.0381523[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x54c7919e8f324660 Time: 0.111029[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0260431[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0196662[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.679689 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.00994384[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(6553600,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0324781[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.032001[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00571327 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.032001[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(409600,1:16,1280,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0356843[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0383905[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0292204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0357557[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0680939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0364693[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0942587[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0279236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.040403[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0248846[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0727125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0352128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0720213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0349408[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0281147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.074208[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0307704[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0367264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0234332[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0337163[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0273255[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0204066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0395058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0317585[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0479223[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0234724[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0384664[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0360736[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0930453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.02856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0682453[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0733483[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0412622[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0247421[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0306938[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0281982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0403259[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0216033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0394477[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0313251[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0280311[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0403461[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0393944[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0264837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0197572[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0222891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0245417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0195733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0744661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.028432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0473973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0470707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0331413[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0248251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0704981[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0312359[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0306987[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0334197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0221007[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0388006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.085936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.108699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0308141[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0707605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0378098[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0343584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0332713[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0241272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0943147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0305125[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0312456[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0266388[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0948[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0283173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0401825[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0257428[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0298907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0190667[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0226731[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0464253[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0374329[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0477928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0272886[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0274642[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0373096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0773888[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0376687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0837493[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0379378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0486065[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0413772[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0477531[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0791061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0256015[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.023997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0482636[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0827968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0370382[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0811691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0246659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0482057[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0393659[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0218673[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0237191[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0340181[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0688107[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0366059[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0491794[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0403804[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0334091[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.085672[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.084408[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0879093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0356043[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0338581[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0238101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0364053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.107851[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0486659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0287484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0280844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0705387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0258576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0372895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0348811[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0235179[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0274076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0264944[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0266347[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.046972[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.0209087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0879787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0230101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0738539[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.04[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0288907[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.047843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0372812[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0750144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.081792[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0323132[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0269005[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.025184[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.038963[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0484206[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.029552[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0311321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0254019[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.438722 seconds. Fastest Tactic: 0xbe784bf72795274c Time: 0.0190667[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbe784bf72795274c[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0267733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0196348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0343285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0349568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0146843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.018528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0291547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0166827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0142249[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0257493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0202692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0113707[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0292542[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0305173[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.016257[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0242728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0272074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0286533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.026953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0131627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0237255[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0166752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0200584[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0153493[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0149315[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.021304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0280907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0431333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0132665[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.030008[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.0165709[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0183023[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.01888[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0142849[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0231033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0299911[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0154565[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0448893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0172427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0270539[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0158565[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0483337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0197314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0296533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0140373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0270211[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0166683[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0257313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0263499[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0296569[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0185938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0203721[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0355147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0253615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0220387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0269005[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0185241[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0199611[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0194809[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0376462[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0436373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0161326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0306124[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0263475[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0194175[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0153251[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0492815[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0171909[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0162651[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0192699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0175601[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0205195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.019235[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0440253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0258864[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0317139[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.019978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0294773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0290231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0266027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0282133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.044684[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0224277[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0224811[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0139058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0266421[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0150692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0168027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0265625[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0342389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0244549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0112857[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0263549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0356203[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0135181[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.288945 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.0112857[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/MaxPool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,102400:4,320,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0xfa45342d0e1d409a Time: 0.0116259[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0xa88280db27a5d09f Time: 0.0101337[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0x3acbbd865df539ed Time: 0.0104577[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0x3d35b618fd3968f1 Time: 0.0102587[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0xbeae815d02985cde Time: 0.0127257[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0xe09c44661dba1b5c Time: 0.00903957[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0xb331e89337ca2bad Time: 0.0102174[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0xc4335d27b08156bf Time: 0.012465[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0xbadabf84bb0f736c Time: 0.00837493[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x80d8e857bc044afb Time: 0.0124919[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0x199aaf5950022512 Time: 0.0129198[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x67734dfa5b8c00c1 Time: 0.00791988[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0xf307ae442c39b4a3 Time: 0.0138924[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0x2eae5c3accbac70e Time: 0.0128726[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x9fe4504b077a26fb Time: 0.011408[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0x63077323e21b2f73 Time: 0.0118227[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x9dff93820f6f4021 Time: 0.0104068[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x923de46f0f4ddb62 Time: 0.0113017[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0xc9170fb073b2e283 Time: 0.00953783[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0xd3ce7ffb6015b945 Time: 0.0119425[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x072517eca2b23ed1 Time: 0.0120232[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0x1340f65033fdc032 Time: 0.00901333[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0x47a86a624f206290 Time: 0.00989176[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x9a01981cafa3113d Time: 0.00987357[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x69dd2a2a81e4ca53 Time: 0.00895747[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x4a8c38f58c13d6ac Time: 0.012045[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0x5d711a295c873956 Time: 0.0122827[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0x1dee9180e9950aa0 Time: 0.00844667[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x899a723e9e20bec2 Time: 0.0135497[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0xedb816f1de89af60 Time: 0.0161397[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0xa01139e8f028471d Time: 0.0173211[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x898e8c271f222050 Time: 0.0145813[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0xc04763fe0916790d Time: 0.00938192[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0x6e2321b421289b4f Time: 0.0123303[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0x27ecc653ee9e3337 Time: 0.0126187[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x9351f452d5078ab3 Time: 0.00931141[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0xbee85cb73ffdd634 Time: 0.0113106[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x0165782a59f89027 Time: 0.00683015[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0xcee9042ed37eb39f Time: 0.0101379[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0xb474d8546167b9fe Time: 0.00926429[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0x74fa51ff328fc089 Time: 0.015633[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0x543380407ea3cd6f Time: 0.0141538[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0x3465da56879df37f Time: 0.00910213[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kMAX Tactic: 0x1f6c40e3e09ec730 Time: 0.00624198[0m
[38;5;104m[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.108427 seconds. Fastest Tactic: 0x1f6c40e3e09ec730 Time: 0.00624198[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x1f6c40e3e09ec730[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,102400:32,320,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX Tactic: 0x94215b398b8eb3ba Time: 0.00657611[0m
[38;5;104m[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.00286657 seconds. Fastest Tactic: 0x94215b398b8eb3ba Time: 0.00657611[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x94215b398b8eb3ba[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0209447[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.020453[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00556737 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.020453[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0129239[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0144253[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0100555[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00945955[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.011545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0131713[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0123706[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0104957[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0147223[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.010064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0121307[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0131737[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0124602[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0116752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0119741[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0116811[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00953234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0131766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.011701[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0119387[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0115899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00889656[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0120621[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0129592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0090831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0130745[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.010879[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0116241[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0763754 seconds. Fastest Tactic: 0x5f5aa01645d48746 Time: 0.00889656[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5f5aa01645d48746[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0144267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00959512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0155093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.018281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00922205[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0107003[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0147404[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00780601[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00841387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0160203[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00962712[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00732382[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0124725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0150006[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00882976[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0128279[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0130035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0146124[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0118349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00955642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0128546[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0106547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0124638[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00856588[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00877754[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0124022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.015002[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0201405[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.006992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0138176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00914278[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00945452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.008456[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00882274[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0124883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0151709[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00930874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0233806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00953813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0127581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00941392[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0220347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0105303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0150001[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00996863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0118459[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00907704[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0160523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0115432[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0127186[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0111612[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0121036[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0166461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0131979[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0126621[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0153295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0098491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0124361[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0123387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0194151[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0228359[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00949778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0154177[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0162875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.012325[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00916584[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0222891[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0109505[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00962042[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0116292[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00808254[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00936652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00894877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0203288[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0126392[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0143138[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0112089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0150163[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0132554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0163815[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.013175[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0206664[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0125867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.01044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00694487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0117205[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00869443[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00991749[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0115619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0162276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0119444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00683058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0149194[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0157304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00693551[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.257653 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.00683058[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.021884[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.020964[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00548465 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.020964[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0104753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0168133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0119547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0150803[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0125211[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0117867 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.0104753[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0174571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0134635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0239787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0160549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0152005[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0138987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0166981[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0135309[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0144022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0150442[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0157033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0154749[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0137195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0170875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.013667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0134729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0122907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0155544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0135821[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0156562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0137289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0172976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0242895[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0534345 seconds. Fastest Tactic: 0xb936321f82fd390c Time: 0.0122907[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb936321f82fd390c[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00834613[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00829399[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0084664[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00858639[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0115848 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00829399[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.0114677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0104065[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0125764[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00714038 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.0104065[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00958781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00794768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00816911[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00914623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00821099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00901391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0119859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00873327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0100232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00956526[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0097347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00928415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00804724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.0083824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00895411[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.010628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.010964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00813333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00856561[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00823285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00881656[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0104523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00986416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00910703[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0081535[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0082828[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00792757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00854373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0108965[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00982118[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00910702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00884604[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0109488[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0838378 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00792757[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x0f47434ace2a7d18, 0.020453 ms[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4)] got cached result: CaskConvolution, tactic 0x5f5aa01645d48746, 0.00889656 ms[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x9dafb2758560cc1d, 0.00683058 ms[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0226738[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0217793[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00551208 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0217793[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0123093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.021636[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.014144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0182557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0145939[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0123679 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.0123093[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.015066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0144862[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0223296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.017609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0158555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0114862[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0150544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0147135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0119726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0154099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0168741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0157299[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0111879[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0154512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0118915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0136627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.013437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0163596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0138551[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0132201[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0121962[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0148647[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0228004[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0598302 seconds. Fastest Tactic: 0x0e07dc8353bf7e9f Time: 0.0111879[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0e07dc8353bf7e9f[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(102400,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00326183[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00342351 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00326183[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(12800,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00302415[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00351719 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00302415[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0161244[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0153944[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00571774 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0153944[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.00735536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00663508[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00869388[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00617442[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.00739084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00757831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00732777[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00615894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00660852[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.00930637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00723699[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00680256[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00739293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0112256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00689589[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00748729[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00634747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00772509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00683407[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0115575[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0112985[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00771515[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00722315[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00669098[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00810387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00756385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00639115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0112398[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0765552 seconds. Fastest Tactic: 0xb60ee5b2b8916a65 Time: 0.00615894[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb60ee5b2b8916a65[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00768751[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00616495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00620188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.00806705[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00657736[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00659221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00901391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00769188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.00946874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00611705[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.012155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00957196[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00712034[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00958019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00762012[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00912317[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0116723[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00625225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0075309[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.00808813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00653456[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00682732[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00695666[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00738296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00778692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0112686[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00620603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00588052[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00835067[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00674218[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.00920475[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00976396[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00778295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00990557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00830829[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00757964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00679723[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0121653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00694226[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00969813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00675477[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0119295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.00926963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0115792[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.012463[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00810667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00777898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00946755[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00773964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00794641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00705733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00857162[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00941007[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00644657[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00838747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.00936711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00643241[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00989302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00936415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.00929185[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00887046[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0123536[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00829763[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00792583[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0084408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00780304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00688435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00667755[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0113045[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.00736557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00627714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00696555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00922349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00875515[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00954088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00833733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0114254[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00741689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00699622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0115189[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00844987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00672341[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0114084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.00926696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00821281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00682492[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0104753[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.252186 seconds. Fastest Tactic: 0x705baf38e41eee0b Time: 0.00588052[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x705baf38e41eee0b[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0281511[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0218893[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00537634 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0218893[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00901189[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.013574[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00912115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0116417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00934015[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0121538 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.00901189[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.021086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0117186[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0204587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0103224[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0128361[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0137583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.013289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0111078[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0150038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.012465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0104817[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0128265[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0135407[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0135518[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00972739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00955977[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0102985[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0131885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00976366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0117929[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0099658[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.020886[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0206701[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0556768 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00955977[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00720113[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.007078[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00734377[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00719342[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0119083 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.007078[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00694117[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00669077[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00807898[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00777635 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00669077[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00946459[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00527583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00615273[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00615234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0063201[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0088539[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00623724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0068295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0065943[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00598[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00606623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00631628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00541609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00551886[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00692615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00584331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00593666[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00553285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00590372[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00557849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00861456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00675925[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00645333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00612597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00539768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00567413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00548669[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0060221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00650626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00627911[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00599086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00598476[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00705844[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0889924 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00527583[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0295333[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0267011[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00551794 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0267011[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0105837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00869744[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0132632[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00819304[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0106087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0106823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0102022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0083528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00862551[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0144622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0101036[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00849013[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0100232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0182641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00909376[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0107534[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00837733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0104837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00896225[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0184006[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0181956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.010977[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00992784[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00850827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.01184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.010478[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00862933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.017687[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0699387 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00819304[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0102714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00741452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00796978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0116495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00891902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00802489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0119787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0106623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00698533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0141009[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00778815[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00639557[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0190655[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0129977[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0096643[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0138911[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0107895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0123055[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0179655[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00872807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0100969[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0112124[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00900463[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00930074[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00919524[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0103842[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.00871576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0104129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0177196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00950578[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00767903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00719047[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00727559[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0164038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0121829[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00905341[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.013248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.012912[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0101149[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.00976305[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00755437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0144511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00952716[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0109691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00919121[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.019187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00821854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.00966308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0126827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00904159[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0180132[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00800203[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0140751[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0176758[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0193481[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0110293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0107695[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0142782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0102814[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0111129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00954545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00960427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00890611[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0121901[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0141209[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00875733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0134114[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.014316[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0142729[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.012578[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0194353[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0114961[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0102956[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0113099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0108583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00764776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00729925[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0177594[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0104917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00780378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00837573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0125136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0105363[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0143578[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0104663[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0183315[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0104643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00883088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00951467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0180811[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0121227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00907157[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0179733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0141627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0101311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00955489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00926429[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0139564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00958903[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.282122 seconds. Fastest Tactic: 0x214f03e23f252333 Time: 0.00639557[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x214f03e23f252333[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0288249[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0225479[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00539396 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0225479[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00989019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0163947[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0097091[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0131266[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0101944[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0120403 seconds. Fastest Tactic: 0xa8b56a226b057463 Time: 0.0097091[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa8b56a226b057463[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0217773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0116469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0193766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00982055[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0130564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0129419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0124788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.01136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0142093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0126206[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0096707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0129309[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.01264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.012702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0088713[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00957044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0106643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0136307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00965668[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0104517[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00918544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0214407[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0197026[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0551507 seconds. Fastest Tactic: 0xad886d4d69834922 Time: 0.0088713[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xad886d4d69834922[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00281636[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00294802 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00281636[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00276461[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00310409 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00276461[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.027575[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0258568[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00551552 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0258568[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0105377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00710059[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0130658[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00692288[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0105017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0106637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00667115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00694511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00701689[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0141836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00662295[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00703867[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00656795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0179228[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00771879[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0106273[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00707111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00692702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00762448[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0183916[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0181642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0110111[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00647159[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00700889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0117035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00689437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00719614[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0177578[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0804227 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00647159[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00861265[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00740835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00750672[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0116094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0061345[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0058194[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00757144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.010784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0142564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00781247[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0191591[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00817509[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00980988[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0138244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0107606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00758115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0182776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00871275[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00670741[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0084616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00872315[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00933837[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0084688[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00991968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0088786[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0180643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00707955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00706111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0122952[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00842373[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0132386[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00773382[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0086173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0145456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00682558[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0110923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00628405[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0194519[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0060641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00753588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00906119[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0185393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0142684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0180789[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0194293[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00901053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00691853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0143396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00686062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00719455[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00892183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00685954[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.008832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00860827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0122526[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.014312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00857627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00829633[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0143382[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.00870154[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0128233[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0194252[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0086028[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00881825[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0091791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0110311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00795048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00765212[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0179705[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0105083[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.007104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00604857[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00767806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00722542[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.01432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00700089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0183719[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00998588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00922032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0182232[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0122827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00884491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0180261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0141022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.006988[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00874639[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0104103[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.261649 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.0058194[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0501151[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0380622[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00529637 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0380622[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0116252[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0201104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.009408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00932178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0120457[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0119753 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00932178[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0345088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0105387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0341653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0114976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0110366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0189185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0185157[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00988957[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0215287[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0108711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0114336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.010986[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0186939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.018829[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0113892[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00858092[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00873709[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0112871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00958171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0151021[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0116303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0341813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0342987[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0549181 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00858092[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00823154[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00773697[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00825522[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00792434[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0119127 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00773697[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00658154[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00548704[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00556267[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0082271 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00548704[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0104116[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00415763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00618548[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00473375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00633218[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00971367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00524983[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00708667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00953295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00441207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00455164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00484839[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00428253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00491153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00715325[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0048708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00494557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00426855[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00455467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00443733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00947141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00968259[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00649518[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00610366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00484035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00430427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00429224[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00475252[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00493945[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0060994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00551572[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00596248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00992941[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0903281 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00415763[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0539413[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0486903[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00576467 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0486903[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.017152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0104136[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0223964[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0102571[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.01708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0172704[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00960335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0102746[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0103803[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0244526[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00946666[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0102949[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00946341[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0309566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0110448[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0172683[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0103968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00993569[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0129017[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0319505[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0319661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0177269[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00945067[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0104993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0193493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00981176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0105603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0309498[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0914007 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00945067[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0135428[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0105097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0114069[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.018963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00844107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0073767[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0103806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0171205[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00943556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0241714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.011772[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00938755[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0332335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0109447[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0147293[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0221993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0173888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0104052[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0312155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0136772[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00939466[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0131512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0136798[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0141111[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0128308[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0156693[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0102319[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0137156[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0310982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0161473[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0102403[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00750056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0103783[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0294062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0200232[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0127909[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0216233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0108256[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0128808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0162123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00968107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0245006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00920994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0179043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00876499[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0335125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00849173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.0161549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.010677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0138722[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0312281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00828566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0240335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0309498[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0335957[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0131475[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.010261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0243756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00964388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0103861[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0138432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00919582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0137596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0135433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0200797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0241791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0135633[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0111033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.024227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0135859[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0202798[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0336096[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0134127[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0130441[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0134426[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0174517[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0109234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.010806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0310662[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0171509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0108256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00943318[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0129875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0109825[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0242537[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0092877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0317905[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0157716[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0138743[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0160757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0317983[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0196104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0137894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0310448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0240785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00805435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0161062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0149644[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0160559[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.307588 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.0073767[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0508419[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.038701[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00534568 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.038701[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0125444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0229767[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0101754[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0101295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0130613[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0117939 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0101295[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0350133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0100558[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0346133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0115619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0112117[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0190756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0185499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00930815[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.021584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.010815[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0113728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.011084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0187597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0188071[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0113575[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00829346[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0084064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0113141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.009384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0146866[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0114887[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0347424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0349195[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0549864 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00829346[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00270262[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00303544 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00270262[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00294129[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00310273 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00294129[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0519055[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0481371[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00543851 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0481371[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0171141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0104116[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0224953[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.010133[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0170592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0172421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00795048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0102086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0102859[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.024419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00791764[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0102309[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00801956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0310322[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0110242[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0173637[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0103421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00841467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0107775[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0316722[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0317905[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0176483[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00799365[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0103166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0192332[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00835627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0104607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0308936[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0761925 seconds. Fastest Tactic: 0x19e870769dcaba51 Time: 0.00791764[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x19e870769dcaba51[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0135066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0105183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0113468[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0190098[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00884183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00740104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00717458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0174288[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0242019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.011976[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0332567[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00809752[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0152136[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.022848[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0176196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00735861[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0315491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0139413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00931822[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0121543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0138125[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.014618[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0129116[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0159233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0135356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.031199[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0102639[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0104267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0202096[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0128295[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0221107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0086906[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0124626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0244731[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00924166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0182759[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00872041[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0335253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00844907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00929274[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0140724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0317081[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0241966[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0312242[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0335808[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0116517[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0100737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0243642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00945629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0103331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0138752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00914133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0136661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.013527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0203482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0242217[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0135599[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00830725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0242423[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0135194[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0211047[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0336139[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0122613[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.012785[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0118301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0176988[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0115439[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.010964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0311622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0172032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0103357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00860417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00739501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00862277[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.024445[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00887411[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.032033[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0157619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0143542[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0319253[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0203959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0138274[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0312087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0241608[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00751218[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00821281[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.249891 seconds. Fastest Tactic: 0x322f337abc345152 Time: 0.00717458[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x322f337abc345152[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0934453[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0705792[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00538884 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0705792[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0181345[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0334432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0142271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0128166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0188207[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.011391 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0128166[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0611787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0148373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0605813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0175077[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0321765[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0316994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0133436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0374139[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0104287[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0174885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0105433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.031968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0321852[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0175237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0114528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0114204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0114507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0138342[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0245128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0177112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0607004[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0610116[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0638497 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.0104287[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0104767[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00935289[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0107603[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0096896[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.011188 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00935289[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00704533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00547008[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00540662[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00817547 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00540662[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.011322[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00393236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0067505[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00457877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00699467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0109757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00549438[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00712329[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0110651[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00414406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00418493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0045741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0040631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00520271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00730922[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.004896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00502687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00388316[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0041912[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00415973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.010837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.011158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.006784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00688544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00511935[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00395219[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00412444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00445397[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00482392[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00578648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00603143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00675008[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0111712[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0910017 seconds. Fastest Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00388316[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65fbe45b4cb1d8a5[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.106653[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0920773[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00580017 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0920773[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0304727[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0169115[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0406791[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0167307[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0303205[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0305522[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0123658[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0168459[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0169307[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.044744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0123962[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0168208[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0123253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0574756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0175189[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0305241[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0170107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0130158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0175551[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.059088[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.058768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0311971[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0123789[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0168816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0343701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0128825[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0171163[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0573209[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0673422 seconds. Fastest Tactic: 0x08af511817b7463e Time: 0.0123253[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x08af511817b7463e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0234382[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0170864[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0190246[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0338304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0135262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0109863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0109082[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0305901[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0158647[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0440973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0197628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0159208[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0612658[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0110779[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0254095[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0396516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.030689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0102807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0578436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0236956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0142582[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0207385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0237504[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0245128[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0216993[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0278062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0166891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0235641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0576924[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0293876[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0168859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00982243[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0170181[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0559129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0358485[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.021572[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0389025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0135889[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0206281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0295102[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.0161046[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0444067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0145753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0321687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0135872[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0620462[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0134135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.029376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.015142[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0239909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0578276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0113966[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0441973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0574827[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.06168[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.019005[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0166869[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0442907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0144941[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0168587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0237383[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0144493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0236551[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0235733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0361696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0441[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.02351[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0112839[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.044208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0235676[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0362453[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0618756[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0207667[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0211727[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0192439[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0308393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0177527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0171456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0575484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0304262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0168827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0135953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0103641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0134016[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0441427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0143773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0589991[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0271319[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.023907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0291484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0590649[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.034912[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0238179[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0575716[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.04414[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0113451[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0291796[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0235854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0121798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0292809[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.272952 seconds. Fastest Tactic: 0x1d53511430a5d47e Time: 0.00982243[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1d53511430a5d47e[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0941333[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0712235[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00545872 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0712235[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0191739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0362848[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0150697[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.013597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0199065[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0114874 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.013597[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0617031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0152005[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0611502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0178268[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0112405[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0322473[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.031776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0136461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0373227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.010345[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.01771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.010516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0320398[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.032096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0177622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0115263[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0117411[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0115174[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0138978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0245432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.018034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0610987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.061408[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0553026 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.010345[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146245[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.012003[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0150358[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126902[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0105661 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.012003[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00642031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00464281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00446208[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00861801 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00446208[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0138607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00372693[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0084008[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00460975[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00851307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0135552[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00576331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00866626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0132546[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00426234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00439986[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00453146[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00373511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00578299[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00908339[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0054769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00554055[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00382764[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0041222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00406001[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0133534[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0133879[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00829893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00833587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00561973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00386795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00392847[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00395884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00467659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00671061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0059781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00818394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0134272[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0926899 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00372693[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]}[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023])[0m
[38;5;13m[V] Compiler backend is used during engine build.[0m
[38;5;104m[X]  (foreignNode) Set user's cuda kernel library[0m
[38;5;104m[X] Subgraph compilation completed in 2.812 seconds.[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0475307[0m
[38;5;104m[X] {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023]) profiling completed in 2.83647 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0475307[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.0085408[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00844507[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0090093[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00864[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00893726[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.014961 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00844507[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00849947[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00839067[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00899509[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00857956[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00891902[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.014613 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00839067[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00595391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00786481[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00561707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00668565[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00608175[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00677781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00688326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00671466[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00745126[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00614187[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00678528[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00583117[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.006962[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00650421[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00613586[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00784744[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00581646[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00562774[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00556373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00762109[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00654118[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00648677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00584644[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00627398[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00666457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00666165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00598629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00668437[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0804428 seconds. Fastest Tactic: 0x712e1cc2c7813ee9 Time: 0.00556373[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x712e1cc2c7813ee9[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00560409[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00598534[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00786009[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00559911[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00626805[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00671061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00605276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00680337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00686084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.0067456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00745884[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.006144[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00679744[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00580984[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.0060831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00700666[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00650708[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00609261[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00560622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00784844[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00581205[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00567376[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00613353[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00764776[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00583798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.00643549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00648882[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00655059[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.00582106[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.006464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00687543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00743348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00581425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.00561902[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00629575[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00651528[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00669483[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00610405[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00544172[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00618588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00699378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.00597029[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.138241 seconds. Fastest Tactic: 0xa6235a0b3508ed71 Time: 0.00544172[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa6235a0b3508ed71[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00533198[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00630199[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00626608[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00589792[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00649641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00628069[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00675904[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00514314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00504884[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00648369[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00742471[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00692745[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00671467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00550505[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00616883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00617038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00562933[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00552184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00693703[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00556551[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0062078[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00576497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00596095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00689828[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00566382[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00529829[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00505859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00742803[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00649744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00565044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00577159[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00560196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00709044[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00588108[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.0056913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00625087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00552813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00592562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00577894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00694644[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00553006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00571101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00676736[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00527283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00573849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00589979[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00612209[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00758691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00624909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00613275[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00597619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00609532[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00513904[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00731571[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00578225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0058545[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00519089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00617309[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00649436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00613392[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00631568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00571336[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00548389[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00686367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.0061312[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.0059215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00579016[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.006998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00634144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00600648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00679104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00618015[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0062242[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00639074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.0051648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00582492[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00785662[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00568769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00559485[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00573849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00762739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00567141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00547987[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00591738[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00550435[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0068591[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00607612[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00596019[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00566002[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00602495[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00628188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0053694[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.0052605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00704533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00613702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00583099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00639416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.006044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00641682[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00753635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00684756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0079102[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00552324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00735838[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00716686[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00567828[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00751786[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00630118[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00554684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00557742[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00528283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00690373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00621077[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00544275[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00547218[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00626963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00595791[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00563271[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00578501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00609086[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00545394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00586274[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00655875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00715211[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00538632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00824221[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.364132 seconds. Fastest Tactic: 0x4f35593c356e2e7e Time: 0.00504884[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4f35593c356e2e7e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00534248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00627931[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00621452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00591682[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00654222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00634365[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00678379[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00513067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00505131[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00647692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00742827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00686955[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00674133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00549491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.0060833[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00615661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00565442[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00548494[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.0068948[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0055648[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00621472[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00574572[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.0059084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00687412[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.0056772[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0052927[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00503212[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00743632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00651487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00558773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00578244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00555226[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00714281[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00589193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.005664[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00624573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00552271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00595524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00576368[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.006976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00552883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00571733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00676267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00531894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.0057532[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00586835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00612461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00760291[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00625087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00618114[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00599733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00613547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00515019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00735026[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00577545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00577858[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00517219[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00614478[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00647364[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00615603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00637726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.0057459[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00543295[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00684234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00613159[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00589942[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00578979[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00693595[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00636357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00601238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00677141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00616883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00616689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00640503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00511095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0058446[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0077822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00574029[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00558898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00576497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00760194[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0056819[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00545893[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00579954[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00549543[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00696378[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0061087[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00590147[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00562809[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00599429[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00631225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00538529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.0052855[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00712034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00615564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00585619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00639054[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00609067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00642626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00754276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00677802[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00788316[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.0055033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00735165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00720136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00567847[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00755129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00631044[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00555034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.0056016[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00530269[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00690808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00621768[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.005456[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00546588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00626331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00593553[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00563165[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00579237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00602172[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00541282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00590522[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00656941[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.0071655[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00529727[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0082227[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.358816 seconds. Fastest Tactic: 0x5e4918ccf433630e Time: 0.00503212[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4918ccf433630e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00733217[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00727954[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0091102[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.0085136[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00888898[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146657 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00727954[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.0073324[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00727188[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00908281[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00848267[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00885305[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146485 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00727188[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00580874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00493129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00530235[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00439845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00567774[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0041201[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00428212[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00746406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00440028[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00427528[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00412523[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00544516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00442246[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0073113[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.0061506[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00491122[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00437292[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00530303[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00430154[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00431713[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00739223[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.0073753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.0054197[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00437624[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00413682[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00411904[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00566653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00430031[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0799685 seconds. Fastest Tactic: 0x937b84b4175ec19c Time: 0.00411904[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x937b84b4175ec19c[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00527617[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00586835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00489785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00528617[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00730597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00438261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00563574[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00409496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00424452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00746098[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00439944[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00425357[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00412405[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00541695[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00432014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00438835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00733959[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00617154[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.0041672[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00491812[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00436641[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00529558[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00426964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00432246[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00593984[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.00738945[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00397638[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00736904[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.00543398[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00740777[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00429867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00441361[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00543467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.0052855[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00432055[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00397752[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00410367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00409262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00567413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00429142[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00489569[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.00427706[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.121072 seconds. Fastest Tactic: 0x788dd0382d5ebd44 Time: 0.00397638[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x788dd0382d5ebd44[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00570504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00452627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00440196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00468504[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00795479[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.0042761[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00823337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00543123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00547375[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00801219[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00446635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00464918[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00823415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00467363[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00724962[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00446834[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00645046[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.0043293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00465689[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00508412[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.006736[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00455769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00485766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00432561[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00465718[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00562187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00531336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00564978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00774133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00468444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00482468[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00632231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00530844[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00451488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00392897[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00415249[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00452425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00738296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.0043107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00489229[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00460961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00488858[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0059541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00481554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.0064558[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.0075072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00469979[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00759564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00428499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00667669[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00440323[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00549229[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00440196[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00444407[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0041107[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00481874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00409327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.0044992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.005056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00800736[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00628109[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00507588[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00826433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00454746[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00455913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00644082[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00457264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00435283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.0040194[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0045029[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00417187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00497694[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00468044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00385974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00480884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00584184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00629193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00631728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00644985[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00473945[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0063992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00632433[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00633157[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00606759[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00822712[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00469619[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.0045525[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00449579[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00471962[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00485101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00581058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00564427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00493804[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00752782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00433953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00452699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00428294[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00438178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00461574[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00452036[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00464281[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00498022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00474561[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00469949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00662735[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0044014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00768073[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00506424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00475628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00453535[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00509834[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00752545[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00463067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00659012[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00579439[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00741713[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00494337[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00616146[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00438835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.0074688[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00494447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00410367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00471331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00477882[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00399911[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00541282[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.357226 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00385974[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00573523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00453694[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0043653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00465422[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00794717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00417053[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00821177[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00540473[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00550155[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00796978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00445895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00465793[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00821386[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00463215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00723313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00447659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00644985[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00435477[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00473435[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00507604[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00676992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00463778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00494102[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.0043315[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00467155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00564516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00531742[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00568136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00774352[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00469559[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0048099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00630481[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00532504[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00451993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00393412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00415842[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00455668[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00735327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.0043387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00490126[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00465318[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00487606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00587434[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00482377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00644267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00748871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00469466[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00756243[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00427555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00671083[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.0044233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00549508[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00438751[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00443916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00410732[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00484066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00412352[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00449607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00507539[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00799695[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00632715[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00507814[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00827837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00457278[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00456782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00643303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00454126[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00435422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00406246[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00448569[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00412457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00494494[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0046837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00385876[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00481508[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00581554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00630581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00625738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00643426[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00474201[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00645826[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00628859[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00632996[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00602667[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00824872[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00466089[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00455639[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00449522[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0046923[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00484282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00581058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00561476[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00496533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00755105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00434951[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00448953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00430414[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00438303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00459105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00455913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00467007[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00501524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00472864[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00467289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00660371[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00436364[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00768849[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00505584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00472233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.004544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00507927[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00752901[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.0046156[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00661877[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00577802[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00743514[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00496408[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00613256[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00438723[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00744344[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00498181[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00410263[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00471887[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00477684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00398806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00544207[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.361036 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00385876[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27[0m
[38;5;104m[X] =============== Computing costs for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00978072[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0087628[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0100709[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00904649[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0113532 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0087628[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00603067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00454025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00434701[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00869223 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00434701[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0110128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00336736[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.007086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0039901[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00713781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.010585[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00493239[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0071562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0103593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00384625[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00395708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00400178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00344094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00496361[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00729252[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00465881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00478019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00355041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00375982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00365728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.010527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.010412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00677099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00674666[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00480823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00355779[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00352898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00356914[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.004184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00561938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00503578[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00661229[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.010579[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0918433 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00336736[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00244049[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00237435[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.0024112[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00254263[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00240621[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00225665[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00310933[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00270305[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00248604[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00239619[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00245137[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0312547 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00225665[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00240353[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00238071[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00240921[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00252735[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00239748[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00222976[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00310035[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00270305[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.0024651[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00241159[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.0024313[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0295292 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00222976[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00243022[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00236402[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00240798[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00250714[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00239642[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00226241[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00311506[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00270262[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00249043[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.0023971[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00246141[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0296023 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00226241[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00229793[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00250468[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00240882[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00279129[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.0027141[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00261325[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00354448[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00334443[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00335062[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00312125[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00254681[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00251124[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00229085[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00278613[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00252646[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.00241466[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00351708[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.00294587[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.00270572[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.00279565[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00246969[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00239482[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00268553[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00318456[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00228369[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00253487[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00248407[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0798554 seconds. Fastest Tactic: 0x000000000000001c Time: 0.00228369[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.0023496[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00252551[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00279022[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00367074[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.0023277[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0143739 seconds. Fastest Tactic: 0x000000000000001f Time: 0.0023277[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001f[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00359444[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00352169[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00386146[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.00864732 seconds. Fastest Tactic: 0x000000000000001d Time: 0.00352169[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001d[0m
[38;5;104m[X] =============== Computing costs for /model/encoder/Resize[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,400,20,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00914133[0m
[38;5;104m[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.00225788 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00914133[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00254352[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00252937[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00284909[0m
[38;5;104m[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.00842533 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00252937[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0147228[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0120069[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0151137[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126226[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0108555 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0120069[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00675456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00513083[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00517317[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0083033 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00513083[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0140044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00472533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00833733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00466667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00857135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0135966[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00589193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00869087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0112917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00457775[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00467185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00469964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00491749[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00578888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00911567[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00568769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0056734[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00441642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00467496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00444729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0133141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0113863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00828176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00831402[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00566888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00454385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00435283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00491843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00497624[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00676352[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00613295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00820189[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0116476[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0890279 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00435283[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.010466[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00466178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.010305[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00750388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00455582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00527267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00714122[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00472969[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00435408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0104743[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.0044608[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00606914[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00432766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00521067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00537465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00471031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00532301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00445739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00429593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0103674[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0106043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00696911[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00551904[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00736348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00703066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00701178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00536025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00457103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00443003[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00505956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0111559[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00426072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00548582[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0919001 seconds. Fastest Tactic: 0x709ddd0e503c7fd7 Time: 0.00426072[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x709ddd0e503c7fd7[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0110555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00566038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00703378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0056602[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00532588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0179537[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00788788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.007[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0107689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00543432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0106323[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0178998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00715756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0108762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0068837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.005996[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00549054[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00532301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0182524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00532809[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0179963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00726748[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0567486 seconds. Fastest Tactic: 0x6fd15a9d85252b17 Time: 0.00532301[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6fd15a9d85252b17[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6fd15a9d85252b17, 0.00532301 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0274724[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0216447[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00503139 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0216447[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00818576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0134289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00665391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00631628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00841707[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0126095 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00631628[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.021014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00630058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0189576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00757891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00567214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.012256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0117576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00622973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0135484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00542176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00747615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00564285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0120282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0121215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00747354[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00565894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00546553[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0057635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00635089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00941807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00771442[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0207718[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.019168[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0601898 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.00542176[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00295995[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.0028386[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00284845[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00292463[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00278373[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00283589[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.0035209[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00307141[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.0029625[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.0028056[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00292791[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0337592 seconds. Fastest Tactic: 0x0000000000000004 Time: 0.00278373[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000004[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00365368[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00418773[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00568552[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00555591[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00660141[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.0097472[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00872287[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.0103011[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.0160869[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0195301[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00350422[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00338338[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00425046[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00601867[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.0029821[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00325799[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00346249[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00422049[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.0055824[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00352326[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00311417[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0541777 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.0029821[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00607806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00390512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00603981[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00507701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0041226[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00399416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00497192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0041952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00405526[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00630239[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00396273[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00481493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00361726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.0039014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00412168[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.0040401[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0041172[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00406233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00357504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00615021[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00626331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00472368[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00429019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00495075[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.0048064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00469185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00410627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00386121[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00385569[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00388602[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00650072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00392671[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00427405[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0984027 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00357504[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3[0m
[38;5;104m[X] =============== Computing costs for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00762836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00469126[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00770982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00602933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0045985[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00441951[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00581113[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00476221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00454717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00787696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00445724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00530523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00410263[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00445739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00455913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.0048388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00453852[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00460771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00408728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00784198[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00771418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.0055824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00469514[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00577324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00561742[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00550732[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00452267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00447474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00435269[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00434715[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00822999[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00461472[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00462276[0m
[38;5;104m[X] model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.093313 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00408728[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3[0m
[38;5;104m[X] =============== Computing costs for /model/encoder/Resize_1[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0292542[0m
[38;5;104m[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00189398 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0292542[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00368844[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.0033725[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00416987[0m
[38;5;104m[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00819173 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.0033725[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0152427[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0121688[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0160894[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0128226[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107297 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0121688[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00852214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00816312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0105197[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00743346 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00816312[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0140902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.010154[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00969874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00928267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00987639[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.013766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00896814[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0100838[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0128693[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00828878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0083464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00927466[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0102507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00837093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.010528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00863781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00859241[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00944681[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00979139[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00891902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0134093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.013161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00997584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00982619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.008416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00951111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00884941[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0103273[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00830205[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00990651[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0084536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00973653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0132919[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0833218 seconds. Fastest Tactic: 0x960e9baa2a6cad5b Time: 0.00828878[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x960e9baa2a6cad5b[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,2560,32) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0106793[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.0102807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0104155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00760873[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00857026[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00595144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00728209[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0102972[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00921571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.010568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00945096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00746761[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00664972[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00620998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00667733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.0105153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00631306[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00946104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00658322[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.01055[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0107929[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00708489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00680359[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00734632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00718979[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00712329[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00607748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00683298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00910587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00614924[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0111758[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.0090978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00628931[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0862459 seconds. Fastest Tactic: 0x458f02d2b10db57c Time: 0.00595144[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x458f02d2b10db57c[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0110424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0102762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00741523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0131618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00818992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.017984[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00819018[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0078849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0107874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0127372[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0107267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0179116[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00737785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0109357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00780403[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00888758[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0130576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0124583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0183309[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0100053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0180334[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00752284[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0551353 seconds. Fastest Tactic: 0xbd976ef514eaa406 Time: 0.00737785[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbd976ef514eaa406[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0xbd976ef514eaa406, 0.00737785 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0218893 ms[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x23b890da05937b9e, 0.00901189 ms[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0x2d8ab2aa0639fda9, 0.00955977 ms[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1), Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00538099[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00422616[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00423885[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00401181[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00388998[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00379103[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00399136[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00380994[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00387298[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00388688[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00528833[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0288564 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00379103[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00815532[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00679403[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.0082508[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00726678[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00836027[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.011158[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00914162[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.0120427[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.016513[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0201387[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00713827[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.0065805[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00661375[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00702422[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00604172[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00601105[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00601448[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00660476[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00885362[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00699978[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00602038[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0459426 seconds. Fastest Tactic: 0x0000000000000019 Time: 0.00601105[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000019[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00628603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00727327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00629333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00563076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00788143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.0060703[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.0054748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00771612[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00703333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00641169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.0075501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00801397[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00610657[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.0063042[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00660308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00747638[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00648431[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.007664[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00593479[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00636438[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00635875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.0052515[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00705867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00540938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.005283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00517973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00629494[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00659367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.007032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00624849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0066353[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00696955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00644267[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0902732 seconds. Fastest Tactic: 0x65a38dbc9e991257 Time: 0.00517973[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65a38dbc9e991257[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0123154[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0122206[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0130498[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126902[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0105209 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0122206[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.0113447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00994729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0116373[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0069427 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00994729[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.014573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0101669[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0124401[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0102416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0124871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0142111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0117797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0128357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0138709[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.010303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0104847[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0101152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0103499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.0117069[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0128615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0113351[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0114322[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.010432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0107936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.0098469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0139676[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0138624[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0120328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0122[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.011382[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0103987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00958537[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.010694[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0112686[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0139373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0120903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.0120164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0139898[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0816559 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00958537[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178363[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.010503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0130695[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00841387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0314356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0119901[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.010518[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0116999[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0316412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0112999[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0105283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00858475[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0129165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0115388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0321406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0103143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0316461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.010735[0m
[38;5;104m[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0521888 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.00841387[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.010527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.010529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0131085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.008436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.031584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0119768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0105207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0180497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0116513[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0314347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0112729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0105273[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00857326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0129432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0114884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0321096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0102949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0315307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0107253[0m
[38;5;104m[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0545757 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.008436[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0xff6944b17d5b2e32, 0.0120069 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x733ba2a91a48d431, 0.00513083 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x9ec201b34455146e, 0.00435283 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) [Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x709ddd0e503c7fd7, 0.00426072 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6fd15a9d85252b17, 0.00532301 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6fd15a9d85252b17, 0.00532301 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0216447 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x85c1a5f7f239cf84, 0.00631628 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x45f7566cdb2b10fb, 0.00542176 ms[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000004, 0.00278373 ms[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000018, 0.0029821 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x2eba0b6a8ec55fa3, 0.00357504 ms[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00988643[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00880505[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0102526[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00903005[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0108867 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00880505[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00570811[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00484715[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00498834[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0083519 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00484715[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0110083[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00488672[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00701511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00535263[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00717889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.010734[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00550942[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00722156[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0103997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.0049998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00507038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00547445[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00485503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00548809[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0074221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00528433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00536974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00506149[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00537583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00532131[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0104733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.010652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00694733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00688631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0053889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0049807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.005238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00489677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00524583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00621097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00567449[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00676331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0105987[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0877275 seconds. Fastest Tactic: 0x5bd8221bd57baf93 Time: 0.00485503[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5bd8221bd57baf93[0m
[38;5;104m[X] =============== Computing costs for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(6400,1:16,320,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00759467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00626963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00743467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0313542[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0117164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.010335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00683799[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0173387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0315219[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0110169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0103017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00864356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00573343[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00672853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0319835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00713123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0312659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0106717[0m
[38;5;104m[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0539827 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00573343[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178386[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.007576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104723[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00625679[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00743372[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0314027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0117422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0103169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00685475[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0174144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0314027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0110359[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0103095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00865067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00572909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0067072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0320378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00716505[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0313571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0106377[0m
[38;5;104m[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.055521 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00572909[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146305[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0120046[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0151709[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126487[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107941 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0120046[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00675392[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00507135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00477105[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00842898 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00477105[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0138863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00344577[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0084088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00452079[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00855412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0135543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00581665[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00869005[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.011189[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00432164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00438961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00452151[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00353774[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00584386[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00904014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00547497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00562311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00377912[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00407948[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.0040329[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0131606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0113774[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00817119[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00817899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00569239[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00384589[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00402249[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00381382[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00464918[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00675754[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00595314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00805968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0115943[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0896944 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00344577[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.010473[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00358469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0101631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00740359[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00430455[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00531251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00722927[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00383179[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00373926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0105233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00399162[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00594377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00412444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00512953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00529134[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00365055[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00526833[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00400724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00412457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0103004[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0107303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00700689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00552219[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00709379[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00687064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00676352[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00539613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00449251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00394704[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00510885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0111538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00372291[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00555822[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0929264 seconds. Fastest Tactic: 0xc6cdb1e47323bb01 Time: 0.00358469[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc6cdb1e47323bb01[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0110366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00563627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00701555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00472864[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00533994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0179312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00783628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00707533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0107313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00497098[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.010626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0180132[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00718389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0108666[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00687042[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.0059661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00437901[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00485565[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0182204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00523933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0179357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00732892[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0579403 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00437901[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1)] got cached result: CaskConvolution, tactic 0xc985777c89c6b3a4, 0.00437901 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0272189[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0217573[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00504343 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0217573[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00820917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0134869[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0066125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00626746[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0084856[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0126853 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00626746[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0210587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00622756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0189031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00754916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00436557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0122206[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0117278[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00618647[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0135245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00485874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00750222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00502289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0119638[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0121524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00757025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00549631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00534789[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00467926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00619714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00939496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00778071[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0207674[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0190791[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0589499 seconds. Fastest Tactic: 0xd14bd6d95fefd45e Time: 0.00436557[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd14bd6d95fefd45e[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,400,20,1), Float(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00242056[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00254279[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.0023968[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00277439[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.0024952[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00240522[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00338888[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00296477[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00268228[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00251292[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00238514[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0317592 seconds. Fastest Tactic: 0x000000000000001c Time: 0.00238514[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1600,400:32,20,1), Float(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.0033698[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00390487[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00545634[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00545101[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00662065[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.00946489[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00862633[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.0103767[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.0146676[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0190121[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00285703[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00320954[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.004192[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00600515[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00241886[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00270968[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00318527[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.0041772[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.0051799[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00289939[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00252078[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0522117 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00241886[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(6400,1:16,320,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00606255[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00318375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00601638[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00508768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00357628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.0038947[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00338899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00325406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00620267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00345479[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00464652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00333792[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00376977[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00404434[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00314019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00396624[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00336085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00327153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00618153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00625304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.0046797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.0042164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00485782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.0047079[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00464252[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00399974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00372705[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00364707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.0036984[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00646503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00322544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00410276[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.101392 seconds. Fastest Tactic: 0x44824770683c7b80 Time: 0.00314019[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x44824770683c7b80[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00981772[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00878512[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0100718[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00901766[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0109178 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00878512[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00565532[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00419453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00406451[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00873535 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00406451[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0108831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00335915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00688261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00397905[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00703955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.010622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00491733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00711149[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0103425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00378223[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00385618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0039621[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00340495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00493067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00729623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00468711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00474877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00340854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00368844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00362818[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0103509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0103971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00672725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00679338[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0047939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00350945[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00359455[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0036043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00412866[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00565171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00506376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00667115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.010528[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0903687 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00335915[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]}[0m
[38;5;104m[X] *************** Autotuning format combination: Int64(2,1), Float(1638400,6400,80,1), Float(409600,1600,40,1), Float(102400,400,20,1) -> Float(57600,192,24,2,1), Float(57600,192,24,2,1), Float(57600,192,24,2,1), Float(28800,96,12,1), Float(28800,96,12,1), Float(28800,96,12,1), Int64(300,1), Float(1200,4,1), Float(300,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023])[0m
[38;5;104m[X]  (foreignNode) Set user's cuda kernel library[0m
[38;5;104m[X] Subgraph compilation completed in 9.486 seconds.[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.410624[0m
[38;5;104m[X] {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023]) profiling completed in 9.61819 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.410624[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00496314[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557049[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00493129[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00825774 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00493129[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00925722[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00574427[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00402686[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00728693 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00402686[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.028808[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0240145[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0287653[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00521482 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0240145[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.215957[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0188693[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.216197[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00608641 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0188693[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0101498[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00581278[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102035[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00659118 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00581278[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0295564[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0251985[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0295378[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00554067 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0251985[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.216064[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0226524[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.215947[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00636476 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0226524[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00851467[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00963169[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.008496[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00717269 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.008496[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0300036[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0121234[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0300293[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00624991 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0121234[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.216117[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0306812[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0159568[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0061123 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0159568[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00888618[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0503284[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00890667[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00706376 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00888618[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0109609[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0142453[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0108893[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00717965 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0108893[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.217088[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0241272[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.217253[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00677939 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0241272[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0117326[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.050179[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0117028[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00702337 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0117028[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0130039[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0150734[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00521967[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00813614 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00521967[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0300693[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0125262[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0300356[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00663602 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0125262[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0261465[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0120495[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0260825[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00636389 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0120495[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0639698[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0125531[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0640231[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00652285 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0125531[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00986039[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0107895[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00470415[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00791616 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00470415[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0198776[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00969265[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0199021[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00637723 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00969265[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0120495 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0125531 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00470415 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00969265 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(1638400,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0500343[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0117381[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0500008[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00618314 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0117381[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.138997[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0108391[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.138852[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00674157 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0108391[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,102400:32,320,1) -> Int8(1638400,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0155059[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0156436[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.006528[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00716686 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.006528[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.010575[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0108559[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00953691[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00684594 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00953691[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102885[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00966187[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.010241[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00667852 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00966187[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0264501[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00967192[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0036807[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00748315 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0036807[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0132816[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00833493[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0133367[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00636478 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00833493[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00620484[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00779237[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00336533[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00800687 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00336533[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102972[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00706022[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102772[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00658252 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00706022[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00948503[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00717912[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00436253[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00726948 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00436253[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0111406[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00671701[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0111399[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00654232 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00671701[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.027856[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00709266[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0277243[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00603632 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00709266[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102823[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00720431[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102752[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00657621 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00720431[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0264862[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00701155[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00367918[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00718281 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00367918[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0135185[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00672149[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0134921[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0063688 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00672149[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0341387[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00705244[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0342464[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0063514 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00705244[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00627674[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00681861[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00337034[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00887576 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00337034[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0103215[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00630118[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102274[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00710725 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00630118[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00672149 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00337034 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00436253 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709266 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00367918 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00672149 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00337034 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0416083[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00691156[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0416284[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00682823 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00691156[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0222007[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00772461[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0221753[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00668398 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00772461[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0415964[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00783826[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0415799[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00656257 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00783826[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0221767[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00877362[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.022204[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00667252 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00877362[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00520501[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00732637[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00523533[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00832169 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00520501[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00847174[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00614071[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.003728[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00820876 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.003728[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00938844[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552586[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00938163[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00721767 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552586[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0277813[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556889[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.027824[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0062879 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00556889[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0224007[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00569347[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0223893[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00641387 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00569347[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0226482[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00621729[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0225273[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00645409 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00621729[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00930281[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00648205[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00929896[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00718305 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00648205[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0409304[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00637142[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.040941[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00650849 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00637142[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00436253 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00671701 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709266 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00720431 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00367918 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00672149 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00337034 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00630118 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00672149 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00337034 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00691156 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00772461 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00520501 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.003728 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552586 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00556889 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00569347 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00621729 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00648205 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00637142 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00436253 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709266 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00367918 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00672149 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00337034 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00436253 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00671701 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709266 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00720431 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00367918 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00672149 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00337034 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00630118 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0126862[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00767976[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.012749[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00696277 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00767976[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.022565[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00794997[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0225436[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00632015 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00794997[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00463511[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00716709[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00286687[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00959033 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00286687[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:4,80,1) -> Int8(12800,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00948415[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00569654[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00259787[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00964134 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00259787[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,6400:32,80,1) -> Int8(102400,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00362771[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055824[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00268723[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0104667 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268723[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.02203[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563129[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0220813[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00658951 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563129[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0123508[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549316[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0123368[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00686278 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00549316[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.022112[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549263[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0221227[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.0065927 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00549263[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0123536[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00536601[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.01232[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00698763 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00536601[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00380994[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00546536[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00383534[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00817402 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00380994[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0053633[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545669[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0030104[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00798323 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0030104[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00964968[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559662[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00964968[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00651476 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559662[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0153867[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541265[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0153726[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00620985 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00541265[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0125867[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545256[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0125558[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00633523 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00545256[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0127269[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00601029[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0127921[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00640452 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00601029[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00583099[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00647836[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00579311[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00732144 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00579311[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0222848[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00681535[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0222542[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00632883 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00681535[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00620642[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00909867[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00326058[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00793047 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00326058[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00938074[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00756385[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.009296[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0068821 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00756385[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.015631[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00798324[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.015697[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00593444 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00798324[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00644308[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0079097[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00649539[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00693334 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00644308[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0156175[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0075309[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00318192[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00734701 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00318192[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00767976 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00794997 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286687 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00637102[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00656627[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00633942[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0071773 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00633942[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00767976 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00794997 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286687 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563129 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00549316 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00380994 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0030104 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559662 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00541265 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00545256 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00601029 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00579311 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00681535 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00326058 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00798324 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00318192 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00767976 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00794997 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286687 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00326058 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00756385 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00798324 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00644308 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00318192 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00767976 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00794997 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286687 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00633942 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00807721[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00574731[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00811937[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00688119 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00574731[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0132139[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00574391[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0132443[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00645397 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00574391[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00359467[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005808[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00269419[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00871568 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00269419[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00613993[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00576184[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00260523[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00831634 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00260523[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00312683[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00569293[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00253099[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00878254 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00253099[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.012352[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563467[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0123879[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00631185 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563467[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00746927[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554912[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0074624[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00684914 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554912[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0124468[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557316[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0124041[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00629881 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557316[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00747662[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552638[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0074688[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00683511 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552638[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0029801[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00546082[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00295722[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00823262 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00295722[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00397663[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562791[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00255267[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00787478 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00255267[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00709111[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552603[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00710672[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00688633 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552603[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00918544[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0056336[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00914854[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00658152 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0056336[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00760048[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553932[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00756267[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00676833 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00553932[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00772824[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054734[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00774109[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00684106 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0054734[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0041692[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558045[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00422022[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00780758 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0041692[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0126846[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00546323[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.012717[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00643909 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00546323[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00448469[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565008[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0029197[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00816888 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0029197[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00733611[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055648[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00736325[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00697681 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055648[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00951318[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00573017[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00952625[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00659813 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00573017[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00470565[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554369[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00469244[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00823951 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00469244[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00941155[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554422[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00303564[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00765937 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00303564[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574731 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574391 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00269419 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00453117[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555611[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0045489[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00764978 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00453117[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574731 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574391 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00269419 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(409600,1600,40,1) -> Float(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563467 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554912 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00295722 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00255267 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552603 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0056336 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00553932 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0054734 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0041692 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00546323 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0029197 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573017 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00303564 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574731 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574391 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00269419 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0029197 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055648 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573017 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x0000000000000000, 0.00469244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00303564 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574731 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574391 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00269419 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00453117 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00574319[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562276[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00573921[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00723798 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562276[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00844907[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557831[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00845067[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00667903 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557831[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00314168[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543811[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00306995[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00817254 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00306995[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00453449[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.010966[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00359123[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00731892 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00359123[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00389879[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0107957[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00361921[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00671875 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00361921[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00731154[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0106257[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00730551[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00697128 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00730551[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00504549[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0101766[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00503833[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0073997 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00503833[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00731594[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054769[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00731571[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00688839 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0054769[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00503212[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00539768[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00502384[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00750543 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00502384[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00269342[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544344[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00270899[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00878612 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00269342[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0031365[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547078[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00238964[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00862742 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00238964[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00517087[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549316[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00513822[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00733245 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00513822[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00608369[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551624[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0060769[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00704362 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551624[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0050369[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00542624[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00503498[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00739748 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00503498[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0051904[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548914[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00517481[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00736386 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00517481[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00327247[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547812[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00329506[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00834904 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00327247[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00792285[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554404[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00790946[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00680393 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554404[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00341475[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00566635[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00268852[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00855079 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268852[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0057224[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054634[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00574536[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0073422 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0054634[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00629011[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567159[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0062803[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00700837 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00567159[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00375083[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00570613[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00373049[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00796632 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00373049[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00622163[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00574265[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00297308[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0075734 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00297308[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562276 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557831 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00306995 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00356959[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00564693[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00354448[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00814962 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00354448[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562276 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557831 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00306995 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(204800,400,20,1) -> Float(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00730551 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(6400,400:32,20,1) -> Float(204800,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00503833 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00238964 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00551624 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00517481 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554404 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00297308 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00306995 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00370134[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549963[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00364835[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00810523 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00364835[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00326058 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00756385 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00798324 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00644308 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00318192 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00767976 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00794997 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286687 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00633942 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0252168[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00636579[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0252404[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00609828 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00636579[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0256615[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561547[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0255756[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00612423 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561547[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0430347[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00598[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0430947[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00594085 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00598[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0132074[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00647098[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.013255[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00632601 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00647098[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00629917[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00637746[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00340159[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00777247 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00340159[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0103043[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00613101[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0103111[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00655129 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00613101[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0222613[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00646339[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0222343[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00607191 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00646339[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0229006[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00682101[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0229497[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00604444 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00682101[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0103444[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00719796[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0103677[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0066082 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00719796[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0419973[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00738829[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.042372[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00586436 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00738829[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0132751[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00765212[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0132148[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00650171 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00765212[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00636176[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00714939[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00342498[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00795682 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00342498[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.010316[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00675733[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0103069[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00658845 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00675733[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0273789[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00697733[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00387758[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00703372 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00387758[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0029197 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055648 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573017 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x0000000000000000, 0.00469244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00303564 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574731 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574391 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00269419 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00453117 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00881544[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00621413[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00880505[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00672655 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00621413[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00898583[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00667051[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00902112[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00654911 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00667051[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.013948[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00648862[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0140533[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00618128 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00648862[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00586723[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00647529[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00587097[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00712093 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00586723[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00364997[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00595505[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267895[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00849217 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267895[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00453939[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567846[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00453564[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00757215 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00453564[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00781445[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552673[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0078467[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00684964 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552673[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00820683[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00569003[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00826199[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00668138 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00569003[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00451964[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00566436[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00450782[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00760259 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00450782[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0131577[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561689[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0132197[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0063624 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561689[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00584589[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00571263[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00585207[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00708073 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00571263[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00358779[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00564[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0027034[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00845618 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0027034[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00452598[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556373[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00454342[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00759621 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00452598[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00989741[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567847[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267503[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00765018 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267503[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.002758[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555873[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00228895[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00847368 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00228895[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00436807[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557707[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00438192[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00762468 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00436807[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00359123 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00361921 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00353033[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054529[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00238522[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00807917 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00238522[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00271168[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555051[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.005211[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00794053 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00271168[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00355779[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00542039[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00240115[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00801045 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00240115[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00495184[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548984[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00496972[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00745693 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00495184[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00443537[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555541[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00524817[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00748055 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00443537[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00364835 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00268834[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549071[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267537[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00877461 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267537[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00368844[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541454[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00369454[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00805912 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00368844[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00270194[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549928[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00269497[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00884073 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00269497[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0112124[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00597505[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0112782[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00642586 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00597505[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00353785[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545204[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00239634[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00804835 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00239634[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00269841[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554842[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00524167[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00795943 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00269841[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00355643[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544413[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00242597[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00799083 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00242597[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00494808[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00538202[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.004999[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00744193 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00494808[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00444168[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557014[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.005255[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0074343 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00444168[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00376677[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547445[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00236565[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00797731 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00236565[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.003744[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557618[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00373985[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00801219 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00373985[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0026694[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549945[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267486[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00871838 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0026694[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0058435[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557707[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00584926[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00711684 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557707[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0109457[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00590522[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0110083[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00647255 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00590522[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0023263[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055712[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0055033[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0075075 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0023263[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00351809[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544791[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00353235[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00820627 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00351809[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00358116[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00537979[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00357424[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00816516 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00357424[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00493192[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00536872[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00492973[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00751723 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00492973[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00443523[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0167227[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00442077[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00749205 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00442077[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0037216[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548844[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00236868[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00995655 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00236868[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00268459[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547585[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0026822[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00905344 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0026822[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00371236[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549596[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00374139[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00804275 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00371236[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00588407[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00546973[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00586012[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00719063 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00546973[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0113106[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00598724[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0113088[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00700087 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00598724[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00368035[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00542108[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00368059[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00834679 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00368035[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00268629[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553548[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00268211[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00972218 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268211[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00365913[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00538202[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00369079[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00846633 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00365913[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00276267[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545118[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00277616[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00942318 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00276267[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0112736[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00590129[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0112658[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00658131 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00590129[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279724[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0160163[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00555556[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00773327 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00279724[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00367461[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0170949[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0036793[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00770381 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00367461[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00280793[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0160848[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00281125[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00867533 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00280793[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00369559[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0170672[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00369489[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00772454 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00369489[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00487791[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.017128[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00486524[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00748959 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00486524[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00241381[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00519335[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00238438[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00825206 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00238438[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(102400,400,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00436807 ms[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00372682[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00540284[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00372385[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00794613 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00372385[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00541196[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005254[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00544172[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00726383 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.005254[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00241021[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00262891 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00241021[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00436336[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00252541 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00436336[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00374068[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00536567[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00374568[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00793909 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00374068[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00541402[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543449[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00539699[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00727966 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00539699[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00381261[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00533333[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00381782[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0078623 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00381261[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00530032[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00540714[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00531928[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00722905 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00530032[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0028442[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00317062 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0028442[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00435699[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00250717 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00435699[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00468044[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551397[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00471316[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00756751 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00468044[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00347278[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547235[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00347889[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00849773 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00347278[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279298[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0053694[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00277545[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00862515 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00277545[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00442695[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00546168[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00291655[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00810595 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00291655[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00734725[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541781[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00732452[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00695015 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00541781[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00953082[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551012[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00949363[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00653466 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551012[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00585432[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547602[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00585076[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00705904 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00547602[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0035848[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547585[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00270675[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00853488 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00270675[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00451647[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548774[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00454025[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.007599 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00451647[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00980571[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553548[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267196[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00771021 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267196[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00624415[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054037[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0033711[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00772655 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0033711[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0159441[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553075[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.016033[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00615139 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00553075[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0154046[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552988[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00402133[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00701021 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00402133[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0140564[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559734[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.014068[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00623009 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559734[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0236892[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00586281[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0236949[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00614834 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00586281[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00472218[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00612596[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00282694[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00822311 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00282694[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00553075 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00402133 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00586281 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00253099 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,1600,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00735629[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00531877[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00733588[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00686827 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00531877[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00504247[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543277[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00499423[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00737647 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00499423[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00531877 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00499423 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00623803[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544998[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00623822[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00706128 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544998[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00581499[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549963[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00587677[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006]) profiling completed in 0.00708978 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00549963[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00453405[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555121[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00454371[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006]) profiling completed in 0.00759344 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00453405[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573017 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00783479[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541695[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00780949[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00685215 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00541695[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574391 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00584908[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552096[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00587397[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00708754 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552096[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00363675[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549351[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00365241[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00809948 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00363675[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00989584[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565084[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00441263[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00712135 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00441263[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0208213[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00632131[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0207128[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00610056 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00632131[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0288684[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00623546[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0287991[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00598124 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00623546[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0132603[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00647774[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0132464[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00637485 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00647774[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00632272[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0066376[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00345754[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00778998 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00345754[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0103037[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00644513[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0103179[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.0065217 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00644513[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0275454[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0067328[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00384883[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00703648 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00384883[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0168069[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00816[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00621551[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00648993 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00621551[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0538057[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00689611[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.053827[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00627601 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00689611[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0482545[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.008608[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0111548[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00612972 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.008608[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0478933[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00803378[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0478385[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00593357 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00803378[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.08304[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00870783[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0829632[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00651623 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00870783[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00960366[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00880309[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00432506[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.007681 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00432506[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00689611 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.008608 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00870783 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286687 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563129 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00549316 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563129 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00549316 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00798324 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0132968[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00722973[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0132714[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00652426 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00722973[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00636498[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00726609[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00340822[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00787478 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00340822[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102817[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00700844[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0103166[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00654674 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00700844[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00985286[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00667157[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00434396[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00742486 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00434396[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0289342[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00663446[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0289236[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00596211 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00663446[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0266281[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0068258[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00442133[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00698358 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00442133[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0229013[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00683298[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0229376[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0060868 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00683298[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0421653[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00732058[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0421613[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00583782 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00732058[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0063203[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00751194[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00341562[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0078657 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00341562[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00663446 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00442133 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00732058 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00586723 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267895 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x0000000000000000, 0.00453564 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571263 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0027034 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00452598 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267503 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00282192[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563822[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0027728[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00873649 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0027728[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00440337[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544981[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00294147[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00786022 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00294147[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00730736[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00542606[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00729878[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00693779 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00542606[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00952716[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005592[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00955368[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00646035 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.005592[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00783553[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0053877[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00781991[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00682604 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0053877[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00826173[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00539991[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00818862[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00670173 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00539991[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00450897[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544774[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00451445[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00759655 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00450897[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0132029[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544946[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0131914[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00640252 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544946[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00583356[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541832[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00584368[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00741931 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00541832[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00359742[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544344[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00270142[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00852255 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00270142[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00452497[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00550103[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00455438[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00758719 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00452497[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00999247[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545858[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267682[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00779402 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267682[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0033711 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00553075 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00402133 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559734 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00586281 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00282694 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00553075 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00402133 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00586281 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00253099 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00531877 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00499423 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00531877 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00499423 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00544998 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) [Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00549963 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00360809[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543191[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00270176[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00860803 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00270176[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00453405 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0029197 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573017 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00303564 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574731 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574391 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00269419 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573017 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00303564 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00574391 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00345413[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00546868[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00348911[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00814779 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00345413[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00285183[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054369[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00231467[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.0083771 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00231467[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0032078[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054252[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0031806[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.0083099 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0031806[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00349133[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557724[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00349789[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00870389 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00349133[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00284719[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557102[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0023502[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00845249 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0023502[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00319309[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562489[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00318659[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00833459 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00318659[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00461837[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567937[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00254133[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00760158 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00254133[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00239124[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00538168[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00241669[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00817997 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00239124[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00277739[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00546833[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00226961[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00836138 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00226961[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.003792[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545927[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00381212[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00828905 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.003792[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00436059[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00527183[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00438863[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00765342 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00436059[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00372148[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00542383[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00373369[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00789711 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00372148[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0038682[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00530895[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00385005[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00788271 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00385005[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00298629[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0052165[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00299381[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00814429 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00298629[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00533994[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00531115[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00534146[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00727583 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00531115[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00239703[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00263753 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00239703[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279547[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00295415 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00279547[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00380558[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00264373 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00380558[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00434978[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00251377 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00434978[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0037216[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00531115[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00372848[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00794558 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0037216[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00384208[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541041[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00385214[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00795852 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00384208[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00297194[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00534654[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00297553[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00874985 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00297194[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00537634[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00526717[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00537075[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0073379 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00526717[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00379394[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00525933[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00380097[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00792017 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00379394[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00377756[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00528367[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00379588[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00801165 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00377756[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00296486[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00525283[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00298791[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00888308 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00296486[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00535721[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00537961[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00531996[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00720459 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00531996[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00283625[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00296828 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00283625[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00284429[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00287797 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00284429[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00384172[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00264725 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00384172[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00436156[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0025116 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00436156[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268852 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00567159 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00297308 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562276 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557831 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00306995 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00567159 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00297308 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557831 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1600,400:32,20,1) -> Int8(12800,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00274929[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00537228[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00217551[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00850219 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00217551[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,400,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00363594[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00531572[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00360969[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0080693 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00360969[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1600,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00306965[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0052365[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0030673[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00861353 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0030673[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(51200,400,20,1) -> Float(1600,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00360969 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(1600,400:32,20,1) -> Float(51200,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0030673 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00383203[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00538202[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00382073[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00798585 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00382073[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00361921 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.022122[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00685148[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0220647[0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0059966 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00685148[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.1/conv/Conv_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554912 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/Conv_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00364835 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] Adding reformat layer: Reformatted Output Tensor 0 to PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (/model/encoder/lateral_convs.0/act/Mul_output_0) from Float(102400,400,20,1) to Float(1,400,20,1)[0m
[38;5;104m[X] Formats and tactics selection completed in 26.3529 seconds.[0m
[38;5;104m[X] After reformat layers: 86 layers[0m
[38;5;104m[X] Total number of blocks in pre-optimized block assignment: 82[0m
[38;5;13m[V] Detected 2 inputs and 9 output network tensors.[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Host Persistent: 4880 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/MaxPool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Host Persistent: 4944 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 13107200 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Host Persistent: 308 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 54067200 bytes[0m
[38;5;104m[X] Skipped printing memory information for 18 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.[0m
[38;5;13m[V] Total Host Persistent Memory: 307312 bytes[0m
[38;5;13m[V] Total Device Persistent Memory: 0 bytes[0m
[38;5;13m[V] Max Scratch Memory: 54067200 bytes[0m
[38;5;13m[V] [BlockAssignment] Started assigning block shifts. This will take 83 steps to complete.[0m
[38;5;104m[X] STILL ALIVE: Started step 76 of 83[0m
[38;5;13m[V] [BlockAssignment] Algorithm ShiftNTopDown took 1.44843ms to assign 6 blocks to 83 nodes requiring 63129600 bytes.[0m
[38;5;104m[X] Total number of blocks in optimized block assignment: 6[0m
[38;5;13m[V] Total Activation Memory: 63129600 bytes[0m
[38;5;13m[V] Total Weights Memory: 23935536 bytes[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Set kernel index: 0[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Set kernel index: 1[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Set kernel index: 2[0m
[38;5;104m[X] Finalize: /model/backbone/MaxPool Set kernel index: 3[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Set kernel index: 2[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Set kernel index: 4[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Set kernel index: 5[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Set kernel index: 2[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Set kernel index: 6[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Set kernel index: 7[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Set kernel index: 8[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Set kernel index: 9[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Set kernel index: 10[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Set kernel index: 11[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Set kernel index: 12[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Set kernel index: 7[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Set kernel index: 13[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Set kernel index: 14[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Set kernel index: 15[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Set kernel index: 13[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Set kernel index: 14[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Set kernel index: 7[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Set kernel index: 16[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Set kernel index: 17[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Set kernel index: 18[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Set kernel index: 19[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Set kernel index: 17[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Set kernel index: 20[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Set kernel index: 21[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Set kernel index: 22[0m
[38;5;104m[X] Finalize: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Set kernel index: 20[0m
[38;5;104m[X] Finalize: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: /model/encoder/Resize Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Set kernel index: 5[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Set kernel index: 25[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 26[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 26[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 17[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Set kernel index: 27[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Set kernel index: 28[0m
[38;5;104m[X] Finalize: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Set kernel index: 28[0m
[38;5;104m[X] Finalize: /model/encoder/Resize_1 Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Set kernel index: 29[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Set kernel index: 30[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 31[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 31[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 14[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Set kernel index: 32[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Set kernel index: 33[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Set kernel index: 10[0m
[38;5;104m[X] Finalize: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Set kernel index: 34[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Set kernel index: 5[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Set kernel index: 25[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 26[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 26[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 17[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Set kernel index: 27[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Set kernel index: 28[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Set kernel index: 10[0m
[38;5;104m[X] Finalize: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Set kernel index: 35[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Set kernel index: 15[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Set kernel index: 36[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 35[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 35[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 37[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Set kernel index: 27[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Set kernel index: 38[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Set kernel index: 20[0m
[38;5;104m[X] Total number of generated kernels selected for the engine: 39[0m
[38;5;104m[X] Kernel: 0 CASK_STATIC[0m
[38;5;104m[X] Kernel: 1 CASK_STATIC[0m
[38;5;104m[X] Kernel: 2 CASK_STATIC[0m
[38;5;104m[X] Kernel: 3 CASK_STATIC[0m
[38;5;104m[X] Kernel: 4 CASK_STATIC[0m
[38;5;104m[X] Kernel: 5 CASK_STATIC[0m
[38;5;104m[X] Kernel: 6 CASK_STATIC[0m
[38;5;104m[X] Kernel: 7 CASK_STATIC[0m
[38;5;104m[X] Kernel: 8 CASK_STATIC[0m
[38;5;104m[X] Kernel: 9 CASK_STATIC[0m
[38;5;104m[X] Kernel: 10 CASK_STATIC[0m
[38;5;104m[X] Kernel: 11 CASK_STATIC[0m
[38;5;104m[X] Kernel: 12 CASK_STATIC[0m
[38;5;104m[X] Kernel: 13 CASK_STATIC[0m
[38;5;104m[X] Kernel: 14 CASK_STATIC[0m
[38;5;104m[X] Kernel: 15 CASK_STATIC[0m
[38;5;104m[X] Kernel: 16 CASK_STATIC[0m
[38;5;104m[X] Kernel: 17 CASK_STATIC[0m
[38;5;104m[X] Kernel: 18 CASK_STATIC[0m
[38;5;104m[X] Kernel: 19 CASK_STATIC[0m
[38;5;104m[X] Kernel: 20 CASK_STATIC[0m
[38;5;104m[X] Kernel: 21 CASK_STATIC[0m
[38;5;104m[X] Kernel: 22 CASK_STATIC[0m
[38;5;104m[X] Kernel: 23 TRT_SERIALIZABLE:generatedNativePointwise[0m
[38;5;104m[X] Kernel: 24 TRT_SERIALIZABLE:ResizeVectorizedC4x4NearestKernel[0m
[38;5;104m[X] Kernel: 25 CASK_STATIC[0m
[38;5;104m[X] Kernel: 26 CASK_STATIC[0m
[38;5;104m[X] Kernel: 27 TRT_SERIALIZABLE:generatedNativePointwise[0m
[38;5;104m[X] Kernel: 28 CASK_STATIC[0m
[38;5;104m[X] Kernel: 29 CASK_STATIC[0m
[38;5;104m[X] Kernel: 30 CASK_STATIC[0m
[38;5;104m[X] Kernel: 31 CASK_STATIC[0m
[38;5;104m[X] Kernel: 32 TRT_SERIALIZABLE:generatedNativePointwise[0m
[38;5;104m[X] Kernel: 33 CASK_STATIC[0m
[38;5;104m[X] Kernel: 34 CASK_STATIC[0m
[38;5;104m[X] Kernel: 35 CASK_STATIC[0m
[38;5;104m[X] Kernel: 36 CASK_STATIC[0m
[38;5;104m[X] Kernel: 37 CASK_STATIC[0m
[38;5;104m[X] Kernel: 38 CASK_STATIC[0m
[38;5;13m[V] Compiler backend is used during engine execution.[0m
[38;5;104m[X] Disabling unused tactic source: JIT_CONVOLUTIONS[0m
[38;5;13m[V] Engine generation completed in 26.9119 seconds.[0m
[38;5;104m[X] Layers:
    Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: images, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 864}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8, TacticValue: 0x5cc792a989a1d1a6, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_1/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 9216}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3, TacticValue: 0x13463e9bf9ae0d73, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_2/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 18432}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_3/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear]
    Name: /model/backbone/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/MaxPool]
    Name: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xb936321f82fd390c, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 4096}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x0e07dc8353bf7e9f, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 73728}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x705baf38e41eee0b, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0x23b890da05937b9e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 8192}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x214f03e23f252333, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0xa8b56a226b057463, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 294912}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 1179648}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32, TacticValue: 0x322f337abc345152, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x65fbe45b4cb1d8a5, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x1d53511430a5d47e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/act/Relu]
    Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye9020_0_myl37_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddResTra_myl37_1, LayerType: kgen, Inputs: [ { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_MulAddResTra_0x862813689358e08ec79eab32f31fafdf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/encoder/Reshape][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1]
    Name: __mye8937_myl37_2, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_TraAdd_myl37_3, LayerType: kgen, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Constant_output_0_constantFloat, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_TraAdd_0x5a9388c92c5b2a167638420a28fa3cf0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Transpose][ONNX Layer: /model/encoder/encoder.0/layers.0/Add]
    Name: __mye8939_myl37_4, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_2_myl37_5, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8387_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8277/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8278/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8626_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2]
    Name: __mye8941_myl37_6, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_1+/model/encoder/encoder_0/layers_0/self_attn/MatMul_myl37_7, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8769_dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye8319/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8320/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8774_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye8684, Dimensions: [2,400,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add]
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_3_myl37_8, LayerType: gemm, Inputs: [ { Name: __mye8684, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8684, Dimensions: [8,32,400], Format/Datatype: Float }, { Name: __mye8642, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye8333/model/encoder/encoder_0/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x128x16_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl37_9, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x486901888507314d28178a529899ff30, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax]
    Name: __mye8943_myl37_10, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_myl37_11, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8343/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8344/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl37_12, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0x053154cc4b930530fcf23b0caf04c63a, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3]
    Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_myl37_13, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8849dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8357/model/encoder/encoder_0/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8358/model/encoder/encoder_0/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_116_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl37_14, LayerType: kgen, Inputs: [ { Name: __mye8585_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8575_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye9016_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x81c6f38dc18b20647aef42cb9b16a94b, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/encoder/encoder_0/layers_0/linear1/MatMul_myl37_15, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Int8 }, { Name: __mye8854dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye8646_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye8653zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear1_bias _ ONNXTRT_Broadcast_131_constantFloat, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_gelu_erf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Div][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Erf][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl37_16, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }, { Name: __mye8859dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye8657_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8664zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear2_bias _ ONNXTRT_Broadcast_145_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/Add_2]
    Name: __myl_ResMeaSubMulMea_myl37_17, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMea_0xade3a566ff3432c2f2753f66a7f593a6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization]
    Name: __myl_AddSqrDivMulMulAddResTra_myl37_18, LayerType: kgen, Inputs: [ { Name: __mye8539_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8549_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Reshape_1_output_0, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_AddSqrDivMulMulAddResTra_0x0caebe133d43683f5670c896620d9227, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization][ONNX Layer: /model/encoder/Transpose_1]
    [ONNX Layer: /model/encoder/Reshape_1]
    Name: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x4f35593c356e2e7e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x483ad1560c6e5e27, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Reshape_1_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: Reformatted Output Tensor 0 to PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul), Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.0/act/Mul]
    Name: Reformatting CopyNode for Output Tensor 0 to PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul), LayerType: NoOp, Inputs: [ { Name: Reformatted Output Tensor 0 to PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul), Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], TacticValue: 0x0000000000000000, StreamId: 0, Metadata: 
    Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    Name: /model/encoder/Resize, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize]
    Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_2]
    Name: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x709ddd0e503c7fd7, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/Resize_1, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize_1]
    Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_3]
    Name: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x960e9baa2a6cad5b, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xbd976ef514eaa406, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xbd976ef514eaa406, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000019, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x65a38dbc9e991257, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/Resize_1_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_4]
    Name: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x709ddd0e503c7fd7, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/Add]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xd14bd6d95fefd45e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/Add]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish, TacticValue: 0x44824770683c7b80, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye158089_0_myl85_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^signal^1_myl85_1, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^wait^1_myl85_2, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl85_3, LayerType: kgen, Inputs: [ { Name: __mye155645_dconst, Dimensions: [1,1,6400], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xb7911a963641d99b9b7644b75b6b02a0, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulMinMaxRouCasResTra_myl85_4, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: __mye157989_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x53ec280dcdcbc7be42089db5a99e26ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl85_5, LayerType: kgen, Inputs: [ { Name: __mye155668_dconst, Dimensions: [1,1,1600], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xc7826108fa2ff5e34bf8bfa07dbc52f7, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/input_proj.1/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __myl_MulMinMaxRouCasResTra_myl85_6, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: __mye157989_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x8592f20b4eb6c9ee9a9e56f44ec5871e, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __mye157455_myl85_7, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __mye157457_myl85_8, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_myl85_9, LayerType: kgen, Inputs: [ { Name: __mye157989_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __mye155691_dconst, Dimensions: [1,1,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __mye154083_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_0x14d97ab92d57b85a1bd3815e99f6e152, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Concat_3][ONNX Layer: /model/decoder/Reshape_2]
    [ONNX Layer: /model/decoder/Transpose_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear]
    Name: __mye157459_myl85_10, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157461_myl85_11, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_1/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_0/cross_attn/value_proj/MatMul_myl85_12, LayerType: gemm, Inputs: [ { Name: __mye154083_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156340dconst, Dimensions: [3,256,256], Format/Datatype: Int8 }, { Name: __mye154107_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye154128_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye155126_dconst, Dimensions: [3,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye154083, Dimensions: [3,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add]
    Name: /model/decoder/enc_output/proj/MatMul_myl85_13, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156345dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153194_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153201zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_proj_bias _ ONNXTRT_Broadcast_275_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/proj/MatMul][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/Add]
    Name: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl85_14, LayerType: kgen, Inputs: [ { Name: model_decoder_enc_output_norm_weight _ ONNXTRT_Broadcast_279_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_norm_bias _ ONNXTRT_Broadcast_281_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: __mye157999_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0xf1c80ff651c1b506b1815818d6281ad3, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/norm/LayerNormalization][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear]
    Name: __mye157463_myl85_15, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157465_myl85_16, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_score_head/MatMul_myl85_17, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156355dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153232_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153239zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_enc_score_head_bias _ ONNXTRT_Broadcast_289_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/enc_score_head/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/Add]
    Name: __myl_Max_myl85_18, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400,1], Format/Datatype: Float }], TacticName: __myl_Max_0x4330a02939b906fc5f8c1bd769456467, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/ReduceMax]
    Name: __myl_Top_myl85_19, LayerType: kgen, Inputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/TopK_output_0'.1, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x7e62297dffa2e596ee60049838a70f81, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/TopK]
    Name: __mye157467_myl85_20, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_bbox_head/layers_0/MatMul_myl85_21, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157529_xformed___mye156350dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157537_xformed___mye153216_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153212zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157533_xformed___mye153225_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.0/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/Add]
    Name: /model/decoder/enc_bbox_head/layers_1/MatMul_myl85_22, LayerType: gemm, Inputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157541_xformed___mye156360dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157549_xformed___mye153254_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153250zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157545_xformed___mye153263_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.1/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act_1/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/Add]
    Name: __myl_FcAdd_myl85_23, LayerType: fusion, Inputs: [ { Name: model_decoder_anchors_constantFloat, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156365dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153270_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153277zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_enc_bbox_head_layers_2_bias _ ONNXTRT_Broadcast_311_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.2/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/Add][ONNX Layer: /model/decoder/Add]
    Name: __mye157469_myl85_24, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_myl85_25, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __mye158003_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], Outputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,4], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], TacticName: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_0xea994e8a02766a6b87cc77a0ab1bb663, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/Unsqueeze][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Sigmoid][ONNX Layer: /model/decoder/GatherElements]
    Name: __myl_MovCon_myl85_26, LayerType: kgen, Inputs: [ { Name: __mye156623, Dimensions: [1,300,12], Format/Datatype: Int8 }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,4], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MovCon_0x9482c2d60923b5d68d1030431d0b6d2e, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_0/MatMul_myl85_27, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156637_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153292_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153288zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153301_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1/MatMul_myl85_28, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156375dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153308_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153315zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye149975_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/Add]
    Name: __myl_RepGatResAdd_myl85_29, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], Outputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_RepGatResAdd_0x3585782c9d9cf8f0d2b18744e46affde, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/GatherElements_1][ONNX Layer: /model/decoder/decoder/layers.0/Add]
    Name: __mye157471_myl85_30, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157473_myl85_31, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_2_myl85_32, LayerType: gemm, Inputs: [ { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye149953_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149287/model/decoder/decoder/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149288/model/decoder/decoder/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye153089_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_2]
    Name: __mye157475_myl85_33, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_1+/model/decoder/decoder/layers_0/self_attn/MatMul_myl85_34, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156380dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149341/model/decoder/decoder/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149342/model/decoder/decoder/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155376_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye154068, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add]
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_3_myl85_35, LayerType: gemm, Inputs: [ { Name: __mye154068, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye154068, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153149, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149376/model/decoder/decoder/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl85_36, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Softmax]
    Name: __mye157477_myl85_37, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_myl85_38, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149386/model/decoder/decoder/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149387/model/decoder/decoder/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl85_39, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_0/self_attn/Gemm_myl85_40, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye149991_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149400/model/decoder/decoder/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149401/model/decoder/decoder/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_351_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl85_41, LayerType: kgen, Inputs: [ { Name: __mye152985_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152975_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158007_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x91b2c7046943674462a660380f1917c4, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/Add_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __myl_FcMulAdd_myl85_42, LayerType: fusion, Inputs: [ { Name: __mye154035_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: __mye154017_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156385dconst, Dimensions: [1,256,288], Format/Datatype: Int8 }, { Name: __mye153994_dconst, Dimensions: [1,1,288], Format/Datatype: Float }], Outputs: [ { Name: __mye154044mul_beta, Dimensions: [1,300,288], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add]
    Name: __mye157479_myl85_43, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157481_myl85_44, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_ResMaxSubExpSum_myl85_45, LayerType: kgen, Inputs: [ { Name: __mye154044mul_beta, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_44, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_43, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_ResMaxSubExpSum_0xa182a58986fd27ac679c4af0e2d9234d, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax]
    Name: __mye157483_myl85_46, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_myl85_47, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye154083, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18670, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18685, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18700, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18715, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18446, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18461, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18476, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18491, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18222, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18237, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18252, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18267, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150485_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150475_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150465_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149996_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }, { Name: __mye150575, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150579, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye154044mul_beta, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_48, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_47, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_46, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_0xcf8d1c939cf66a04d87f9ca7514807d6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8]
    Name: __mye157485_myl85_48, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl85_49, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_43, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye158021_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_47, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_46, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_48, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_44, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150954_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl85_50, LayerType: kgen, Inputs: [ { Name: __mye150954_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_50, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl85_51, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_50, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155546_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153341_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153348zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_577_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_51, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl85_52, LayerType: kgen, Inputs: [ { Name: __mye152940_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152930_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158025_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_51, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_53, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_0/linear1/MatMul_myl85_53, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157553_xformed___mye156390dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153363_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153359zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153372_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_54, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl85_54, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_53, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_54, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156395dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153379_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153386zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_linear2_bias _ ONNXTRT_Broadcast_599_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_55, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl85_55, LayerType: kgen, Inputs: [ { Name: __mye152904_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152890_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158029_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_55, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157487_myl85_56, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157489_myl85_57, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_2_myl85_58, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150056_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149485/model/decoder/decoder/layers_1/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149486/model/decoder/decoder/layers_1/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152875_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_2]
    Name: __mye157491_myl85_59, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/MatMul_myl85_60, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157557_xformed___mye156400dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153401_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153397zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153410_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_59, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_1/MatMul_myl85_61, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_59, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157561_xformed___mye156405dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153428_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153424zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153437_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_60, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/MatMul_myl85_62, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_60, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156410dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153444_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153451zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_0_layers_2_bias _ ONNXTRT_Broadcast_629_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add]
    Name: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_myl85_63, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156644, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye158003_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_63, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_0xefac8e563c6580f9cd110df4750663ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log][ONNX Layer: /model/decoder/decoder/Sigmoid_1][ONNX Layer: /model/decoder/decoder/Add][ONNX Layer: /model/decoder/decoder/Div][ONNX Layer: /model/decoder/decoder/Sub][ONNX Layer: /model/decoder/decoder/Clip]
    Name: /model/decoder/decoder/query_pos_head/layers_0_1/MatMul_myl85_64, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156660_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153466_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153462zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153475_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_64, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_1/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_1/MatMul_myl85_65, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_64, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156420dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153482_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153489zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150074_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add]
    Name: __myl_Add_myl85_66, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_1+/model/decoder/decoder/layers_1/self_attn/MatMul_myl85_67, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156425dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149550/model/decoder/decoder/layers_1/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149551/model/decoder/decoder/layers_1/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155386_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153981, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_3_myl85_68, LayerType: gemm, Inputs: [ { Name: __mye153981, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153981, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153153, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149585/model/decoder/decoder/layers_1/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_68, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl85_69, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_68, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_68, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_69, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Softmax]
    Name: __mye157493_myl85_70, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_myl85_71, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_69, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149595/model/decoder/decoder/layers_1/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149596/model/decoder/decoder/layers_1/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_70, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4]
    Name: __myl_Tra_myl85_72, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_70, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_1/self_attn/Gemm_myl85_73, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150090_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149609/model/decoder/decoder/layers_1/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149610/model/decoder/decoder/layers_1/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_self_attn_out_proj_bias _ ONNXTRT_Broadcast_674_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl85_74, LayerType: kgen, Inputs: [ { Name: __mye152825_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152815_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158036_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_74, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __myl_FcMulAdd_myl85_75, LayerType: fusion, Inputs: [ { Name: __mye153948_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: __mye153930_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156430dconst, Dimensions: [1,256,288], Format/Datatype: Int8 }, { Name: __mye153907_dconst, Dimensions: [1,1,288], Format/Datatype: Float }], Outputs: [ { Name: __mye153957mul_beta, Dimensions: [1,300,288], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add]
    Name: __mye157495_myl85_76, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157497_myl85_77, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_ResMaxSubExpSum_myl85_78, LayerType: kgen, Inputs: [ { Name: __mye153957mul_beta, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_78, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_77, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_ResMaxSubExpSum_0xa182a58986fd27ac679c4af0e2d9234d, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax]
    Name: __mye157499_myl85_79, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_myl85_80, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_63, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye154083, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19369, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19384, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19399, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19414, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19145, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19160, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19175, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19190, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18921, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18936, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18951, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18966, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150515_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150505_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150495_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150095_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }, { Name: __mye150603, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150607, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye153957mul_beta, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_82, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_81, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_80, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_0x5f2d42de2a7117676a3ba0e68cc7f7ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8]
    Name: __mye157501_myl85_81, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl85_82, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_77, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye158049_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_81, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_80, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_82, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_78, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150960_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl85_83, LayerType: kgen, Inputs: [ { Name: __mye150960_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_84, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl85_84, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_74, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_84, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155492_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153515_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153522zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_900_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_85, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl85_85, LayerType: kgen, Inputs: [ { Name: __mye152780_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152770_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158053_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_85, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_87, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_1/linear1/MatMul_myl85_86, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157565_xformed___mye156435dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153537_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153533zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153546_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_88, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.1/linear1/Add]
    Name: __myl_FcAdd_myl85_87, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_87, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_88, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156440dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153553_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153560zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_linear2_bias _ ONNXTRT_Broadcast_922_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_89, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl85_88, LayerType: kgen, Inputs: [ { Name: __mye152744_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152730_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158057_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_89, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157503_myl85_89, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157505_myl85_90, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_2_myl85_91, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150151_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149694/model/decoder/decoder/layers_2/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149695/model/decoder/decoder/layers_2/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152715_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_2]
    Name: __mye157507_myl85_92, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/MatMul_myl85_93, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157569_xformed___mye156445dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153575_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153571zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153584_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_93, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_1/MatMul_myl85_94, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_93, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157573_xformed___mye156450dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153602_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153598zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153611_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_94, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/MatMul_myl85_95, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_94, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156455dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153618_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153625zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_1_layers_2_bias _ ONNXTRT_Broadcast_952_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_95, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add]
    Name: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_myl85_96, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_63, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_95, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156667, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye158003_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_97, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_96, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_0xa06819df43d11e9f71ec4d6314dfc9b2, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log_1][ONNX Layer: /model/decoder/decoder/Add_1][ONNX Layer: /model/decoder/decoder/Sigmoid_2][ONNX Layer: /model/decoder/decoder/Div_1][ONNX Layer: /model/decoder/decoder/Sub_1][ONNX Layer: /model/decoder/decoder/Clip_3]
    Name: /model/decoder/decoder/query_pos_head/layers_0_2/MatMul_myl85_97, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_96, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156683_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153640_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153636zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153649_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_98, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_2/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_2/MatMul_myl85_98, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_98, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156465dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153656_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153663zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150169_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add]
    Name: __myl_Add_myl85_99, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_1+/model/decoder/decoder/layers_2/self_attn/MatMul_myl85_100, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156470dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149759/model/decoder/decoder/layers_2/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149760/model/decoder/decoder/layers_2/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155396_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153894, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_3_myl85_101, LayerType: gemm, Inputs: [ { Name: __mye153894, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153894, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153157, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149794/model/decoder/decoder/layers_2/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_102, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl85_102, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_102, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_102, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_103, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Softmax]
    Name: __mye157509_myl85_103, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_myl85_104, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_103, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149804/model/decoder/decoder/layers_2/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149805/model/decoder/decoder/layers_2/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_104, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4]
    Name: __myl_Tra_myl85_105, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_104, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_2/self_attn/Gemm_myl85_106, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150185_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149818/model/decoder/decoder/layers_2/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149819/model/decoder/decoder/layers_2/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_self_attn_out_proj_bias _ ONNXTRT_Broadcast_997_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl85_107, LayerType: kgen, Inputs: [ { Name: __mye152665_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152655_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158064_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_108, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __myl_FcMulAdd_myl85_108, LayerType: fusion, Inputs: [ { Name: __mye153861_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: __mye153843_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156475dconst, Dimensions: [1,256,288], Format/Datatype: Int8 }, { Name: __mye153820_dconst, Dimensions: [1,1,288], Format/Datatype: Float }], Outputs: [ { Name: __mye153870mul_beta, Dimensions: [1,300,288], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add]
    Name: __mye157511_myl85_109, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157513_myl85_110, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_ResMaxSubExpSum_myl85_111, LayerType: kgen, Inputs: [ { Name: __mye153870mul_beta, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_112, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_111, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_ResMaxSubExpSum_0xa182a58986fd27ac679c4af0e2d9234d, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax]
    Name: __mye157515_myl85_112, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_myl85_113, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_97, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye154083, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20068, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20083, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20098, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20113, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19844, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19859, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19874, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19889, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19620, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19635, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19650, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19665, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150545_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150535_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150525_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158011_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150190_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }, { Name: __mye150631, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150635, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye153870mul_beta, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_116, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_115, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_114, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_0x5f2d42de2a7117676a3ba0e68cc7f7ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8]
    Name: __mye157517_myl85_114, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl85_115, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_111, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye158077_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_115, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_114, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_116, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_112, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150966_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl85_116, LayerType: kgen, Inputs: [ { Name: __mye150966_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_118, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl85_117, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_108, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_118, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155438_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153689_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153696zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_1223_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_119, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl85_118, LayerType: kgen, Inputs: [ { Name: __mye152620_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152610_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158081_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_119, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_121, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_2/linear1/MatMul_myl85_119, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157577_xformed___mye156480dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153711_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153707zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153720_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_122, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.2/linear1/Add]
    Name: __myl_FcAdd_myl85_120, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_121, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_122, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156485dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153727_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153734zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_linear2_bias _ ONNXTRT_Broadcast_1245_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_123, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl85_121, LayerType: kgen, Inputs: [ { Name: __mye152584_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158085_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152578_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_123, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x3f53c92c8e85fb99f9934c06da28da1c, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157519_myl85_122, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157521_myl85_123, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_score_head_2/MatMul_myl85_124, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156490dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153738_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153745zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_dec_score_head_2_bias _ ONNXTRT_Broadcast_1293_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,300,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/dec_score_head.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/Add]
    Name: __myl_GatResNegExpAddDivRes_myl85_125, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,1,300,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_126, Dimensions: [1,24000], Format/Datatype: Float }], TacticName: __myl_GatResNegExpAddDivRes_0x1d563258c32f843400fb4233ccab3fa6, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/Sigmoid][ONNX Layer: /postprocessor/Flatten][ONNX Layer: /model/decoder/Gather_8]
    Name: __myl_Top_myl85_126, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_126, Dimensions: [1,24000], Format/Datatype: Float }], Outputs: [ { Name: scores, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_128, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x1c85ccd1fad109f046189f0d3e8dff44, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/TopK]
    Name: __mye157523_myl85_127, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/MatMul_myl85_128, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157581_xformed___mye156495dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153760_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153756zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153769_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_129, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_1/MatMul_myl85_129, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_129, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157585_xformed___mye156500dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153787_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153783zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153796_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_130, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/MatMul_myl85_130, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_130, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156505dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153803_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153810zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_2_layers_2_bias _ ONNXTRT_Broadcast_1275_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_131, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add]
    Name: __mye157525_myl85_131, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_myl85_132, LayerType: kgen, Inputs: [ { Name: orig_target_sizes, Dimensions: [1,2], Format/Datatype: Int64 }, { Name: __myln_k_arg__bb1_128, Dimensions: [1,300], Format/Datatype: Int32 }, { Name: __mye150647, Dimensions: [1,1], Format/Datatype: Int64 }, { Name: __mye150651, Dimensions: [1,1], Format/Datatype: Float }, { Name: __mye150655, Dimensions: [1,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_131, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_97, Dimensions: [1,300,4], Format/Datatype: Float }], Outputs: [ { Name: labels, Dimensions: [1,300], Format/Datatype: Int64 }, { Name: boxes, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_0x046287ea34a14bdbbd780dcf069cdb4a, StreamId: 0, Metadata: [ONNX Layer: Cast_3039][ONNX Layer: /model/decoder/decoder/Clip_6][ONNX Layer: /model/decoder/decoder/Sub_2][ONNX Layer: /model/decoder/decoder/Div_2][ONNX Layer: /model/decoder/decoder/Sigmoid_3][ONNX Layer: /model/decoder/Gather_9][ONNX Layer: /model/decoder/decoder/Unsqueeze_3][ONNX Layer: /model/decoder/decoder/Add_2][ONNX Layer: /model/decoder/decoder/Log_2][ONNX Layer: /postprocessor/Split][ONNX Layer: /postprocessor/Squeeze_1][ONNX Layer: /postprocessor/Squeeze_2][ONNX Layer: /postprocessor/Mul][ONNX Layer: /postprocessor/Add][ONNX Layer: /postprocessor/Unsqueeze_2][ONNX Layer: /postprocessor/Sub][ONNX Layer: /postprocessor/Unsqueeze][ONNX Layer: /postprocessor/Concat][ONNX Layer: /postprocessor/Mul_2][ONNX Layer: /postprocessor/GatherElements][ONNX Layer: /postprocessor/Unsqueeze_5][ONNX Layer: /postprocessor/Unsqueeze_3][ONNX Layer: /postprocessor/Add_1][ONNX Layer: /postprocessor/Squeeze][ONNX Layer: /postprocessor/Unsqueeze_1][ONNX Layer: /postprocessor/Sub_1][ONNX Layer: /postprocessor/Mul_1][ONNX Layer: /postprocessor/Squeeze_3][ONNX Layer: /postprocessor/Mul_3][ONNX Layer: /postprocessor/Sub_2][ONNX Layer: /postprocessor/Div][ONNX Layer: /postprocessor/Unsqueeze_4][ONNX Layer: /postprocessor/Tile]
    
    Bindings:
    images
    orig_target_sizes
    /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0
    /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0
    /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0
    /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0
    /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0
    /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0
    labels
    boxes
    scores[0m
[38;5;13m[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 83 MiB[0m
[38;5;104m[X] Adding 1 engine(s) to plan file.[0m
[38;5;104m[X] Adding 1 engine weights(s) to plan file.[0m
[38;5;10m[I] Finished engine building in 27.068 seconds[0m
[38;5;13m[V] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Clip_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CoordConvAC version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResizeDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResize version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DetectionLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::FlattenConcat_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GenerateDetection_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchor_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 3[0m
[38;5;104m[X] Plugin creator already registered - ::LReLU_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Normalize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PillarScatterPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PriorBox_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Proposal version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Region_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ResizeNearest_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::RPROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 2[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterND version 1[0m
[38;5;104m[X] Plugin creator already registered - ::SpecialSlice_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Split version 1[0m
[38;5;104m[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1[0m
[38;5;13m[V] Loaded engine size: 29 MiB[0m
[38;5;104m[X] Deserialization required 6865 microseconds.[0m
[38;5;104m[X] Adding 1 engine(s) to plan file.[0m
[38;5;104m[X] Adding 1 engine weights(s) to plan file.[0m
[I] Saving engine to default_mtq_int8_q_qint8mark_outputs_of_fused_nodes-output_modified.engine
[38;5;13m[V] [MS] Running engine with multi stream info[0m
[38;5;13m[V] [MS] Number of aux streams is 1[0m
[38;5;13m[V] [MS] Number of total worker streams is 2[0m
[38;5;13m[V] [MS] The main stream provided by execute/enqueue calls is the first worker stream[0m
[38;5;104m[X] Total per-runner device persistent memory is 0[0m
[38;5;104m[X] Total per-runner host persistent memory is 307312[0m
[38;5;104m[X] Allocated device scratch memory of size 63129600[0m
[38;5;104m[X] - Runner scratch: 63129600 bytes[0m
[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +61, now: CPU 0, GPU 83 (MiB)[0m
[38;5;104m[X] CUDA lazy loading is enabled.[0m
[38;5;13m[V] Loading inputs from data loader[0m
[38;5;11m[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. [0m
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] trt-runner-N1-05/19/25-15:35:51    
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[38;5;104m[X] trt-runner-N1-05/19/25-15:35:51     | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}[0m
[38;5;13m[V] trt-runner-N1-05/19/25-15:35:51     | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: labels to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: boxes to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: scores to: tensor([67,  0,  0,  ...,  0,  0,  0], device='cuda:0', dtype=torch.uint8)[0m
[I] trt-runner-N1-05/19/25-15:35:51    
    ---- Inference Output(s) ----
    {/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[38;5;104m[X] trt-runner-N1-05/19/25-15:35:51     | Inference Time: 8.505 ms | Received outputs:
        {'/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0': tensor([[[[[-0.8963,  0.6956],
                   [-0.5254,  1.7670],
                   [-2.2810, -0.8121],
                   ...,
                   [-0.5561,  1.1478],
                   [-1.9939, -0.5075],
                   [-1.2878, -0.1165]],
        
                  [[ 1.1380,  0.2296],
                   [ 2.2390, -2.0784],
                   [ 0.2305, -0.9704],
                   ...,
                   [-0.4895,  1.2591],
                   [-0.4545,  0.5306],
                   [ 0.2747,  0.5528]],
        
                  [[-0.3245, -1.8669],
                   [ 0.9353,  1.3311],
                   [-1.5568,  0.8156],
                   ...,
                   [-2.1488,  0.2939],
                   [-1.2080,  0.5707],
                   [-2.2873, -1.1484]],
        
                  ...,
        
                  [[-0.9334,  2.4490],
                   [ 0.2514,  0.9833],
                   [ 1.7204,  0.7797],
                   ...,
                   [ 1.3627,  2.8123],
                   [ 1.1151,  0.9394],
                   [ 3.7173,  4.5882]],
        
                  [[-0.0584, -1.4051],
                   [-0.2981, -1.3028],
                   [ 0.2791, -0.6878],
                   ...,
                   [-2.6416,  0.3198],
                   [-0.8056,  0.6278],
                   [-1.3508,  2.2057]],
        
                  [[ 0.6033,  0.6719],
                   [ 0.3441,  0.2035],
                   [-2.2509,  1.0055],
                   ...,
                   [-2.4730,  1.3631],
                   [-2.4026,  1.5228],
                   [-1.1939, -1.7908]]],
        
        
                 [[[ 0.5062,  0.2299],
                   [ 0.1407,  1.0714],
                   [-0.6340, -0.9786],
                   ...,
                   [-1.4298, -0.0271],
                   [-0.8640,  0.9425],
                   [-1.8250,  0.1531]],
        
                  [[-0.8078,  1.2934],
                   [ 1.6840, -0.5434],
                   [ 0.8882,  0.2055],
                   ...,
                   [ 1.0481,  0.8013],
                   [ 0.1357, -0.9027],
                   [-0.4644, -0.8551]],
        
                  [[-1.2332,  0.7336],
                   [ 2.9835,  1.7879],
                   [-0.4999,  0.7741],
                   ...,
                   [ 0.0508,  0.7772],
                   [ 2.2293,  0.3225],
                   [-0.9709, -1.1172]],
        
                  ...,
        
                  [[-1.8310,  0.5369],
                   [-1.3692, -0.0932],
                   [-0.3601, -0.0249],
                   ...,
                   [ 0.9962,  0.6297],
                   [ 1.5127,  0.6330],
                   [ 2.4589,  2.4193]],
        
                  [[ 1.0974, -2.2721],
                   [-1.9154, -1.6030],
                   [ 2.9465, -0.6527],
                   ...,
                   [ 1.8096, -0.0992],
                   [-1.8626,  0.4584],
                   [ 0.6462,  1.1398]],
        
                  [[ 2.2943,  0.7295],
                   [ 1.3767, -0.0171],
                   [ 0.0372, -0.0329],
                   ...,
                   [-3.8405, -0.2387],
                   [-1.6198,  1.6388],
                   [-2.1757,  1.9075]]],
        
        
                 [[[-0.6247,  0.4902],
                   [-0.4354,  1.3849],
                   [-1.4593, -0.2821],
                   ...,
                   [-1.2127,  1.1081],
                   [-2.4200, -0.6120],
                   [-2.2411,  0.6365]],
        
                  [[-1.1016,  0.6000],
                   [ 1.6440, -1.1235],
                   [ 0.2987, -1.1265],
                   ...,
                   [-0.0757,  2.0628],
                   [-1.1051,  1.6108],
                   [-0.0464,  0.5253]],
        
                  [[-0.3830, -1.9670],
                   [ 1.7756,  1.5042],
                   [-1.8388,  0.5928],
                   ...,
                   [ 0.2685,  0.1686],
                   [ 1.2497,  0.4615],
                   [-0.4853, -1.9393]],
        
                  ...,
        
                  [[-1.4205,  1.2089],
                   [-0.3694,  0.7802],
                   [ 0.9443,  1.2241],
                   ...,
                   [ 1.6384,  3.0304],
                   [ 2.0510,  0.8437],
                   [ 3.4807,  4.8754]],
        
                  [[ 0.5924, -1.4850],
                   [-1.8234, -1.1895],
                   [ 1.8095, -0.5087],
                   ...,
                   [-0.5076,  0.6069],
                   [ 0.1318,  0.8999],
                   [-0.0156,  2.6608]],
        
                  [[ 1.5477,  0.8619],
                   [ 0.5069,  0.5835],
                   [-1.9587,  0.9232],
                   ...,
                   [-4.6504,  2.0553],
                   [-3.4827,  1.5824],
                   [ 1.4645, -0.4877]]],
        
        
                 ...,
        
        
                 [[[-0.4373, -0.0090],
                   [-0.5673,  0.7193],
                   [-1.6367, -0.9151],
                   ...,
                   [-1.0977,  0.2490],
                   [-2.5632, -0.6190],
                   [-1.7503,  0.0647]],
        
                  [[-0.9137,  0.2965],
                   [ 1.7642, -2.6202],
                   [ 0.3169, -1.0002],
                   ...,
                   [ 0.8667,  0.4765],
                   [-1.1037, -1.3555],
                   [ 0.0585, -0.0127]],
        
                  [[ 0.0134, -1.7382],
                   [ 1.5523,  1.4680],
                   [-1.2470,  0.9567],
                   ...,
                   [ 0.9472,  0.3509],
                   [ 3.2681,  0.6265],
                   [ 0.1968, -1.8839]],
        
                  ...,
        
                  [[-1.4037,  0.0902],
                   [-0.1998,  0.3445],
                   [ 0.8456,  0.5882],
                   ...,
                   [ 1.5027,  1.3694],
                   [ 1.6014,  1.0654],
                   [ 2.6952,  4.5962]],
        
                  [[ 0.3869, -1.9535],
                   [-0.9418, -1.5178],
                   [ 1.3676, -0.6540],
                   ...,
                   [ 1.7794,  0.1587],
                   [ 1.0083,  0.7186],
                   [ 1.2567,  2.2312]],
        
                  [[ 1.4832, -0.0542],
                   [ 0.5949, -0.4089],
                   [-1.6888,  0.4589],
                   ...,
                   [-4.1442,  2.3390],
                   [-2.4461,  0.3484],
                   [ 2.3039, -0.4888]]],
        
        
                 [[[-0.6019,  0.2395],
                   [-0.7050,  1.0805],
                   [-1.5100, -0.6362],
                   ...,
                   [-1.4024,  0.6367],
                   [-2.0194,  0.4727],
                   [-2.4261,  0.3559]],
        
                  [[-1.5662,  0.9304],
                   [ 1.3905, -1.3952],
                   [ 0.4348, -0.4944],
                   ...,
                   [ 0.0173, -0.0810],
                   [-0.9326, -1.2737],
                   [ 0.0262, -2.0069]],
        
                  [[ 0.3701, -1.0128],
                   [ 1.4290,  1.5848],
                   [-1.5327,  0.8205],
                   ...,
                   [ 0.1115,  0.8641],
                   [ 1.1861,  1.0448],
                   [-1.1463, -1.5605]],
        
                  ...,
        
                  [[-1.4985,  1.0239],
                   [-0.4844,  0.5225],
                   [ 0.4188,  0.6887],
                   ...,
                   [ 1.4384,  2.1075],
                   [ 1.8847,  0.5106],
                   [ 2.7660,  5.1287]],
        
                  [[ 0.9351, -1.9752],
                   [-1.8701, -1.4666],
                   [ 2.4608, -0.6078],
                   ...,
                   [ 0.7129,  0.0791],
                   [-0.9006,  0.8039],
                   [-0.1013,  1.9792]],
        
                  [[ 1.6401,  0.5262],
                   [ 0.5770, -0.1464],
                   [-1.6119,  0.3069],
                   ...,
                   [-3.4328,  1.6357],
                   [-2.5117,  1.1953],
                   [-0.8849,  0.3506]]],
        
        
                 [[[-0.7232,  0.1597],
                   [-0.7235,  1.0252],
                   [-1.4858, -0.6939],
                   ...,
                   [-1.4905,  0.1686],
                   [-2.3996,  0.6086],
                   [-2.3751,  0.3796]],
        
                  [[-0.1642,  1.0471],
                   [ 1.7045, -1.8095],
                   [ 0.6780, -0.3333],
                   ...,
                   [ 1.1229,  1.0065],
                   [-0.7002, -1.4172],
                   [-0.0609,  0.2981]],
        
                  [[ 0.2231, -0.9602],
                   [ 1.7164,  1.7502],
                   [-1.4271,  1.0837],
                   ...,
                   [-0.6420,  0.6069],
                   [ 2.2157,  0.6809],
                   [-0.9345, -1.4185]],
        
                  ...,
        
                  [[-1.8405,  0.1765],
                   [-0.5435,  0.3173],
                   [ 0.0748,  0.5609],
                   ...,
                   [ 1.6039,  1.6382],
                   [ 1.9431,  0.9347],
                   [ 2.4462,  4.4355]],
        
                  [[ 0.5329, -2.3087],
                   [-2.2099, -1.6304],
                   [ 1.9305, -0.7572],
                   ...,
                   [ 1.6198,  0.1036],
                   [-1.2339,  0.8761],
                   [ 0.3268,  2.0112]],
        
                  [[ 1.8470, -0.0139],
                   [ 0.7915, -0.4434],
                   [-1.3870,  0.3097],
                   ...,
                   [-5.3775,  1.5266],
                   [-2.9289,  0.2678],
                   [ 2.4232, -0.0334]]]]]), '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0': tensor([[[[[-4.1000e-01,  5.4291e-01],
                   [-8.9084e-01, -8.7631e-01],
                   [-1.4576e+00, -5.5949e-01],
                   ...,
                   [-1.3269e+00,  4.1904e-01],
                   [-3.5505e-01,  9.1952e-02],
                   [-2.0685e+00,  6.1546e-01]],
        
                  [[ 1.4583e+00,  3.8866e-01],
                   [ 4.5072e-01, -4.4660e-02],
                   [-8.2461e-01, -1.5829e+00],
                   ...,
                   [-4.4348e-01, -5.2675e-01],
                   [-9.4553e-01, -1.3930e+00],
                   [ 2.5711e+00,  4.9091e-01]],
        
                  [[-9.9914e-01, -2.2829e+00],
                   [-1.1702e-01,  7.7657e-01],
                   [-1.0481e+00,  4.0618e-01],
                   ...,
                   [ 1.9695e+00, -9.5949e-01],
                   [-4.4220e-01,  6.2678e-02],
                   [ 1.0396e+00, -7.3743e-01]],
        
                  ...,
        
                  [[-3.1095e+00,  1.0689e+00],
                   [-1.2797e+00,  1.3279e+00],
                   [-5.7083e-01,  2.4013e+00],
                   ...,
                   [ 1.2546e-01,  1.5331e+00],
                   [ 3.2411e+00, -1.3095e+00],
                   [-8.5072e-01,  8.7033e-01]],
        
                  [[ 8.7160e-01,  3.3325e+00],
                   [ 5.3785e-01, -3.9928e-01],
                   [ 1.5748e-01,  2.2972e-01],
                   ...,
                   [-6.6033e-01,  8.8420e-01],
                   [ 1.2739e-01,  3.5190e-01],
                   [ 9.0867e-01,  1.2108e+00]],
        
                  [[ 2.4914e+00, -2.8105e-01],
                   [ 1.9938e+00,  2.3445e+00],
                   [ 1.8178e-01,  2.3944e+00],
                   ...,
                   [-9.6939e-01,  8.9174e-01],
                   [-2.5185e-01,  1.7664e+00],
                   [-2.0642e+00,  2.4055e+00]]],
        
        
                 [[[ 1.4411e+00,  1.6967e+00],
                   [ 6.1896e-02, -2.0210e+00],
                   [-1.8952e-02, -9.5530e-01],
                   ...,
                   [-5.3318e-01, -1.0509e-01],
                   [-7.1857e-01,  3.7387e-01],
                   [-2.7948e+00, -4.3092e-01]],
        
                  [[ 1.8905e+00,  1.9447e+00],
                   [ 1.3430e+00,  4.7533e-01],
                   [ 4.1237e-01, -1.4925e-01],
                   ...,
                   [-2.7362e-01, -9.7205e-02],
                   [-7.8201e-01, -1.7077e+00],
                   [ 4.8508e-01, -2.3499e+00]],
        
                  [[-5.7725e-02, -1.5910e-01],
                   [-1.9244e-01,  1.2960e+00],
                   [-2.3021e+00,  6.3858e-01],
                   ...,
                   [ 5.6322e-01,  1.3591e-01],
                   [ 1.3009e+00,  1.0240e-02],
                   [ 5.5171e-01, -4.9458e-01]],
        
                  ...,
        
                  [[-2.5632e+00,  7.2690e-01],
                   [-1.4930e+00,  7.4158e-01],
                   [-7.2220e-01,  1.4585e-01],
                   ...,
                   [-1.2015e-01,  8.3568e-01],
                   [ 1.1094e+00, -4.5749e-01],
                   [ 8.9261e-01,  1.7354e+00]],
        
                  [[-1.0211e+00,  7.7157e-01],
                   [-5.7351e-01, -1.3847e+00],
                   [-2.3750e+00, -5.0193e-01],
                   ...,
                   [ 2.0674e-02, -7.4905e-02],
                   [-7.1458e-01,  6.5182e-01],
                   [-2.8735e-01,  1.4836e+00]],
        
                  [[ 2.5216e+00, -9.1904e-01],
                   [ 1.4751e+00,  2.2861e+00],
                   [ 6.1098e-01,  1.9593e+00],
                   ...,
                   [-3.5642e+00,  7.6919e-01],
                   [-6.9139e-01,  1.0424e+00],
                   [-3.0051e+00,  2.3892e+00]]],
        
        
                 [[[ 1.3894e+00,  1.3374e+00],
                   [ 2.4294e-01, -9.6849e-01],
                   [-1.1627e-01, -1.4495e-01],
                   ...,
                   [-5.3811e-01,  7.1100e-01],
                   [-4.9139e-01,  1.3693e-01],
                   [-2.2181e+00,  1.6463e+00]],
        
                  [[ 1.8617e+00,  1.2541e+00],
                   [ 1.2918e+00,  4.4862e-01],
                   [ 1.9377e-01, -7.6989e-01],
                   ...,
                   [-1.0430e+00,  1.8235e-01],
                   [-6.2807e-01, -1.7897e+00],
                   [ 3.5988e+00,  4.6234e-01]],
        
                  [[-1.1194e+00, -1.9171e+00],
                   [ 1.3701e+00,  7.5330e-01],
                   [-6.2247e-01,  3.8233e-01],
                   ...,
                   [ 2.5157e+00, -8.8026e-01],
                   [ 9.4279e-01,  2.7495e-01],
                   [ 2.7899e+00, -1.4716e+00]],
        
                  ...,
        
                  [[-1.9003e+00,  6.0282e-01],
                   [-1.0341e+00,  1.0021e+00],
                   [-5.1549e-01,  1.8487e+00],
                   ...,
                   [ 2.8194e-01,  1.7822e+00],
                   [ 2.9258e+00, -4.3665e-01],
                   [ 5.5383e-01,  1.1580e+00]],
        
                  [[ 5.9781e-01,  3.7537e+00],
                   [ 1.6839e-01, -1.2031e-01],
                   [-1.1339e+00,  2.8633e-01],
                   ...,
                   [-4.2142e-01,  9.1032e-01],
                   [ 4.6218e-01,  5.0308e-01],
                   [ 1.2648e+00,  1.9842e+00]],
        
                  [[ 2.3752e+00, -2.3378e-01],
                   [ 1.5320e+00,  2.1800e+00],
                   [ 1.6644e-01,  1.9913e+00],
                   ...,
                   [-2.1801e+00,  1.1885e+00],
                   [ 4.8710e-01,  1.9462e+00],
                   [-1.2908e+00,  2.7482e+00]]],
        
        
                 ...,
        
        
                 [[[ 7.1743e-01,  7.6490e-01],
                   [-3.3388e-01, -9.6636e-01],
                   [-5.9342e-01, -5.6271e-01],
                   ...,
                   [-7.0013e-01,  3.1233e-01],
                   [-6.6050e-01,  6.1921e-02],
                   [-2.4289e+00,  5.6541e-01]],
        
                  [[ 1.5029e+00,  1.0971e+00],
                   [ 2.0041e-01,  1.7895e-01],
                   [-3.5534e-01, -1.0054e+00],
                   ...,
                   [-1.0861e+00,  2.7997e-01],
                   [-3.5161e-01, -9.0540e-01],
                   [ 2.9364e+00,  5.5804e-01]],
        
                  [[-4.9456e-01, -1.2597e+00],
                   [ 3.2551e-01,  8.2293e-01],
                   [-9.4610e-01,  3.5575e-01],
                   ...,
                   [ 2.3234e+00, -1.0141e+00],
                   [ 1.0625e-01,  4.1208e-03],
                   [ 9.3857e-01, -1.2206e+00]],
        
                  ...,
        
                  [[-2.0045e+00,  8.7117e-01],
                   [-1.0522e+00,  1.0606e+00],
                   [-5.4324e-01,  1.5661e+00],
                   ...,
                   [ 7.4290e-01,  2.4958e+00],
                   [ 2.3608e+00, -1.7554e+00],
                   [ 1.3917e+00,  1.0816e+00]],
        
                  [[ 2.8006e-01,  3.0795e+00],
                   [ 1.4621e-01, -4.3447e-01],
                   [-9.0300e-01,  1.1512e-01],
                   ...,
                   [-8.4938e-01,  4.9778e-01],
                   [ 9.9369e-01,  4.7247e-01],
                   [ 1.6478e+00,  1.7450e+00]],
        
                  [[ 1.4660e+00, -5.7234e-01],
                   [ 1.4205e+00,  1.6193e+00],
                   [ 2.3312e-01,  1.8282e+00],
                   ...,
                   [-1.7137e+00,  1.6693e+00],
                   [ 4.5543e-01,  2.0958e+00],
                   [-7.7052e-01,  2.2696e+00]]],
        
        
                 [[[ 9.5154e-01,  8.8726e-01],
                   [ 3.7632e-01, -1.8754e+00],
                   [ 1.9301e-02, -9.1076e-01],
                   ...,
                   [-1.0326e+00,  9.9774e-02],
                   [-6.8288e-01, -1.5472e-02],
                   [-2.9614e+00, -2.0495e-01]],
        
                  [[ 1.7498e+00,  3.4619e-01],
                   [ 4.3806e-02,  3.5311e-01],
                   [-1.2939e-01, -1.2846e+00],
                   ...,
                   [-9.9249e-01,  2.9610e-01],
                   [-6.9585e-01, -1.4173e+00],
                   [ 1.9545e+00, -4.9376e+00]],
        
                  [[-5.2770e-01, -8.8607e-01],
                   [-1.8630e-01,  1.0437e+00],
                   [-2.2865e+00,  4.4281e-01],
                   ...,
                   [ 1.0842e+00, -2.6683e-01],
                   [ 1.4026e-01,  2.5099e-01],
                   [ 3.9494e-01, -8.0844e-01]],
        
                  ...,
        
                  [[-2.5773e+00,  1.1704e+00],
                   [-1.4112e+00,  1.5511e+00],
                   [-6.4378e-01,  1.5372e+00],
                   ...,
                   [ 3.6975e-01,  1.5400e+00],
                   [ 2.7222e+00, -2.2309e+00],
                   [ 7.1439e-01,  2.5690e+00]],
        
                  [[-6.8478e-01,  2.2268e+00],
                   [ 3.0328e-01, -8.3365e-01],
                   [-1.7352e+00,  4.3218e-02],
                   ...,
                   [ 2.9562e-01,  7.6935e-01],
                   [-1.5921e-01,  5.2135e-01],
                   [ 2.4790e-02,  2.1843e+00]],
        
                  [[ 2.0907e+00, -9.2292e-01],
                   [ 1.5647e+00,  1.5578e+00],
                   [ 4.0733e-01,  1.8108e+00],
                   ...,
                   [-3.0144e+00,  1.0447e+00],
                   [ 3.2932e-01,  2.9369e+00],
                   [-2.6865e+00,  2.2651e+00]]],
        
        
                 [[[ 1.0048e+00,  2.6140e-01],
                   [ 4.1243e-02, -2.1853e+00],
                   [-2.1736e-01, -1.3037e+00],
                   ...,
                   [-9.3303e-01, -6.8403e-01],
                   [-8.5856e-01, -2.6406e-01],
                   [-3.0460e+00, -2.6512e-01]],
        
                  [[ 1.7453e+00,  9.2378e-02],
                   [ 3.4103e-01, -1.4820e-02],
                   [-2.9861e-01, -1.6882e+00],
                   ...,
                   [-9.2399e-01, -6.1788e-01],
                   [-2.8059e-01, -2.0311e+00],
                   [ 3.1984e+00, -3.2210e+00]],
        
                  [[-2.0411e-01, -8.3966e-01],
                   [ 4.5899e-02,  9.7955e-01],
                   [-1.7646e+00,  2.7994e-01],
                   ...,
                   [ 1.2991e+00, -9.1511e-01],
                   [ 2.8431e-01, -1.5035e-01],
                   [ 3.1085e-01, -1.3314e+00]],
        
                  ...,
        
                  [[-2.4523e+00,  4.0543e-01],
                   [-1.3184e+00,  6.2913e-01],
                   [-5.6906e-01,  9.3555e-01],
                   ...,
                   [ 4.9355e-01,  1.0496e+00],
                   [ 2.6181e+00, -1.8407e+00],
                   [ 1.1776e+00,  1.3994e+00]],
        
                  [[-2.5630e-01,  1.4482e+00],
                   [-6.3370e-02, -1.1115e+00],
                   [-1.6479e+00, -6.5667e-02],
                   ...,
                   [-2.1917e-01,  5.8945e-01],
                   [-2.2700e-01,  6.9218e-01],
                   [ 8.2505e-02,  1.9387e+00]],
        
                  [[ 1.9363e+00, -1.2857e+00],
                   [ 1.5205e+00,  1.3102e+00],
                   [ 4.4439e-01,  1.4756e+00],
                   ...,
                   [-2.2188e+00,  1.2232e+00],
                   [-2.1465e-01,  2.2683e+00],
                   [-2.3510e+00,  2.1080e+00]]]]]), '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0': tensor([[[[[-0.9894, -0.2692],
                   [-1.7247,  0.9899],
                   [-2.6524,  0.5370],
                   ...,
                   [-1.3473,  0.7223],
                   [-2.6798,  1.0078],
                   [-2.9530,  1.4401]],
        
                  [[-4.3413, -0.9755],
                   [ 2.0010, -0.9868],
                   [ 1.0000, -0.5505],
                   ...,
                   [-0.5894, -1.3606],
                   [-0.8849, -0.0432],
                   [-0.2690, -0.1333]],
        
                  [[-0.3596, -1.6247],
                   [ 0.8042,  1.7893],
                   [-2.2733,  0.7614],
                   ...,
                   [-0.7747, -0.9024],
                   [-0.0092,  0.8898],
                   [ 0.1571,  0.0240]],
        
                  ...,
        
                  [[-0.3756,  1.8963],
                   [ 1.1032,  0.0269],
                   [ 1.6547,  1.5179],
                   ...,
                   [ 0.7989,  2.4730],
                   [ 2.3204,  1.9589],
                   [ 3.4098,  4.0800]],
        
                  [[-1.5587,  1.9321],
                   [-0.4494, -1.0593],
                   [ 0.0831, -0.4146],
                   ...,
                   [-0.9937,  0.8494],
                   [ 0.4452,  1.2264],
                   [ 0.7474,  0.4309]],
        
                  [[-2.0312, -1.9254],
                   [-0.9933,  0.2300],
                   [-0.9370,  3.8839],
                   ...,
                   [ 1.4370,  0.2670],
                   [-2.5596,  1.6023],
                   [ 0.4383, -0.3603]]],
        
        
                 [[[-1.1359, -0.6737],
                   [-0.8053,  0.0984],
                   [-1.7118, -1.2894],
                   ...,
                   [-1.5065,  1.3997],
                   [-2.2256, -0.0315],
                   [-4.0040, -1.0464]],
        
                  [[-2.1972, -1.1741],
                   [ 1.7905, -2.4153],
                   [ 0.7729, -0.0894],
                   ...,
                   [-1.5500, -1.0308],
                   [-0.4170, -2.6773],
                   [-0.9913, -1.8639]],
        
                  [[ 0.6561, -0.9621],
                   [ 0.7648,  1.9247],
                   [-1.8447,  0.7521],
                   ...,
                   [-0.3216, -0.2735],
                   [ 1.2817,  0.4740],
                   [-1.2260, -0.1217]],
        
                  ...,
        
                  [[-1.9629,  1.2488],
                   [-0.3959, -0.8244],
                   [-0.0405,  0.5233],
                   ...,
                   [ 2.8244,  0.4619],
                   [ 2.0893,  0.8467],
                   [ 2.3521,  3.0506]],
        
                  [[ 0.9997,  1.3186],
                   [-0.3373, -1.8639],
                   [-1.5113, -0.6583],
                   ...,
                   [-1.8846, -0.3367],
                   [ 1.8740,  0.3928],
                   [ 0.0490,  0.8003]],
        
                  [[-0.0570, -2.1584],
                   [ 1.3083, -0.9917],
                   [ 0.4703,  3.1099],
                   ...,
                   [-0.4140,  0.5966],
                   [-3.2888,  0.4440],
                   [-1.7879,  0.6616]]],
        
        
                 [[[-0.9654, -0.8559],
                   [-1.7110,  0.7117],
                   [-2.4709, -0.2982],
                   ...,
                   [-0.6811,  0.4876],
                   [-2.6880,  0.2825],
                   [-3.3284,  0.3104]],
        
                  [[-1.8176, -2.3260],
                   [ 1.4766, -2.2357],
                   [ 0.6528, -0.9205],
                   ...,
                   [-0.8225, -2.1806],
                   [-0.4070, -1.1723],
                   [-0.2468, -1.1813]],
        
                  [[-0.2367, -1.4332],
                   [ 0.6522,  1.5796],
                   [-1.7070,  0.7757],
                   ...,
                   [-0.4231, -1.7490],
                   [ 0.7511,  0.7857],
                   [ 0.5978,  0.0968]],
        
                  ...,
        
                  [[-1.4400,  1.3183],
                   [ 0.9548,  0.0485],
                   [ 0.7801,  1.3503],
                   ...,
                   [ 1.9818,  1.9221],
                   [ 2.4375,  1.7016],
                   [ 3.6274,  3.6347]],
        
                  [[-0.1116,  2.2153],
                   [-0.2334, -1.0415],
                   [-0.9994, -0.4709],
                   ...,
                   [-1.5131,  0.7928],
                   [ 1.4408,  1.5519],
                   [ 1.6568,  0.0124]],
        
                  [[-1.3615, -1.0884],
                   [ 0.5364,  0.2545],
                   [-0.0725,  2.6973],
                   ...,
                   [ 0.5082,  1.3088],
                   [-1.6676,  0.8756],
                   [-0.7114,  0.1455]]],
        
        
                 ...,
        
        
                 [[[-0.8818, -0.0960],
                   [-0.8747,  0.5670],
                   [-1.6696,  0.1857],
                   ...,
                   [-0.3701, -0.0403],
                   [-1.5117,  0.3282],
                   [-1.9272,  0.2415]],
        
                  [[-2.2499, -1.0575],
                   [ 1.8250, -2.0759],
                   [ 0.8917, -0.8508],
                   ...,
                   [-1.2806, -2.5967],
                   [-0.0651, -1.4174],
                   [-0.2142, -0.7482]],
        
                  [[-0.7441, -2.0724],
                   [ 1.3219,  1.3826],
                   [-1.5862,  0.7153],
                   ...,
                   [-0.4884, -2.6549],
                   [ 0.9803,  0.4641],
                   [-0.2336,  0.3896]],
        
                  ...,
        
                  [[-0.8709,  1.0399],
                   [ 1.9517,  0.0209],
                   [ 1.4665,  1.2198],
                   ...,
                   [ 1.9930,  1.8013],
                   [ 2.0648,  1.7218],
                   [ 2.6687,  3.6762]],
        
                  [[-0.8531,  3.8407],
                   [ 0.7042, -1.2866],
                   [-0.5648, -0.4393],
                   ...,
                   [-1.3982,  0.7738],
                   [ 2.0253,  1.2224],
                   [ 1.4045,  0.0904]],
        
                  [[-1.7815, -1.2734],
                   [ 0.5024,  0.1756],
                   [-0.2386,  2.8116],
                   ...,
                   [ 0.1822,  1.8377],
                   [-1.8663,  0.4780],
                   [-1.6272, -0.0374]]],
        
        
                 [[[-1.4672, -0.6737],
                   [-1.8933,  0.3713],
                   [-1.7762, -1.5562],
                   ...,
                   [-1.6901, -0.0303],
                   [-2.3648, -0.3528],
                   [-3.5445,  0.0895]],
        
                  [[-2.7411, -1.6775],
                   [ 1.7332, -3.3365],
                   [ 0.7557, -1.0813],
                   ...,
                   [-1.3162, -1.5312],
                   [-0.4124, -2.2492],
                   [-0.5639, -2.1388]],
        
                  [[ 0.1275, -1.6322],
                   [-0.2138,  1.3663],
                   [-2.6383,  0.8165],
                   ...,
                   [-1.5467, -1.3178],
                   [ 0.4554,  1.1483],
                   [-1.4660,  0.2268]],
        
                  ...,
        
                  [[-1.6596,  1.4836],
                   [ 1.3503, -0.4808],
                   [ 0.3656,  1.2670],
                   ...,
                   [ 2.4003,  1.1807],
                   [ 2.5163,  2.0720],
                   [ 3.0175,  3.7479]],
        
                  [[ 0.3843,  2.3412],
                   [ 0.4093, -1.3635],
                   [-0.9617, -0.5336],
                   ...,
                   [-2.1564,  0.3483],
                   [ 1.6145,  1.0862],
                   [ 1.1645,  0.3047]],
        
                  [[-1.4644, -1.8922],
                   [ 0.7870, -0.5891],
                   [ 0.0940,  1.5223],
                   ...,
                   [ 0.2973,  1.0811],
                   [-2.5906,  1.5783],
                   [-1.0200, -0.1745]]],
        
        
                 [[[-1.4068, -0.9040],
                   [-1.4581,  0.1993],
                   [-1.9119, -1.0151],
                   ...,
                   [-1.5270, -0.2124],
                   [-2.6681,  0.0762],
                   [-3.6867, -0.2422]],
        
                  [[-2.0830, -1.6366],
                   [ 1.7508, -2.8248],
                   [ 0.8437, -0.5858],
                   ...,
                   [-1.8328, -2.3325],
                   [-0.3348, -2.0810],
                   [-0.4590, -1.9091]],
        
                  [[ 0.1983, -0.9624],
                   [ 0.6689,  1.5697],
                   [-1.8509,  0.7196],
                   ...,
                   [-0.7472, -1.4909],
                   [ 0.8255,  0.9009],
                   [-0.5216,  0.0455]],
        
                  ...,
        
                  [[-1.6709,  1.1101],
                   [ 1.2677, -0.6987],
                   [ 0.3476,  0.7695],
                   ...,
                   [ 2.0886,  1.0936],
                   [ 2.2319,  1.7312],
                   [ 3.1411,  3.5304]],
        
                  [[ 0.2631,  1.0733],
                   [-0.0369, -1.6083],
                   [-1.2063, -0.5292],
                   ...,
                   [-1.7021,  0.3085],
                   [ 1.3976,  1.0102],
                   [ 0.7243,  0.4843]],
        
                  [[-0.8977, -1.6888],
                   [ 0.8839, -0.8194],
                   [ 0.1010,  2.7070],
                   ...,
                   [ 0.6772,  0.6640],
                   [-2.6017,  0.6555],
                   [-1.1561,  0.2743]]]]]), '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0': tensor([[[[ 4.3236,  2.5326,  3.0488,  ..., -2.5987, -3.1446, -3.3865],
                  [ 1.9648,  2.6294,  2.7457,  ..., -5.2169, -3.1268, -2.6464],
                  [-0.9244,  3.2073,  3.2265,  ..., -2.7347, -2.7280, -2.8139],
                  ...,
                  [ 3.0503,  2.9409,  3.1409,  ...,  0.1167, -3.5133,  1.4540],
                  [ 3.5110,  3.3262,  3.7015,  ...,  0.9597, -3.9862, -3.6143],
                  [ 0.9745,  1.3039,  1.3822,  ...,  1.3722, -1.5191,  0.3810]],
        
                 [[-1.1925, -1.2902, -1.2627,  ...,  1.0624,  1.6994,  1.1015],
                  [-1.4682,  0.0154,  0.7388,  ..., -0.4538, -0.7265,  0.4474],
                  [-2.0829,  0.7056, -0.3362,  ..., -0.0821,  0.3669, -0.0268],
                  ...,
                  [-0.3010, -0.2276,  0.6248,  ...,  1.0833, -1.2217,  0.6459],
                  [ 0.1646,  0.1939, -0.2637,  ...,  0.6586, -0.0199, -0.6534],
                  [-0.1319, -0.1703, -0.4971,  ...,  0.2262, -0.8154, -0.3251]],
        
                 [[ 4.5934,  2.9118,  2.8534,  ..., -2.9664, -3.4384, -4.0827],
                  [ 1.3492,  2.4925,  3.5011,  ..., -4.5008, -3.7785, -3.1500],
                  [ 0.0293,  4.0704,  3.0167,  ..., -3.0956, -2.6808, -3.4490],
                  ...,
                  [ 3.2889,  3.0223,  4.1405,  ..., -0.0421, -4.3817,  1.0545],
                  [ 3.9738,  3.9036,  2.9366,  ...,  0.5915, -4.3035, -3.9603],
                  [ 1.2685,  1.1877,  0.8452,  ...,  1.4295, -1.9161,  0.6373]],
        
                 ...,
        
                 [[ 3.6410,  2.5292,  2.9300,  ..., -2.0254, -2.8315, -3.4858],
                  [ 1.0657,  2.2191,  2.7714,  ..., -3.3535, -2.2585, -2.5249],
                  [ 0.6921,  3.5065,  2.8404,  ..., -2.9855, -1.9670, -2.4491],
                  ...,
                  [ 2.5854,  2.5530,  3.1902,  ...,  0.1210, -3.0538,  0.9467],
                  [ 3.5709,  3.4255,  3.3935,  ...,  0.1526, -4.9277, -3.1646],
                  [ 1.1985,  1.2082,  0.9186,  ...,  1.1497, -1.1285,  0.6548]],
        
                 [[ 2.1454,  1.7551,  1.9496,  ..., -0.9502, -1.4096, -2.7781],
                  [-0.0095,  1.9691,  2.5062,  ..., -3.2052, -2.1096, -2.2110],
                  [-0.0359,  3.1445,  2.2928,  ..., -2.1928, -1.4512, -3.3483],
                  ...,
                  [ 2.7025,  2.1100,  2.6344,  ...,  0.2386, -3.0579,  1.0527],
                  [ 3.0388,  2.5879,  2.2803,  ...,  0.3101, -3.0962, -2.8020],
                  [ 1.1939,  0.8409,  0.2942,  ...,  1.1142, -1.1673,  0.3047]],
        
                 [[ 1.9377,  1.3811,  1.4906,  ..., -0.3979, -1.4020, -2.2611],
                  [-0.3909,  1.5653,  2.0373,  ..., -2.0255, -1.6803, -1.9579],
                  [-1.0944,  2.2081,  1.8901,  ..., -1.5656, -1.3942, -1.4031],
                  ...,
                  [ 2.1548,  2.0898,  2.3807,  ...,  0.1398, -2.9373,  0.8036],
                  [ 1.9967,  2.0812,  2.0704,  ..., -0.3686, -2.8233, -2.0043],
                  [ 0.9257,  0.8207,  0.3634,  ...,  0.7405, -1.0288,  0.3509]]]]), '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0': tensor([[[[ 2.8505,  2.5319,  1.7828,  ..., -3.5465, -3.9134, -3.1796],
                  [ 1.0867,  2.1627,  2.3653,  ..., -1.5276, -2.3881,  1.0103],
                  [-0.4870,  3.0856,  2.0269,  ..., -3.1275, -3.0165, -2.4815],
                  ...,
                  [ 2.3989,  1.9735,  0.6746,  ..., -0.7015,  0.5968, -3.8112],
                  [ 0.0318,  3.5804,  1.8524,  ..., -2.7330, -5.0579, -2.1875],
                  [ 2.0504,  1.6372,  1.1083,  ...,  1.3979, -2.3751,  0.8373]],
        
                 [[ 0.5450, -0.0638,  0.3062,  ..., -0.5015, -0.1555, -0.2671],
                  [-0.1001,  0.2476,  0.6433,  ...,  0.6742, -1.0469, -0.3716],
                  [-1.6823,  0.8344, -0.1887,  ...,  0.0717,  0.4737, -2.3985],
                  ...,
                  [-0.3007,  0.1658,  0.6032,  ..., -0.0091, -0.0849, -0.7431],
                  [-2.1105,  0.4123,  0.9652,  ..., -0.6153, -1.0608, -0.0346],
                  [-0.5401,  0.6499,  0.6038,  ...,  0.9831, -0.1939,  0.9343]],
        
                 [[ 1.6399,  1.2105,  1.6876,  ..., -2.8087, -2.2077, -2.5090],
                  [ 0.5881,  1.0708,  1.9812,  ..., -1.3086, -1.8085,  0.4329],
                  [ 0.2720,  3.3491,  1.4957,  ..., -4.1169, -3.4639, -4.1485],
                  ...,
                  [ 0.4745,  1.2936,  1.2173,  ...,  0.1989,  1.0462, -2.8452],
                  [-1.1150,  3.6512,  2.8438,  ..., -3.2370, -5.6156, -2.0066],
                  [ 0.5969,  0.9770,  1.4209,  ...,  1.3290, -2.5830,  0.7618]],
        
                 ...,
        
                 [[ 2.2821,  1.9142,  1.8096,  ..., -2.7127, -2.9493, -2.4888],
                  [ 1.0698,  1.3810,  2.1088,  ..., -1.2469, -2.4839,  0.8235],
                  [-0.2728,  2.9172,  1.7382,  ..., -2.8445, -3.2189, -3.5029],
                  ...,
                  [ 1.0657,  1.4179,  1.0372,  ..., -0.4221,  0.4392, -2.8707],
                  [-1.4081,  3.2979,  2.4997,  ..., -3.6586, -4.3139, -2.4868],
                  [ 0.8729,  1.2913,  1.2846,  ...,  1.3523, -2.2468,  0.9592]],
        
                 [[ 1.4818,  1.3138,  1.7631,  ..., -2.4755, -3.0149, -2.4896],
                  [ 1.0069,  1.4200,  1.8379,  ..., -0.8289, -2.6603,  0.9557],
                  [-0.9228,  2.5883,  1.4703,  ..., -1.5417, -2.4248, -3.3137],
                  ...,
                  [ 1.0543,  1.2922,  1.2358,  ..., -0.2236, -0.0745, -2.3883],
                  [-1.8182,  2.6927,  2.2032,  ..., -2.3791, -4.3191, -0.9235],
                  [ 0.5155,  1.0373,  1.2203,  ...,  1.3066, -1.2527,  0.9843]],
        
                 [[ 1.7365,  1.4002,  1.4135,  ..., -2.4729, -3.0887, -2.3449],
                  [ 1.2874,  1.5521,  1.9911,  ..., -0.6748, -3.0327,  0.5418],
                  [-0.8811,  3.0369,  1.1997,  ..., -1.9670, -2.5863, -4.4484],
                  ...,
                  [ 0.9496,  1.3478,  1.0532,  ..., -0.4053, -0.0259, -2.4859],
                  [-1.5979,  2.6902,  2.2266,  ..., -2.8879, -3.8483, -1.2672],
                  [ 0.6673,  1.2508,  1.1549,  ...,  1.4751, -1.4106,  1.1131]]]]), '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0': tensor([[[[ 3.3788,  3.7244,  2.8865,  ..., -2.2496, -2.3788, -3.1286],
                  [ 1.0388,  3.4947,  3.6028,  ...,  0.7706, -3.8803, -4.9329],
                  [-1.1363,  2.2710,  1.7579,  ...,  0.8990, -0.5885, -1.1801],
                  ...,
                  [ 1.7174,  2.5761,  2.6309,  ...,  1.5672, -2.1959,  1.0314],
                  [-0.9674,  2.8146,  1.7052,  ..., -3.0001, -2.1795,  1.4643],
                  [-0.1011,  2.4274,  1.7567,  ...,  0.6836,  0.5298, -1.3100]],
        
                 [[ 0.7888,  1.0880,  0.5639,  ...,  0.0272,  0.0865,  0.1630],
                  [-1.3210,  0.6412,  1.2499,  ...,  0.3852, -0.3434, -1.5281],
                  [-2.7426, -0.0131,  0.4652,  ...,  0.6368,  1.0935,  0.0251],
                  ...,
                  [ 0.1208, -0.0062,  0.2345,  ...,  0.5744,  0.6899,  0.8070],
                  [-4.2015, -0.0623,  0.9324,  ...,  0.2464, -0.2280,  0.2673],
                  [-1.3570,  0.4302,  0.8980,  ..., -0.4498,  0.8424,  0.1728]],
        
                 [[ 2.4377,  2.9161,  2.5890,  ..., -1.0135, -2.4433, -2.6612],
                  [ 1.7607,  2.6826,  2.3555,  ...,  1.0207, -2.2866, -4.2579],
                  [-0.1845,  2.1582,  1.8613,  ...,  0.6674, -1.3473, -1.6441],
                  ...,
                  [ 1.1852,  1.2308,  1.8709,  ...,  1.4357, -1.3805,  0.4656],
                  [-2.0372,  2.4833,  2.5886,  ..., -3.0298, -2.5573,  0.1443],
                  [-1.6591,  1.4899,  1.3982,  ..., -0.4772,  0.2095, -0.8359]],
        
                 ...,
        
                 [[ 3.1847,  3.3696,  2.8115,  ..., -2.1670, -2.7395, -3.7712],
                  [ 2.3819,  3.2216,  3.0694,  ...,  1.3129, -3.2655, -5.4138],
                  [ 1.6988,  2.8758,  1.9969,  ...,  0.4966, -2.0703, -1.0491],
                  ...,
                  [ 1.7199,  2.1566,  2.0190,  ...,  0.5649, -2.4768,  0.4353],
                  [ 0.7539,  3.1565,  2.2805,  ..., -3.2672, -2.7128, -0.2966],
                  [ 0.2904,  2.4346,  1.6013,  ..., -0.5553,  0.6964, -0.2849]],
        
                 [[ 1.9813,  2.3622,  2.1120,  ..., -0.8011, -1.2709, -2.0515],
                  [ 0.3687,  2.3818,  1.5682,  ...,  1.5205, -1.7083, -3.5243],
                  [-1.0202,  1.3162,  1.6750,  ...,  0.6267, -0.4054, -0.5447],
                  ...,
                  [ 1.0726,  1.1173,  1.4203,  ...,  1.0073, -0.7668,  0.1725],
                  [-2.3774,  1.4022,  1.7458,  ..., -0.8488, -1.0567,  0.2591],
                  [-0.9600,  1.2878,  1.0449,  ..., -0.8965,  0.8315, -0.5336]],
        
                 [[ 1.9435,  2.3294,  1.8062,  ..., -0.5852, -1.1669, -1.9550],
                  [ 0.4630,  2.1678,  2.1243,  ...,  1.1974, -1.5979, -3.6773],
                  [-1.3292,  0.8482,  1.2107,  ...,  0.7672,  0.0160, -0.3221],
                  ...,
                  [ 1.0004,  1.2194,  1.0689,  ...,  0.8813, -0.7709,  0.4332],
                  [-1.7392,  1.1247,  1.8542,  ..., -1.2447, -1.5487,  0.1355],
                  [ 0.0757,  1.3597,  1.4594,  ..., -0.7780,  0.8089, -0.5687]]]]), 'labels': tensor([[67, 67, 67, 67,  0, 67, 67, 67,  0, 67, 67, 67, 67,  0, 67,  8,  8,  8,
                 24,  0,  8,  8,  8, 67,  8,  8,  8,  8, 67,  8,  8,  8,  0,  8, 24, 67,
                 67,  0, 24,  8,  0,  0,  8,  0,  8,  8,  8,  8,  8,  0, 24,  8,  0,  8,
                  8,  8, 67,  8,  8,  8,  0,  8, 13, 67,  0, 67,  8,  8,  8,  0,  8,  8,
                  0,  8, 26,  8,  8,  8,  0,  0, 67,  8,  8,  0,  8,  8,  8, 13,  0, 67,
                  8,  8,  8,  0, 24, 67,  8,  8,  0,  2,  0, 24,  8,  8,  0,  8,  0,  0,
                  0,  0,  8,  0, 67,  8,  8, 24,  8,  0,  0,  8, 24, 26, 24,  8,  0,  8,
                  8,  8,  8,  8,  8, 67,  8,  0,  8, 58,  8,  8,  8, 13,  8,  8,  0,  8,
                  0,  8,  8,  0,  8, 24,  0, 13,  0,  8,  0, 39, 74,  0,  0, 79,  0,  0,
                  8, 24,  0, 26, 26,  8,  8, 13,  0,  0,  8,  8, 26,  8, 24,  0,  2, 13,
                  0, 26, 27,  0,  0,  0,  2,  2,  0, 24,  0, 67, 58,  0, 74,  8,  8,  0,
                  0, 13, 24, 13,  8, 26, 26,  0,  0,  0, 79,  2, 13,  8,  8, 13,  8, 28,
                  0,  1, 13,  2, 26, 28, 27,  2,  0,  8,  0,  2,  9,  0, 26, 10,  0, 39,
                  2, 13,  0, 73,  0,  8,  8, 79, 24,  8, 26, 24, 13, 13, 13,  2,  8,  0,
                  2,  0,  0, 39, 13, 24, 13, 13, 13,  8, 24,  0, 26,  0, 13, 26,  0, 58,
                  9,  0, 24, 13, 52, 13, 13,  0, 43,  0, 79,  2,  0,  2, 13, 58, 34, 24,
                  0,  0, 13,  0, 13, 24, 58,  0, 79, 24,  8, 24]]), 'boxes': tensor([[[356.0063, 142.7068, 379.9229, 186.6484],
                 [357.6965, 157.6241, 377.8403, 175.1762],
                 [359.9948, 149.4757, 376.7187, 177.3886],
                 ...,
                 [207.4437, 183.6745, 306.1192, 238.3906],
                 [ 20.0238, 206.8225,  31.5165, 233.4621],
                 [334.9391, 360.3580, 517.6758, 502.9109]]]), 'scores': tensor([[0.3165, 0.2771, 0.2539, 0.2233, 0.2196, 0.1992, 0.1922, 0.1828, 0.1803,
                 0.1578, 0.1537, 0.1474, 0.1458, 0.1424, 0.1322, 0.1308, 0.1132, 0.1127,
                 0.1094, 0.1087, 0.1076, 0.1073, 0.1053, 0.1036, 0.1019, 0.1015, 0.1013,
                 0.1010, 0.1002, 0.0983, 0.0972, 0.0970, 0.0955, 0.0949, 0.0944, 0.0944,
                 0.0942, 0.0933, 0.0918, 0.0886, 0.0883, 0.0872, 0.0861, 0.0837, 0.0818,
                 0.0804, 0.0795, 0.0793, 0.0788, 0.0787, 0.0764, 0.0757, 0.0756, 0.0751,
                 0.0742, 0.0733, 0.0724, 0.0711, 0.0701, 0.0697, 0.0692, 0.0678, 0.0677,
                 0.0676, 0.0672, 0.0671, 0.0661, 0.0659, 0.0658, 0.0657, 0.0651, 0.0649,
                 0.0648, 0.0640, 0.0634, 0.0626, 0.0624, 0.0622, 0.0613, 0.0611, 0.0606,
                 0.0605, 0.0602, 0.0597, 0.0595, 0.0592, 0.0588, 0.0587, 0.0574, 0.0574,
                 0.0573, 0.0563, 0.0560, 0.0560, 0.0556, 0.0554, 0.0552, 0.0548, 0.0544,
                 0.0543, 0.0542, 0.0540, 0.0537, 0.0534, 0.0533, 0.0532, 0.0531, 0.0528,
                 0.0528, 0.0525, 0.0522, 0.0519, 0.0514, 0.0513, 0.0512, 0.0508, 0.0507,
                 0.0504, 0.0503, 0.0499, 0.0498, 0.0493, 0.0488, 0.0482, 0.0480, 0.0476,
                 0.0475, 0.0472, 0.0469, 0.0463, 0.0462, 0.0461, 0.0457, 0.0454, 0.0453,
                 0.0449, 0.0446, 0.0442, 0.0441, 0.0441, 0.0441, 0.0440, 0.0436, 0.0434,
                 0.0430, 0.0428, 0.0428, 0.0426, 0.0421, 0.0420, 0.0414, 0.0412, 0.0412,
                 0.0410, 0.0409, 0.0407, 0.0405, 0.0405, 0.0404, 0.0400, 0.0395, 0.0394,
                 0.0394, 0.0393, 0.0388, 0.0387, 0.0385, 0.0385, 0.0382, 0.0382, 0.0380,
                 0.0379, 0.0379, 0.0378, 0.0378, 0.0377, 0.0374, 0.0373, 0.0371, 0.0370,
                 0.0370, 0.0367, 0.0367, 0.0366, 0.0366, 0.0363, 0.0362, 0.0360, 0.0359,
                 0.0358, 0.0358, 0.0357, 0.0354, 0.0353, 0.0352, 0.0351, 0.0349, 0.0348,
                 0.0347, 0.0347, 0.0346, 0.0345, 0.0344, 0.0344, 0.0343, 0.0341, 0.0341,
                 0.0341, 0.0341, 0.0340, 0.0340, 0.0340, 0.0339, 0.0335, 0.0333, 0.0333,
                 0.0333, 0.0333, 0.0332, 0.0330, 0.0330, 0.0329, 0.0327, 0.0325, 0.0325,
                 0.0325, 0.0324, 0.0324, 0.0324, 0.0323, 0.0322, 0.0321, 0.0320, 0.0319,
                 0.0318, 0.0318, 0.0318, 0.0318, 0.0316, 0.0316, 0.0316, 0.0315, 0.0313,
                 0.0312, 0.0312, 0.0311, 0.0311, 0.0311, 0.0311, 0.0309, 0.0309, 0.0308,
                 0.0307, 0.0306, 0.0305, 0.0305, 0.0303, 0.0303, 0.0302, 0.0302, 0.0302,
                 0.0302, 0.0301, 0.0301, 0.0299, 0.0299, 0.0297, 0.0297, 0.0296, 0.0296,
                 0.0296, 0.0296, 0.0295, 0.0295, 0.0294, 0.0292, 0.0292, 0.0292, 0.0290,
                 0.0289, 0.0289, 0.0289, 0.0287, 0.0287, 0.0286, 0.0286, 0.0286, 0.0284,
                 0.0283, 0.0283, 0.0282, 0.0282, 0.0281, 0.0279, 0.0279, 0.0278, 0.0278,
                 0.0277, 0.0276, 0.0275]])}[0m
[38;5;10m[I] trt-runner-N1-05/19/25-15:35:51     | Completed 1 iteration(s) in 8.505 ms | Average inference time: 8.505 ms.[0m
[38;5;14m[I] onnxrt-runner-N1-05/19/25-15:35:51  | Activating and starting inference[0m
[38;5;14m[I] Creating ONNX-Runtime Inference Session with providers: ['CPUExecutionProvider'][0m
[38;5;11m[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. [0m
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] onnxrt-runner-N1-05/19/25-15:35:51 
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[38;5;104m[X] onnxrt-runner-N1-05/19/25-15:35:51  | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}[0m
[38;5;13m[V] onnxrt-runner-N1-05/19/25-15:35:51  | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}[0m
[I] onnxrt-runner-N1-05/19/25-15:35:51 
    ---- Inference Output(s) ----
    {/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[38;5;104m[X] onnxrt-runner-N1-05/19/25-15:35:51  | Inference Time: 134.993 ms | Received outputs:
        {'/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0': tensor([[[[[-2.9626e-02,  5.0311e-01],
                   [ 1.3804e+00,  1.7026e+00],
                   [ 8.2607e-01, -8.8824e-01],
                   ...,
                   [ 1.5044e+00,  1.3606e+00],
                   [ 9.4686e-01, -4.4774e-01],
                   [ 2.4600e+00, -2.7122e-01]],
        
                  [[ 3.1604e+00,  1.2959e+00],
                   [ 4.5860e+00, -3.6105e-01],
                   [ 3.5112e+00,  1.8520e+00],
                   ...,
                   [ 1.7517e+00,  2.5488e+00],
                   [ 2.6871e+00,  2.7301e+00],
                   [ 4.2822e+00,  3.8652e+00]],
        
                  [[-4.0771e-01, -9.5015e-01],
                   [ 1.3968e+00,  3.6191e+00],
                   [-1.7504e+00,  3.9618e+00],
                   ...,
                   [-2.4087e+00,  2.4920e+00],
                   [-9.4663e-01,  3.7468e+00],
                   [-2.3662e+00,  2.8445e+00]],
        
                  ...,
        
                  [[-2.3464e+00,  1.3922e+00],
                   [-2.0137e+00, -9.7737e-01],
                   [-1.4886e+00, -2.1549e+00],
                   ...,
                   [-4.4665e-01,  9.3095e-01],
                   [-1.5601e+00, -1.9832e+00],
                   [-1.3788e-01,  8.1932e-01]],
        
                  [[-1.6326e-01, -2.8504e+00],
                   [-5.3179e-01, -3.6033e+00],
                   [ 3.5804e-01, -3.8603e+00],
                   ...,
                   [-2.5892e+00, -1.7269e+00],
                   [-1.1735e+00, -2.3692e+00],
                   [-1.6888e+00, -1.7484e+00]],
        
                  [[ 2.1574e+00, -3.7917e-01],
                   [ 2.6141e+00, -1.6792e+00],
                   [ 1.0647e+00, -2.0192e+00],
                   ...,
                   [-1.3369e+00, -4.7207e-01],
                   [ 3.7600e-01, -1.4008e+00],
                   [ 2.7708e+00, -5.5250e+00]]],
        
        
                 [[[-9.5339e-02,  4.4303e-01],
                   [ 1.3873e+00,  1.4964e+00],
                   [ 1.4088e+00, -5.5196e-01],
                   ...,
                   [ 3.8726e-01,  2.7948e-01],
                   [ 5.3749e-01, -1.5643e-01],
                   [ 2.0041e+00,  5.3412e-01]],
        
                  [[ 1.1943e+00,  1.8083e+00],
                   [ 4.2689e+00, -4.0903e-01],
                   [ 3.8032e+00,  2.3514e+00],
                   ...,
                   [ 2.8568e+00,  3.0104e+00],
                   [ 2.4857e+00,  2.0917e+00],
                   [ 4.3961e+00,  4.5826e+00]],
        
                  [[ 2.3091e-01, -7.1097e-01],
                   [ 1.9139e+00,  3.9665e+00],
                   [-1.7010e+00,  4.3265e+00],
                   ...,
                   [-2.9632e-01,  2.5024e+00],
                   [ 2.1429e+00,  3.8519e+00],
                   [-4.6944e-01,  3.0240e+00]],
        
                  ...,
        
                  [[-3.4047e+00,  5.4983e-01],
                   [-2.2266e+00, -1.3346e+00],
                   [-2.5695e+00, -2.0196e+00],
                   ...,
                   [-4.3132e-01,  3.6328e-01],
                   [-1.5082e+00, -1.8268e+00],
                   [-1.5066e+00,  1.1837e+00]],
        
                  [[ 5.0373e-02, -3.2635e+00],
                   [-2.1386e+00, -3.7240e+00],
                   [ 1.4776e+00, -3.8233e+00],
                   ...,
                   [ 1.1425e+00, -1.8731e+00],
                   [ 7.6900e-01, -2.2674e+00],
                   [ 5.1408e-01, -1.6097e+00]],
        
                  [[ 3.0129e+00, -3.6718e-01],
                   [ 2.8157e+00, -2.1283e+00],
                   [ 1.4257e+00, -2.3667e+00],
                   ...,
                   [-1.3589e-02, -9.4063e-01],
                   [-1.1014e-01, -3.0156e+00],
                   [ 6.4843e+00, -5.8382e+00]]],
        
        
                 [[[ 8.4702e-01, -4.3607e-01],
                   [ 1.5967e+00,  7.8520e-01],
                   [ 1.6803e+00, -1.7687e+00],
                   ...,
                   [ 1.2267e+00, -3.9536e-01],
                   [ 1.1491e+00,  1.1001e+00],
                   [ 1.6285e+00, -1.0051e+00]],
        
                  [[ 2.5106e+00,  1.8155e+00],
                   [ 3.9521e+00, -4.5794e-01],
                   [ 3.8971e+00,  2.4780e+00],
                   ...,
                   [ 3.9414e+00,  7.9669e-01],
                   [ 3.0212e+00, -1.3067e+00],
                   [ 3.7526e+00,  2.0901e+00]],
        
                  [[-9.1440e-01,  1.2334e+00],
                   [ 2.6067e+00,  3.9110e+00],
                   [-7.9023e-02,  4.1029e+00],
                   ...,
                   [-7.3608e-02,  2.9596e+00],
                   [ 2.8120e+00,  3.7511e+00],
                   [-9.5722e-01,  3.0524e+00]],
        
                  ...,
        
                  [[-3.0343e+00, -7.8578e-01],
                   [-3.3878e+00, -2.2207e+00],
                   [-3.4036e+00, -3.3212e+00],
                   ...,
                   [-3.5384e-01, -1.7178e+00],
                   [-9.7481e-01, -2.2026e+00],
                   [-1.1872e+00, -1.0879e+00]],
        
                  [[ 3.1452e-01, -3.8772e+00],
                   [-1.0013e+00, -3.9658e+00],
                   [ 1.2621e+00, -3.9341e+00],
                   ...,
                   [ 2.0400e+00, -2.3238e+00],
                   [-1.3470e+00, -2.5756e+00],
                   [ 6.4108e-01, -2.8480e+00]],
        
                  [[ 3.3446e+00, -1.3616e+00],
                   [ 3.4902e+00, -2.2771e+00],
                   [ 3.2048e+00, -3.1084e+00],
                   ...,
                   [-3.6276e+00, -1.6038e+00],
                   [ 9.3202e-01, -2.5812e+00],
                   [ 4.8259e+00, -2.7178e+00]]],
        
        
                 ...,
        
        
                 [[[ 1.2395e+00,  4.1345e-01],
                   [ 2.0213e+00,  1.1960e+00],
                   [ 2.4461e+00, -8.9112e-01],
                   ...,
                   [ 4.6355e-01,  1.4206e-01],
                   [ 1.8286e+00,  1.8150e+00],
                   [ 1.8688e+00,  1.8287e-01]],
        
                  [[ 1.2162e+00,  2.3428e+00],
                   [ 4.1223e+00,  1.2540e+00],
                   [ 4.1351e+00,  3.0117e+00],
                   ...,
                   [ 3.0947e+00,  2.1540e+00],
                   [ 3.0297e+00,  1.2415e+00],
                   [ 3.7037e+00,  2.0812e+00]],
        
                  [[-7.2682e-01,  1.1491e+00],
                   [ 2.8918e+00,  4.0209e+00],
                   [-9.6383e-01,  4.0169e+00],
                   ...,
                   [-1.7757e-01,  3.1797e+00],
                   [ 1.5271e+00,  3.8930e+00],
                   [-1.3283e+00,  3.1096e+00]],
        
                  ...,
        
                  [[-3.3041e+00, -3.1045e-01],
                   [-3.4672e+00, -1.9771e+00],
                   [-3.5393e+00, -2.7796e+00],
                   ...,
                   [-1.0612e+00, -9.3831e-01],
                   [-1.3689e+00, -2.3790e+00],
                   [-1.5812e+00, -2.4498e-01]],
        
                  [[ 1.2117e+00, -3.7133e+00],
                   [-2.4701e+00, -3.8249e+00],
                   [ 3.6423e+00, -3.8554e+00],
                   ...,
                   [ 1.8935e+00, -2.3368e+00],
                   [-2.0856e+00, -2.4843e+00],
                   [ 8.1738e-01, -2.6743e+00]],
        
                  [[ 3.7656e+00, -7.1657e-01],
                   [ 3.5946e+00, -1.9811e+00],
                   [ 3.0353e+00, -3.2562e+00],
                   ...,
                   [-2.0125e+00, -2.1648e+00],
                   [ 9.5891e-01, -1.6325e+00],
                   [ 1.9428e+00, -2.1817e+00]]],
        
        
                 [[[ 6.0771e-01,  1.0262e-01],
                   [ 1.4831e+00,  1.1067e+00],
                   [ 1.0610e+00, -1.2201e+00],
                   ...,
                   [ 7.2343e-01,  2.4883e-01],
                   [ 2.2162e-01, -6.4252e-02],
                   [ 2.0676e+00, -3.9533e-03]],
        
                  [[ 1.3820e+00,  1.4330e+00],
                   [ 4.1732e+00, -9.7843e-01],
                   [ 3.5571e+00,  2.0292e+00],
                   ...,
                   [ 3.2113e+00,  2.6654e+00],
                   [ 1.9980e+00,  1.2863e+00],
                   [ 3.9677e+00,  3.9829e+00]],
        
                  [[-1.3117e-01, -5.9895e-01],
                   [ 1.8525e+00,  3.7325e+00],
                   [-1.6235e+00,  4.0922e+00],
                   ...,
                   [ 4.0785e-01,  2.5183e+00],
                   [ 2.8739e+00,  3.7995e+00],
                   [-5.6733e-01,  2.1583e+00]],
        
                  ...,
        
                  [[-2.7160e+00, -3.2235e-01],
                   [-2.3320e+00, -1.7031e+00],
                   [-2.1535e+00, -2.4451e+00],
                   ...,
                   [-3.5487e-01, -4.1812e-01],
                   [-1.1130e+00, -1.9623e+00],
                   [-1.1510e+00,  8.9924e-01]],
        
                  [[ 4.9821e-01, -3.1058e+00],
                   [-1.4791e+00, -3.7269e+00],
                   [ 1.4693e+00, -3.8320e+00],
                   ...,
                   [ 1.4400e+00, -1.8285e+00],
                   [-2.2986e-01, -2.1105e+00],
                   [ 8.0130e-01, -1.7942e+00]],
        
                  [[ 2.6409e+00, -9.5229e-01],
                   [ 2.7350e+00, -2.5601e+00],
                   [ 1.5331e+00, -2.6201e+00],
                   ...,
                   [-3.4041e+00,  1.6932e-01],
                   [ 9.6725e-02, -2.4743e+00],
                   [ 6.6096e+00, -4.1296e+00]]],
        
        
                 [[[ 6.2046e-01,  2.2938e-01],
                   [ 1.4172e+00,  9.8789e-01],
                   [ 1.1880e+00, -1.0214e+00],
                   ...,
                   [ 4.4593e-01, -1.3600e-01],
                   [ 6.2095e-01, -1.0169e+00],
                   [ 2.2083e+00,  3.7741e-01]],
        
                  [[ 1.0274e+00,  1.3362e+00],
                   [ 4.2800e+00, -1.1181e+00],
                   [ 3.5766e+00,  1.7940e+00],
                   ...,
                   [ 2.9678e+00,  2.4085e+00],
                   [ 2.0956e+00,  1.5165e+00],
                   [ 4.3328e+00,  3.7145e+00]],
        
                  [[ 1.2721e-01, -1.1718e+00],
                   [ 1.8150e+00,  3.7037e+00],
                   [-1.1822e+00,  4.1661e+00],
                   ...,
                   [ 6.0087e-01,  2.4921e+00],
                   [ 3.2088e+00,  3.8520e+00],
                   [ 1.3215e-01,  2.3907e+00]],
        
                  ...,
        
                  [[-2.6953e+00, -6.0336e-01],
                   [-2.1463e+00, -1.6653e+00],
                   [-2.1418e+00, -2.1711e+00],
                   ...,
                   [-5.2993e-01, -4.9240e-01],
                   [-1.5827e+00, -1.8434e+00],
                   [-1.3610e+00,  1.0862e+00]],
        
                  [[ 3.0338e-01, -3.0988e+00],
                   [-1.0183e+00, -3.7937e+00],
                   [ 1.1962e+00, -3.9335e+00],
                   ...,
                   [ 1.6064e+00, -1.9946e+00],
                   [ 1.2463e+00, -2.2435e+00],
                   [ 1.0126e+00, -1.5934e+00]],
        
                  [[ 2.6637e+00, -1.0263e+00],
                   [ 2.6858e+00, -2.4457e+00],
                   [ 1.6577e+00, -2.5366e+00],
                   ...,
                   [-1.5776e+00,  1.4631e-01],
                   [ 2.8820e-01, -2.6687e+00],
                   [ 6.3413e+00, -5.3056e+00]]]]]), '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0': tensor([[[[[ 6.5739e-01,  1.0883e+00],
                   [ 8.2806e-01, -1.0424e+00],
                   [ 1.7466e+00, -8.0331e-01],
                   ...,
                   [ 7.4283e-01,  6.0755e-01],
                   [ 2.7224e+00,  3.4009e-01],
                   [ 2.2032e+00, -3.3789e-01]],
        
                  [[ 2.6222e+00,  6.4460e-01],
                   [ 2.3778e+00,  2.4246e+00],
                   [ 2.5137e+00,  1.8394e+00],
                   ...,
                   [ 2.0093e+00,  1.1712e+00],
                   [ 2.2881e+00,  1.6911e+00],
                   [ 5.9008e+00,  6.7485e+00]],
        
                  [[-1.7750e+00, -2.3288e+00],
                   [-7.2683e-01,  3.2791e+00],
                   [-1.5511e+00,  3.8471e+00],
                   ...,
                   [ 1.1714e+00,  9.8654e-01],
                   [-4.4137e-01,  3.2210e+00],
                   [ 3.6412e-01,  3.5302e+00]],
        
                  ...,
        
                  [[-4.5011e+00,  6.8285e-01],
                   [-3.5966e+00, -4.5044e-01],
                   [-3.7993e+00, -1.3147e+00],
                   ...,
                   [-2.0109e+00, -3.2885e-01],
                   [-5.1884e-01, -4.3916e+00],
                   [-4.8551e+00, -3.6086e+00]],
        
                  [[ 1.9040e+00,  3.4585e+00],
                   [-2.5465e-01, -2.5248e+00],
                   [-4.5155e-01, -3.0928e+00],
                   ...,
                   [-7.0463e-01, -1.2255e+00],
                   [-4.7499e-01, -2.9874e+00],
                   [ 2.3300e-01, -2.7097e+00]],
        
                  [[ 3.7860e+00, -1.2398e+00],
                   [ 4.2269e+00,  9.0767e-01],
                   [ 3.3397e+00, -7.9448e-01],
                   ...,
                   [-1.9680e-01, -1.3941e+00],
                   [ 3.0561e+00, -1.8779e+00],
                   [ 2.2116e+00, -3.0860e+00]]],
        
        
                 [[[ 2.1510e+00,  1.7804e+00],
                   [ 1.6356e+00, -9.6380e-01],
                   [ 2.6788e+00,  2.4698e-01],
                   ...,
                   [ 8.9777e-01,  2.1794e+00],
                   [ 2.2380e+00,  1.8898e+00],
                   [ 1.5055e+00,  4.0926e-01]],
        
                  [[ 3.1203e+00,  2.2685e+00],
                   [ 2.0030e+00,  2.7980e+00],
                   [ 2.9386e+00,  2.1402e+00],
                   ...,
                   [ 1.5628e+00,  2.0165e+00],
                   [ 3.0892e+00,  1.6879e+00],
                   [ 7.0913e+00,  4.0157e+00]],
        
                  [[-1.1152e+00, -8.7121e-01],
                   [-1.2338e+00,  3.4290e+00],
                   [-2.0353e+00,  3.7881e+00],
                   ...,
                   [ 8.1966e-01,  1.5176e+00],
                   [-1.2843e+00,  3.2359e+00],
                   [-2.6138e-01,  3.1810e+00]],
        
                  ...,
        
                  [[-4.5001e+00,  1.8781e+00],
                   [-3.8906e+00,  5.1627e-01],
                   [-4.1603e+00, -7.2525e-01],
                   ...,
                   [-5.2775e-01, -3.0315e-01],
                   [ 7.9997e-01, -4.3354e+00],
                   [-2.1172e+00, -3.1760e+00]],
        
                  [[ 1.3510e+00,  2.2846e+00],
                   [-8.4979e-01, -2.5411e+00],
                   [-1.9194e+00, -3.1269e+00],
                   ...,
                   [-4.9989e-01, -1.2187e+00],
                   [-5.5228e-01, -2.8045e+00],
                   [-1.1339e-03, -1.5672e+00]],
        
                  [[ 3.3089e+00, -7.4703e-01],
                   [ 4.0632e+00,  1.7110e+00],
                   [ 3.5947e+00,  4.3159e-01],
                   ...,
                   [-9.3359e-01, -1.6864e+00],
                   [ 2.9746e+00, -4.7968e-01],
                   [ 2.5973e+00, -2.3041e+00]]],
        
        
                 [[[ 2.5166e+00,  1.3435e+00],
                   [ 2.0963e+00, -2.0124e+00],
                   [ 3.0302e+00, -1.3358e+00],
                   ...,
                   [ 1.2633e+00, -9.7110e-01],
                   [ 1.7567e+00,  9.7590e-02],
                   [ 7.2775e-01, -8.8415e-01]],
        
                  [[ 3.3904e+00,  1.0406e+00],
                   [ 2.5396e+00,  3.0087e+00],
                   [ 3.0274e+00,  2.1086e+00],
                   ...,
                   [ 8.7786e-01,  1.3041e+00],
                   [ 2.7439e+00,  1.4391e+00],
                   [ 6.6604e+00,  3.3852e+00]],
        
                  [[ 5.8358e-01,  2.1911e-02],
                   [-4.9549e-01,  3.7547e+00],
                   [-1.8729e+00,  3.6999e+00],
                   ...,
                   [ 6.1769e-01,  1.8878e+00],
                   [-2.7521e-01,  2.8923e+00],
                   [-9.6377e-02,  3.2404e+00]],
        
                  ...,
        
                  [[-3.8046e+00,  2.6622e-01],
                   [-3.7956e+00, -1.5085e+00],
                   [-3.8546e+00, -2.6863e+00],
                   ...,
                   [-1.6349e+00, -8.8092e-01],
                   [-7.5127e-01, -3.5906e+00],
                   [-2.2702e+00, -2.5099e+00]],
        
                  [[-3.6280e-01, -9.8353e-02],
                   [-6.5593e-01, -3.7751e+00],
                   [-1.8356e+00, -3.6350e+00],
                   ...,
                   [-3.5123e-01, -1.8441e+00],
                   [-2.7946e-01, -2.3063e+00],
                   [-5.0791e-03, -2.1279e+00]],
        
                  [[ 3.2053e+00, -1.8274e+00],
                   [ 3.7130e+00, -1.9607e-01],
                   [ 3.7208e+00, -1.2157e+00],
                   ...,
                   [-9.5014e-01, -1.6978e+00],
                   [ 2.0720e+00, -1.4547e+00],
                   [ 1.1467e+00, -1.8441e+00]]],
        
        
                 ...,
        
        
                 [[[ 3.3671e+00,  2.3563e+00],
                   [ 3.2236e+00, -2.1642e+00],
                   [ 3.6109e+00, -4.6150e-01],
                   ...,
                   [ 2.1834e+00,  4.8578e-01],
                   [ 2.3484e+00,  1.9835e+00],
                   [ 1.4502e+00, -3.7189e-01]],
        
                  [[ 3.7752e+00,  2.4368e+00],
                   [ 3.3799e+00,  3.3091e+00],
                   [ 3.9518e+00,  2.8891e+00],
                   ...,
                   [ 2.2446e+00,  2.0769e+00],
                   [ 2.6340e+00,  1.9792e+00],
                   [ 4.4921e+00,  8.3034e-01]],
        
                  [[ 1.0820e+00,  1.0011e+00],
                   [-7.4493e-01,  4.0727e+00],
                   [-3.1015e+00,  3.9790e+00],
                   ...,
                   [-2.1543e-01,  2.9387e+00],
                   [ 5.6303e-01,  3.3902e+00],
                   [ 2.1638e-01,  3.4134e+00]],
        
                  ...,
        
                  [[-4.3768e+00,  2.6116e+00],
                   [-4.0137e+00,  1.4096e+00],
                   [-3.9284e+00, -6.9659e-01],
                   ...,
                   [-1.9403e+00, -4.7104e-01],
                   [-9.8328e-01, -2.5470e+00],
                   [-2.8691e+00, -1.6740e+00]],
        
                  [[-6.3163e-01, -1.0429e+00],
                   [ 2.7599e-01, -3.6025e+00],
                   [-1.7056e+00, -3.4837e+00],
                   ...,
                   [ 9.6464e-01, -1.8594e+00],
                   [ 2.2095e-02, -2.5001e+00],
                   [-2.9557e-01, -1.7884e+00]],
        
                  [[ 4.3851e+00, -1.5660e+00],
                   [ 3.9040e+00,  1.1550e+00],
                   [ 3.9656e+00, -2.4138e-01],
                   ...,
                   [-1.0957e+00, -1.6508e+00],
                   [ 2.7113e+00, -8.7485e-01],
                   [ 1.1280e+00, -1.8341e+00]]],
        
        
                 [[[ 1.4733e+00,  6.6774e-01],
                   [ 1.5430e+00, -2.1069e+00],
                   [ 2.1853e+00, -1.1576e+00],
                   ...,
                   [ 1.2364e+00, -2.0540e-01],
                   [ 2.1876e+00,  8.4890e-02],
                   [ 1.2665e+00, -8.2904e-02]],
        
                  [[ 2.7641e+00,  1.1478e+00],
                   [ 2.5060e+00,  2.5502e+00],
                   [ 2.5644e+00,  1.5476e+00],
                   ...,
                   [ 9.4182e-01,  2.1180e+00],
                   [ 2.8329e+00,  1.5959e+00],
                   [ 8.1195e+00,  4.8258e+00]],
        
                  [[-2.4606e-01, -5.0298e-01],
                   [-6.3087e-02,  3.3698e+00],
                   [-1.1236e+00,  3.5841e+00],
                   ...,
                   [ 1.6464e+00,  8.6388e-01],
                   [-2.6919e-01,  2.9435e+00],
                   [ 6.9223e-01,  2.8663e+00]],
        
                  ...,
        
                  [[-4.1159e+00, -5.4012e-01],
                   [-3.6219e+00, -1.7354e+00],
                   [-3.7604e+00, -2.0506e+00],
                   ...,
                   [-1.6442e+00, -8.1849e-01],
                   [-8.5850e-01, -5.3801e+00],
                   [-3.3982e+00, -3.3966e+00]],
        
                  [[ 4.2219e-01,  1.8193e+00],
                   [-1.6095e-01, -3.0933e+00],
                   [-1.2672e+00, -3.2446e+00],
                   ...,
                   [-7.5868e-01, -1.4061e+00],
                   [ 2.8518e-01, -2.4764e+00],
                   [ 8.9751e-01, -2.3806e+00]],
        
                  [[ 3.5154e+00, -2.1870e+00],
                   [ 3.9560e+00, -2.8318e-01],
                   [ 3.4325e+00, -1.3633e+00],
                   ...,
                   [ 1.4787e-01, -1.6549e+00],
                   [ 3.6530e+00, -1.6876e+00],
                   [ 2.8583e+00, -3.4443e+00]]],
        
        
                 [[[ 2.0375e+00,  1.7558e+00],
                   [ 1.9177e+00, -1.2042e+00],
                   [ 2.4660e+00, -4.8845e-01],
                   ...,
                   [ 1.4141e+00,  1.0150e+00],
                   [ 2.3307e+00,  7.0787e-01],
                   [ 1.8301e+00,  5.8343e-01]],
        
                  [[ 2.9523e+00,  2.3946e+00],
                   [ 3.0132e+00,  2.6165e+00],
                   [ 3.0536e+00,  2.3304e+00],
                   ...,
                   [ 1.2350e+00,  2.2256e+00],
                   [ 3.3686e+00,  2.4792e+00],
                   [ 6.8504e+00,  5.0026e+00]],
        
                  [[-6.6688e-01, -1.3001e-01],
                   [-9.2719e-02,  3.3487e+00],
                   [-1.2211e+00,  3.6801e+00],
                   ...,
                   [ 2.0577e+00,  1.1145e+00],
                   [-4.5301e-01,  2.9939e+00],
                   [ 7.7996e-01,  3.1046e+00]],
        
                  ...,
        
                  [[-3.9568e+00,  9.1613e-01],
                   [-3.4633e+00, -1.6257e-01],
                   [-3.8687e+00, -7.3980e-01],
                   ...,
                   [-8.4120e-01,  7.5890e-01],
                   [-1.0685e+00, -5.0543e+00],
                   [-2.4148e+00, -2.6555e+00]],
        
                  [[-1.8354e-01,  2.8161e+00],
                   [ 2.7992e-01, -2.5643e+00],
                   [-5.4513e-01, -3.0886e+00],
                   ...,
                   [-4.8855e-01, -1.5227e+00],
                   [ 1.1092e+00, -2.5455e+00],
                   [ 1.5533e+00, -2.1705e+00]],
        
                  [[ 3.6467e+00, -1.5104e+00],
                   [ 3.9722e+00,  6.3705e-01],
                   [ 3.3812e+00, -8.1724e-01],
                   ...,
                   [ 1.7121e-01, -9.8417e-01],
                   [ 3.7802e+00, -7.3074e-01],
                   [ 3.4448e+00, -1.8215e+00]]]]]), '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0': tensor([[[[[-4.7352e-01,  3.3598e-02],
                   [-1.4070e-01,  1.2936e+00],
                   [ 4.9469e-01, -7.6794e-03],
                   ...,
                   [ 6.6284e-01,  1.1107e+00],
                   [ 4.4285e-01,  7.5280e-01],
                   [ 9.6479e-01,  7.7465e-01]],
        
                  [[-3.5531e+00, -6.5854e-02],
                   [ 4.3461e+00,  1.3772e-01],
                   [ 4.1213e+00,  2.2297e+00],
                   ...,
                   [ 2.4992e+00,  9.2022e-01],
                   [ 2.2905e+00,  1.9439e+00],
                   [ 3.5082e+00,  3.4121e+00]],
        
                  [[-2.2690e-01, -1.9739e+00],
                   [ 2.7823e-01,  4.5327e+00],
                   [-3.0656e+00,  4.1107e+00],
                   ...,
                   [-7.4419e-01,  1.5044e+00],
                   [ 3.6999e-01,  4.1330e+00],
                   [-7.8784e-02,  4.5757e+00]],
        
                  ...,
        
                  [[-1.8460e+00,  1.6472e+00],
                   [-9.6404e-01, -2.1842e+00],
                   [-1.7174e+00, -1.1928e+00],
                   ...,
                   [-3.1100e-01,  8.6714e-01],
                   [-5.6231e-01, -1.1950e+00],
                   [-1.3386e+00,  5.7670e-01]],
        
                  [[-1.5801e+00,  1.4103e+00],
                   [-4.0227e-01, -3.7514e+00],
                   [-1.0922e+00, -3.7747e+00],
                   ...,
                   [-1.9982e+00, -9.7746e-01],
                   [ 1.2506e+00, -1.4575e+00],
                   [ 1.2981e-01, -3.7588e+00]],
        
                  [[-7.6281e-01, -3.3634e+00],
                   [ 1.4628e+00, -1.8583e+00],
                   [ 2.1825e+00,  6.2508e-01],
                   ...,
                   [ 3.2999e+00, -1.9366e+00],
                   [ 9.5722e-01, -1.7204e+00],
                   [ 4.3141e+00, -4.5292e+00]]],
        
        
                 [[[-1.5946e+00,  1.0562e-01],
                   [-2.1967e-01,  1.1837e+00],
                   [ 7.6737e-01,  4.9628e-01],
                   ...,
                   [ 3.0471e-01,  1.3565e+00],
                   [ 3.2521e-01,  1.3825e+00],
                   [ 4.0888e-01,  9.6820e-01]],
        
                  [[-1.1914e+00,  5.6363e-01],
                   [ 3.8904e+00,  1.6706e+00],
                   [ 3.9571e+00,  2.1609e+00],
                   ...,
                   [ 1.8348e+00,  9.7345e-02],
                   [ 2.5554e+00,  3.4160e+00],
                   [ 3.1501e+00,  4.3630e+00]],
        
                  [[-1.6444e+00,  1.9500e-01],
                   [-2.0051e-01,  4.5046e+00],
                   [-3.5116e+00,  4.0129e+00],
                   ...,
                   [-8.0531e-01,  8.6508e-01],
                   [-1.7855e-02,  3.8872e+00],
                   [-5.1509e-03,  4.3373e+00]],
        
                  ...,
        
                  [[-2.8189e+00,  2.1720e+00],
                   [-9.9726e-01, -2.3285e+00],
                   [-2.6595e+00, -1.5050e+00],
                   ...,
                   [-1.9221e-01,  6.9497e-01],
                   [-2.0656e+00, -6.6519e-01],
                   [-1.8901e+00,  1.1272e+00]],
        
                  [[-1.0728e+00,  1.4028e+00],
                   [-1.0876e+00, -3.7444e+00],
                   [-2.3806e+00, -3.7723e+00],
                   ...,
                   [-2.1112e+00, -1.3050e+00],
                   [ 8.1377e-01, -1.5707e+00],
                   [-1.0978e+00, -3.5312e+00]],
        
                  [[ 9.6047e-01, -2.8990e+00],
                   [ 1.4905e+00, -1.7308e+00],
                   [ 2.9601e+00,  2.7750e+00],
                   ...,
                   [ 2.1394e+00, -2.3007e+00],
                   [ 6.5247e-01, -3.4163e+00],
                   [ 3.0090e+00, -4.3650e+00]]],
        
        
                 [[[-5.1478e-01, -1.6492e+00],
                   [ 3.6770e-01,  4.7703e-02],
                   [ 1.3655e+00, -1.5458e+00],
                   ...,
                   [ 4.1290e-01,  5.7652e-02],
                   [ 6.3678e-01, -1.9731e-01],
                   [ 4.2777e-01, -8.6644e-01]],
        
                  [[-8.1976e-01, -9.7179e-01],
                   [ 3.8002e+00, -1.1559e+00],
                   [ 3.9451e+00,  2.0552e+00],
                   ...,
                   [ 2.1475e-01,  2.5299e-01],
                   [ 2.8337e+00, -3.9164e-01],
                   [ 3.1142e+00,  3.0502e-01]],
        
                  [[ 1.6287e+00,  8.0700e-01],
                   [-5.2824e-02,  4.2021e+00],
                   [-1.2180e+00,  3.9516e+00],
                   ...,
                   [-3.9314e-01,  1.2119e+00],
                   [ 3.4604e-01,  4.0021e+00],
                   [ 2.5375e-01,  3.7740e+00]],
        
                  ...,
        
                  [[-3.0914e+00,  9.5688e-02],
                   [-1.4856e+00, -3.3280e+00],
                   [-2.9852e+00, -2.4904e+00],
                   ...,
                   [ 7.0262e-02, -1.4160e+00],
                   [-6.6277e-01, -2.4420e+00],
                   [-6.8223e-01, -1.2217e+00]],
        
                  [[-3.1684e-02, -1.4123e+00],
                   [-8.3316e-01, -4.0731e+00],
                   [-1.2797e+00, -3.8913e+00],
                   ...,
                   [-9.5416e-01, -2.1275e+00],
                   [ 9.3690e-01, -2.0956e+00],
                   [-1.0430e-01, -3.0138e+00]],
        
                  [[ 1.2610e+00, -3.2419e+00],
                   [ 2.6335e+00, -2.8255e+00],
                   [ 3.4196e+00, -4.4079e-01],
                   ...,
                   [ 2.1424e+00, -2.0518e+00],
                   [ 6.2761e-01, -2.4444e+00],
                   [ 3.6438e+00, -3.7387e+00]]],
        
        
                 ...,
        
        
                 [[[-5.1500e-01, -6.7004e-01],
                   [ 4.4843e-01,  1.1026e+00],
                   [ 2.0584e+00, -1.1486e+00],
                   ...,
                   [-1.2150e-01,  2.7012e+00],
                   [ 1.4657e+00,  1.0575e+00],
                   [ 3.5539e-01,  6.0421e-02]],
        
                  [[-1.7156e-01,  7.6936e-02],
                   [ 4.1054e+00,  2.0699e-01],
                   [ 3.9616e+00,  2.7943e+00],
                   ...,
                   [ 1.2344e+00,  1.2860e+00],
                   [ 2.8740e+00,  1.2247e+00],
                   [ 2.9821e+00,  2.7093e+00]],
        
                  [[ 6.3843e-01,  1.0487e+00],
                   [ 7.5470e-02,  4.1958e+00],
                   [-2.7262e+00,  4.0948e+00],
                   ...,
                   [ 2.6192e-01,  1.7842e+00],
                   [ 1.3518e+00,  4.1160e+00],
                   [-9.2485e-01,  4.4075e+00]],
        
                  ...,
        
                  [[-3.6725e+00,  2.5786e+00],
                   [-1.4117e+00, -2.8565e+00],
                   [-3.3055e+00, -1.7759e+00],
                   ...,
                   [ 1.4634e+00, -6.5520e-01],
                   [-3.2880e-01, -8.7664e-01],
                   [-1.6835e+00, -5.6301e-01]],
        
                  [[ 5.5111e-01, -3.9213e-01],
                   [ 2.2552e-02, -3.8481e+00],
                   [-7.2458e-01, -3.8959e+00],
                   ...,
                   [-1.3182e+00, -2.2221e+00],
                   [ 1.8592e+00, -2.3242e+00],
                   [-1.7961e-01, -3.0282e+00]],
        
                  [[ 1.9184e+00, -3.1528e+00],
                   [ 3.0906e+00, -3.1710e+00],
                   [ 3.7199e+00,  1.3179e+00],
                   ...,
                   [ 1.2686e+00, -1.3781e+00],
                   [-1.3213e-01, -2.2795e+00],
                   [ 3.0152e+00, -4.1140e+00]]],
        
        
                 [[[-6.3012e-01, -8.6077e-01],
                   [ 4.2006e-01,  3.4247e-01],
                   [ 8.1376e-01, -7.9634e-01],
                   ...,
                   [ 6.6952e-01, -6.3572e-01],
                   [ 8.6912e-01, -5.8667e-02],
                   [ 1.1809e+00, -3.3473e-02]],
        
                  [[-2.2424e+00, -3.4792e-01],
                   [ 4.3823e+00, -6.3835e-01],
                   [ 4.1333e+00,  1.5611e+00],
                   ...,
                   [ 7.1723e-02, -9.0433e-01],
                   [ 2.7694e+00,  1.7506e+00],
                   [ 3.9892e+00,  2.2348e+00]],
        
                  [[ 4.4744e-01, -9.5495e-01],
                   [-8.1057e-02,  3.5762e+00],
                   [-1.6723e+00,  3.9380e+00],
                   ...,
                   [-1.4411e+00, -5.1147e-01],
                   [ 2.3654e-01,  3.6756e+00],
                   [-7.4180e-01,  4.0160e+00]],
        
                  ...,
        
                  [[-2.0353e+00, -3.6569e-01],
                   [-6.2269e-01, -2.5728e+00],
                   [-1.6717e+00, -1.9910e+00],
                   ...,
                   [-5.9524e-01, -9.4032e-01],
                   [-1.1887e+00, -1.6813e+00],
                   [-1.1287e+00, -1.0847e+00]],
        
                  [[-3.7576e-01,  1.8961e+00],
                   [-2.3290e-01, -3.7608e+00],
                   [-6.1545e-01, -3.7136e+00],
                   ...,
                   [-1.9703e+00, -1.2780e+00],
                   [ 1.5136e+00, -1.7880e+00],
                   [ 7.8546e-01, -3.6414e+00]],
        
                  [[-1.5182e+00, -2.9454e+00],
                   [ 2.1548e+00, -2.4746e+00],
                   [ 2.5551e+00, -3.4069e-01],
                   ...,
                   [ 3.8776e+00, -1.1784e+00],
                   [ 3.0255e-01, -1.7217e+00],
                   [ 3.7461e+00, -4.2863e+00]]],
        
        
                 [[[ 1.0898e-01,  9.4199e-01],
                   [ 1.4409e+00,  1.4677e+00],
                   [ 1.5107e+00,  1.2889e+00],
                   ...,
                   [ 1.9393e+00,  1.6154e+00],
                   [ 1.8870e+00,  1.8296e+00],
                   [ 2.0576e+00,  1.3799e+00]],
        
                  [[-1.6038e+00,  2.9070e-01],
                   [ 4.3101e+00,  1.2638e+00],
                   [ 4.2163e+00,  2.6919e+00],
                   ...,
                   [ 6.9087e-01, -3.9337e-03],
                   [ 2.9377e+00,  2.9127e+00],
                   [ 3.6282e+00,  3.8093e+00]],
        
                  [[-1.0497e+00, -1.0438e+00],
                   [ 1.5487e+00,  4.6970e+00],
                   [-1.9864e+00,  4.0547e+00],
                   ...,
                   [-2.2244e-01,  1.0702e-01],
                   [ 7.9862e-01,  3.6338e+00],
                   [ 4.2461e-01,  4.3791e+00]],
        
                  ...,
        
                  [[-2.0941e+00,  1.8278e+00],
                   [-6.6709e-02, -1.4333e+00],
                   [-1.4069e+00, -1.1470e+00],
                   ...,
                   [ 3.0359e-01,  8.4350e-01],
                   [-6.5395e-01, -1.4155e-03],
                   [-6.4852e-01,  1.1953e+00]],
        
                  [[-1.7882e+00,  2.6435e+00],
                   [ 4.0609e-01, -3.5706e+00],
                   [-2.3343e-01, -3.6865e+00],
                   ...,
                   [-6.8734e-01, -1.4038e+00],
                   [ 1.7449e+00, -1.8522e+00],
                   [ 1.2562e+00, -3.5072e+00]],
        
                  [[ 3.7348e-01, -2.7042e+00],
                   [ 2.4422e+00, -1.5350e+00],
                   [ 3.0338e+00,  1.9708e+00],
                   ...,
                   [ 1.3783e+00,  1.7176e-01],
                   [ 1.4641e+00, -2.2724e+00],
                   [ 3.3272e+00, -3.5136e+00]]]]]), '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0': tensor([[[[ 3.8864e+00,  2.6560e+00,  3.0121e+00,  ..., -2.1953e+00,
                   -3.0089e+00, -3.1312e+00],
                  [ 1.6113e+00,  2.4142e+00,  2.6142e+00,  ..., -5.0109e+00,
                   -2.7125e+00, -2.5894e+00],
                  [-9.3098e-01,  3.1053e+00,  3.0030e+00,  ..., -2.6049e+00,
                   -2.4928e+00, -2.7077e+00],
                  ...,
                  [ 2.7089e+00,  2.9258e+00,  3.2356e+00,  ...,  1.8530e-01,
                   -3.3936e+00,  1.4033e+00],
                  [ 3.3739e+00,  3.2462e+00,  3.5344e+00,  ...,  9.9018e-01,
                   -3.7602e+00, -3.5003e+00],
                  [ 8.7974e-01,  1.2086e+00,  1.2997e+00,  ...,  1.4150e+00,
                   -1.3747e+00,  5.1302e-01]],
        
                 [[ 2.9682e+00,  1.3912e+00,  1.6830e+00,  ..., -1.9778e-01,
                   -2.9512e+00, -1.9104e+00],
                  [-2.9359e-01,  1.9063e+00,  2.1364e+00,  ..., -2.8679e+00,
                   -1.6543e+00, -1.7252e+00],
                  [-2.1586e+00,  2.2035e+00,  2.3946e+00,  ..., -2.2343e+00,
                   -1.4859e+00, -1.5427e+00],
                  ...,
                  [ 2.4410e+00,  1.9994e+00,  2.5094e+00,  ...,  3.3492e-01,
                   -2.9186e+00,  1.1084e+00],
                  [ 1.8997e+00,  2.1721e+00,  2.4938e+00,  ..., -1.0955e-01,
                   -3.7008e+00, -1.8711e+00],
                  [ 9.0318e-01,  8.6024e-01,  8.2960e-01,  ...,  1.2557e+00,
                   -1.0860e+00,  5.8152e-01]],
        
                 [[-1.0341e+00, -1.6683e-01,  1.5413e-01,  ...,  1.6568e+00,
                    5.5727e-01,  6.7211e-01],
                  [-1.8131e+00,  5.4374e-02,  4.4451e-01,  ..., -2.6599e-01,
                    2.9458e-01, -6.5159e-01],
                  [-1.2081e+00,  4.3939e-01,  1.5954e-01,  ...,  1.7240e-03,
                    2.4412e-01, -1.1490e-01],
                  ...,
                  [-6.2450e-01,  7.3257e-01,  5.9018e-01,  ...,  1.0572e+00,
                   -5.4897e-01, -5.1677e-02],
                  [ 1.7284e-01,  7.8768e-01,  6.8880e-01,  ...,  4.1344e-01,
                   -1.1270e+00, -7.0857e-01],
                  [-2.5763e-01,  3.1391e-01, -4.0031e-01,  ...,  1.3031e-01,
                   -7.8524e-02, -3.6164e-01]],
        
                 ...,
        
                 [[-1.1155e+00, -1.3151e+00, -1.1665e+00,  ...,  1.6249e+00,
                    1.5542e+00,  1.0035e+00],
                  [-1.9593e+00,  1.8104e-01,  6.0639e-01,  ..., -8.7885e-01,
                   -6.4888e-01,  1.5454e-01],
                  [-2.9724e+00,  7.3425e-01,  1.3069e-01,  ..., -2.7351e-01,
                    5.7789e-01, -8.9149e-01],
                  ...,
                  [-1.0052e-01,  1.0048e-01,  7.1309e-01,  ...,  7.6466e-01,
                   -1.4934e+00,  1.0132e+00],
                  [ 1.4174e-01,  1.4977e-01, -1.8534e-01,  ...,  7.5514e-01,
                   -1.8774e-01, -7.3620e-01],
                  [-1.8478e-01, -8.3342e-03, -5.4790e-01,  ...,  3.6886e-01,
                   -8.3651e-01, -3.3297e-01]],
        
                 [[ 3.7093e+00,  2.3833e+00,  2.7045e+00,  ..., -2.1001e+00,
                   -2.5054e+00, -3.2245e+00],
                  [ 6.4102e-01,  2.0790e+00,  2.9145e+00,  ..., -3.6915e+00,
                   -2.5528e+00, -2.7272e+00],
                  [ 3.4890e-01,  3.3598e+00,  2.7466e+00,  ..., -2.7637e+00,
                   -1.8221e+00, -2.4976e+00],
                  ...,
                  [ 2.3157e+00,  2.4551e+00,  3.5161e+00,  ...,  2.1817e-01,
                   -3.1533e+00,  1.0691e+00],
                  [ 3.2675e+00,  3.7400e+00,  3.3350e+00,  ...,  4.8311e-01,
                   -4.8542e+00, -3.4087e+00],
                  [ 1.0330e+00,  9.8612e-01,  1.0605e+00,  ...,  1.2936e+00,
                   -1.3532e+00,  7.3365e-01]],
        
                 [[ 4.5051e+00,  2.3447e+00,  2.8511e+00,  ..., -2.3870e+00,
                   -3.5071e+00, -3.3199e+00],
                  [ 6.4207e-01,  2.1776e+00,  2.5633e+00,  ..., -3.6744e+00,
                   -2.1550e+00, -2.4500e+00],
                  [ 3.4490e-01,  3.3515e+00,  2.9548e+00,  ..., -3.2627e+00,
                   -1.6678e+00, -2.5257e+00],
                  ...,
                  [ 2.4017e+00,  2.5294e+00,  3.2760e+00,  ...,  2.0372e-01,
                   -3.1030e+00,  1.2690e+00],
                  [ 3.2247e+00,  3.4253e+00,  3.3944e+00,  ...,  7.8766e-02,
                   -5.2547e+00, -3.0413e+00],
                  [ 9.8988e-01,  1.3154e+00,  1.0932e+00,  ...,  1.5855e+00,
                   -1.3272e+00,  8.0677e-01]]]]), '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0': tensor([[[[ 2.6173,  2.4923,  1.8669,  ..., -3.5363, -3.4766, -2.6207],
                  [ 1.5001,  2.1129,  2.4533,  ..., -2.0339, -2.2282,  0.9332],
                  [ 0.7101,  3.0754,  3.3411,  ..., -3.8219, -4.0025, -1.6716],
                  ...,
                  [ 2.3858,  1.6422,  0.9790,  ..., -0.7496,  1.1663, -3.8354],
                  [ 1.0473,  3.7199,  2.7989,  ..., -3.4424, -4.9100, -2.5540],
                  [ 1.7258,  1.2984,  0.7174,  ...,  0.5285, -3.2315,  0.2703]],
        
                 [[ 2.3529,  1.0416,  1.9073,  ..., -2.5955, -2.7914, -2.5528],
                  [ 1.3533,  1.0567,  2.1470,  ..., -1.1430, -2.0085,  0.6657],
                  [-1.0832,  2.7968,  2.7571,  ..., -2.4352, -3.7165, -2.2471],
                  ...,
                  [ 1.1894,  1.8599,  1.9270,  ..., -0.3508,  0.5695, -3.2059],
                  [-0.7137,  3.0389,  2.2639,  ..., -3.2947, -3.4605, -1.5399],
                  [ 0.6152,  1.7135,  1.4253,  ...,  0.9839, -2.8940,  1.2901]],
        
                 [[ 0.6215,  0.4753,  0.6065,  ..., -0.4052, -1.3917, -0.2338],
                  [ 0.6264,  0.5480,  1.0207,  ...,  0.2241, -2.0427, -0.4415],
                  [-1.8538,  1.0131,  0.4768,  ..., -0.1581, -0.6637, -1.9499],
                  ...,
                  [ 0.1402,  0.6538,  0.5636,  ..., -0.2381, -0.1857, -1.7170],
                  [-1.8888,  0.8931,  0.8292,  ..., -1.1609, -1.2398, -0.5222],
                  [-0.1818,  0.5529,  0.6991,  ...,  1.0875, -0.6423,  0.5404]],
        
                 ...,
        
                 [[-0.1215, -0.7094,  0.4629,  ...,  0.5710,  0.4645, -0.1105],
                  [-0.0377, -0.1068,  0.1866,  ...,  0.9044, -0.5807, -0.4958],
                  [-2.6755,  0.3787,  0.5123,  ...,  0.3671,  0.0590, -1.0650],
                  ...,
                  [-0.5134,  0.2979,  0.2152,  ...,  0.6018,  0.1271, -0.5756],
                  [-2.3381,  0.6499,  0.0848,  ...,  0.3056, -0.7057,  0.1302],
                  [-1.2395,  0.3969,  0.7082,  ...,  1.1607, -0.8799,  0.4742]],
        
                 [[ 2.4958,  2.7087,  1.6094,  ..., -4.1675, -4.0522, -3.0272],
                  [ 1.5459,  2.2509,  2.3699,  ..., -1.2259, -2.9062,  1.1884],
                  [-0.4718,  3.2835,  2.1105,  ..., -2.7594, -3.8213, -3.3643],
                  ...,
                  [ 1.6648,  1.5913,  1.1224,  ..., -1.1653,  0.0171, -3.2398],
                  [-0.3361,  3.3210,  2.8777,  ..., -3.8188, -4.5605, -2.2729],
                  [ 1.5497,  1.4222,  1.0335,  ...,  1.0292, -2.2524,  0.7039]],
        
                 [[ 2.8415,  2.3623,  1.8292,  ..., -3.4406, -3.2440, -2.5911],
                  [ 0.9362,  1.5937,  2.2280,  ..., -1.3789, -2.3629,  0.9474],
                  [-0.7605,  2.3421,  2.1674,  ..., -2.8195, -3.0883, -3.0568],
                  ...,
                  [ 1.4676,  1.7857,  0.9614,  ..., -0.8216,  0.5061, -3.0744],
                  [-0.7089,  3.2738,  2.2222,  ..., -3.4601, -4.3843, -2.7069],
                  [ 1.4399,  1.8931,  1.4082,  ...,  1.1610, -2.3385,  1.4377]]]]), '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0': tensor([[[[ 3.9151,  4.2549,  3.1362,  ..., -2.4364, -3.1740, -3.7321],
                  [ 1.3221,  3.7641,  3.1905,  ...,  1.3933, -3.5835, -5.0356],
                  [-0.4713,  2.7925,  3.0537,  ...,  0.9258, -1.6190, -0.3539],
                  ...,
                  [ 2.0376,  2.6577,  3.2978,  ...,  0.9834, -2.5509,  1.5784],
                  [ 0.7834,  3.4902,  3.1609,  ..., -3.4826, -3.1710,  0.8098],
                  [ 0.5707,  2.5263,  1.8921,  ...,  0.1553,  0.9190, -0.7568]],
        
                 [[ 2.7675,  2.5564,  2.1182,  ..., -0.8290, -2.1331, -2.8877],
                  [ 0.5220,  1.7180,  4.0806,  ...,  0.1133, -3.8336, -3.6960],
                  [ 0.1771,  2.2643,  2.4974,  ...,  0.7391, -2.1801, -0.4616],
                  ...,
                  [ 2.0779,  1.8550,  1.4699,  ...,  0.6786, -2.2316,  1.2662],
                  [ 0.0426,  2.7867,  2.3261,  ..., -2.4527, -3.1097, -0.1767],
                  [ 0.9949,  0.8221,  2.1220,  ...,  0.9519,  0.2675, -1.2333]],
        
                 [[ 1.4859,  1.6941,  1.1975,  ..., -0.4795, -0.0805, -0.5503],
                  [-0.7114,  1.3918,  1.0382,  ...,  0.8492, -0.1340, -2.7277],
                  [-2.5990,  0.0690,  1.3065,  ...,  0.7695, -0.1505,  0.5618],
                  ...,
                  [ 0.6071,  0.6872,  1.0442,  ...,  0.6443, -0.0131,  0.7085],
                  [-2.7211,  0.2247,  1.4769,  ..., -1.2173, -1.4020,  0.7598],
                  [ 0.3562,  0.9152,  1.2578,  ...,  0.0775,  0.5086, -1.4401]],
        
                 ...,
        
                 [[ 0.4342, -0.0196,  0.7254,  ...,  1.2087,  0.9078,  0.4840],
                  [-2.4008, -0.1790,  0.8937,  ..., -0.1929,  0.0683,  0.0204],
                  [-3.0332, -0.5475,  1.0886,  ...,  0.5707,  1.3899,  0.8868],
                  ...,
                  [ 0.3103,  0.7728, -0.3897,  ...,  1.3435,  1.3636,  0.7046],
                  [-2.1904,  0.3110,  0.4803,  ...,  0.5611,  0.3131, -0.0674],
                  [-1.1805, -0.3424,  0.6639,  ...,  0.0951,  0.9235, -0.7154]],
        
                 [[ 3.5248,  3.6790,  3.1350,  ..., -2.0765, -2.8139, -3.5629],
                  [ 1.9080,  3.7540,  3.1247,  ...,  1.2883, -3.2482, -5.3389],
                  [-0.2693,  2.8896,  2.2241,  ...,  0.7428, -2.2699, -1.3738],
                  ...,
                  [ 1.8455,  2.6315,  2.5821,  ...,  0.9347, -2.9383,  0.6796],
                  [-0.6148,  2.8612,  2.5634,  ..., -2.9427, -2.7382,  0.4858],
                  [ 0.2511,  2.7839,  2.1011,  ..., -1.5470,  1.4541, -1.1836]],
        
                 [[ 2.8229,  3.3151,  1.8541,  ..., -1.4270, -1.7512, -3.8045],
                  [ 0.8376,  2.2058,  4.1382,  ...,  0.4517, -3.9220, -4.7886],
                  [-0.2909,  2.4804,  1.9853,  ...,  0.4840, -1.2333, -0.9911],
                  ...,
                  [ 2.2202,  2.3059,  1.5454,  ...,  0.6374, -2.4326,  0.4265],
                  [-0.3500,  3.3962,  1.6780,  ..., -2.9704, -2.2736,  0.5250],
                  [ 0.8357,  1.9073,  1.8608,  ...,  2.3757,  0.4869, -1.0527]]]]), 'labels': tensor([[ 0, 67,  8,  8, 26, 67,  8,  0,  0,  0, 28,  8,  8,  0,  8,  8,  0,  0,
                 67,  8,  8,  8,  8,  8,  8,  8,  8,  0, 24,  8, 67, 24, 13,  8,  0,  8,
                  8, 67,  0,  8,  8,  8, 67, 26,  8,  0,  0, 26,  8,  0, 67,  8,  8, 26,
                  0,  0,  8,  0,  8, 13,  8,  8,  8, 28, 13,  8,  0, 28, 24,  8,  0, 26,
                 28, 26,  8, 24,  0, 26,  0,  8, 24,  0, 24,  0,  0, 67,  0, 67, 67,  8,
                 24,  0, 26, 67, 24,  0, 67, 79, 58,  8,  8, 24,  0,  0,  0,  8, 28,  8,
                  8, 56,  0, 58, 28, 67, 67,  0, 76,  0, 26, 24, 73,  8, 60, 24, 28,  8,
                 24,  8, 13,  0,  0, 24,  0,  8, 28, 24, 11,  8, 43,  2, 26,  0, 26,  8,
                  8, 28,  8,  8, 24,  0, 24, 28, 24,  2, 24, 26, 26,  0, 26, 28, 39, 26,
                 26, 13, 56, 24,  8,  0, 26, 13, 67, 24,  8, 24, 24, 26,  8, 28,  0, 28,
                  2, 24, 24, 13,  8, 56, 67, 26, 24,  0, 26,  0, 76, 24,  9,  8,  8, 34,
                  0, 58, 67, 28, 13, 24, 13,  8,  8, 24, 56, 79, 56, 24,  0, 12, 25, 28,
                 43, 26,  3,  0, 27, 58, 74,  3,  8,  8,  2, 26, 58,  0,  8, 74, 60,  9,
                  0, 24,  0, 26, 24, 24,  0, 11,  8,  8,  0, 13,  0,  0,  0,  1, 79,  8,
                 28, 24,  0,  1,  2, 59,  0, 58, 24, 28,  0,  9, 26, 24,  8, 26, 79,  8,
                 26, 26, 24, 24,  8, 24, 13, 67,  2,  8,  0, 26,  0, 13, 26, 24,  0, 58,
                  0,  8, 67,  8, 11, 25,  2,  0, 56,  2, 28, 24]]), 'boxes': tensor([[[262.9576,  71.5327, 547.7870, 476.1678],
                 [361.2971, 137.0405, 373.3970, 169.7942],
                 [ -1.0678, 216.7084,  27.7456, 234.6881],
                 ...,
                 [100.0009, 178.9919, 123.2679, 196.2153],
                 [175.3711, 198.3185, 231.4542, 280.3434],
                 [368.4505, 398.3175, 518.1219, 478.9788]]]), 'scores': tensor([[0.6030, 0.5362, 0.5060, 0.4716, 0.4550, 0.4172, 0.4056, 0.3967, 0.3598,
                 0.3229, 0.2707, 0.2680, 0.2555, 0.2454, 0.2324, 0.2316, 0.2067, 0.2027,
                 0.2022, 0.2010, 0.1952, 0.1934, 0.1921, 0.1915, 0.1900, 0.1865, 0.1848,
                 0.1837, 0.1807, 0.1792, 0.1760, 0.1732, 0.1698, 0.1648, 0.1636, 0.1600,
                 0.1595, 0.1585, 0.1555, 0.1547, 0.1529, 0.1523, 0.1518, 0.1502, 0.1478,
                 0.1432, 0.1393, 0.1369, 0.1360, 0.1356, 0.1341, 0.1318, 0.1289, 0.1285,
                 0.1260, 0.1236, 0.1187, 0.1185, 0.1182, 0.1180, 0.1179, 0.1175, 0.1174,
                 0.1169, 0.1167, 0.1138, 0.1123, 0.1119, 0.1114, 0.1112, 0.1103, 0.1103,
                 0.1102, 0.1101, 0.1092, 0.1092, 0.1083, 0.1081, 0.1059, 0.1050, 0.1040,
                 0.1031, 0.1025, 0.1021, 0.1009, 0.0988, 0.0972, 0.0961, 0.0952, 0.0948,
                 0.0943, 0.0942, 0.0942, 0.0942, 0.0940, 0.0934, 0.0934, 0.0932, 0.0930,
                 0.0925, 0.0924, 0.0922, 0.0921, 0.0915, 0.0913, 0.0910, 0.0907, 0.0899,
                 0.0898, 0.0883, 0.0872, 0.0868, 0.0864, 0.0864, 0.0859, 0.0849, 0.0835,
                 0.0833, 0.0830, 0.0819, 0.0814, 0.0810, 0.0804, 0.0800, 0.0790, 0.0788,
                 0.0786, 0.0780, 0.0779, 0.0776, 0.0776, 0.0773, 0.0771, 0.0770, 0.0766,
                 0.0765, 0.0760, 0.0759, 0.0752, 0.0751, 0.0749, 0.0748, 0.0744, 0.0742,
                 0.0740, 0.0736, 0.0732, 0.0732, 0.0731, 0.0730, 0.0722, 0.0718, 0.0717,
                 0.0717, 0.0710, 0.0706, 0.0705, 0.0693, 0.0692, 0.0691, 0.0686, 0.0684,
                 0.0680, 0.0678, 0.0677, 0.0677, 0.0672, 0.0670, 0.0656, 0.0656, 0.0655,
                 0.0655, 0.0654, 0.0654, 0.0652, 0.0645, 0.0640, 0.0639, 0.0638, 0.0631,
                 0.0628, 0.0626, 0.0619, 0.0617, 0.0612, 0.0612, 0.0611, 0.0610, 0.0609,
                 0.0606, 0.0606, 0.0604, 0.0604, 0.0604, 0.0604, 0.0601, 0.0597, 0.0597,
                 0.0597, 0.0596, 0.0595, 0.0593, 0.0589, 0.0586, 0.0586, 0.0586, 0.0585,
                 0.0578, 0.0574, 0.0573, 0.0573, 0.0571, 0.0570, 0.0568, 0.0567, 0.0562,
                 0.0557, 0.0556, 0.0555, 0.0555, 0.0555, 0.0544, 0.0542, 0.0535, 0.0533,
                 0.0533, 0.0531, 0.0530, 0.0527, 0.0525, 0.0522, 0.0521, 0.0519, 0.0518,
                 0.0517, 0.0515, 0.0514, 0.0512, 0.0507, 0.0507, 0.0507, 0.0505, 0.0504,
                 0.0503, 0.0502, 0.0502, 0.0499, 0.0496, 0.0495, 0.0487, 0.0487, 0.0485,
                 0.0484, 0.0482, 0.0480, 0.0480, 0.0480, 0.0476, 0.0472, 0.0467, 0.0466,
                 0.0462, 0.0460, 0.0460, 0.0460, 0.0459, 0.0456, 0.0455, 0.0455, 0.0455,
                 0.0454, 0.0451, 0.0451, 0.0445, 0.0444, 0.0444, 0.0443, 0.0441, 0.0441,
                 0.0440, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437, 0.0436, 0.0435,
                 0.0432, 0.0430, 0.0429, 0.0426, 0.0426, 0.0425, 0.0423, 0.0419, 0.0419,
                 0.0419, 0.0417, 0.0417]])}[0m
[38;5;10m[I] onnxrt-runner-N1-05/19/25-15:35:51  | Completed 1 iteration(s) in 135 ms | Average inference time: 135 ms.[0m
[38;5;13m[V] Successfully ran: ['trt-runner-N1-05/19/25-15:35:51', 'onnxrt-runner-N1-05/19/25-15:35:51'][0m
[38;5;14m[I] Accuracy Comparison | trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) with '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([-0.8963,  0.6956, -0.5254,  ...,  0.2678,  2.4232, -0.0334])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([-0.0296,  0.5031,  1.3804,  ..., -2.6687,  6.3413, -5.3056])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 | Stats: mean=0.18473, std-dev=1.486, var=2.2083, median=0.22698, min=-6.4988 at (0, 284, 4, 0, 0), max=6.6612 at (0, 104, 1, 0, 0), avg-magnitude=1.1564, p90=2.0059, p95=2.5988, p99=3.9355
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-7.84  , -6.28  ) |        4.0 | 
                (-6.28  , -4.73  ) |       95.0 | 
                (-4.73  , -3.18  ) |      718.0 | #
                (-3.18  , -1.62  ) |     5679.0 | #########
                (-1.62  , -0.0675) |    17004.0 | ############################
                (-0.0675, 1.49   ) |    24183.0 | ########################################
                (1.49   , 3.04   ) |     8097.0 | #############
                (3.04   , 4.6    ) |     1629.0 | ##
                (4.6    , 6.15   ) |      190.0 | 
                (6.15   , 7.7    ) |        1.0 | 
[I]         onnxrt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 | Stats: mean=0.15685, std-dev=2.0738, var=4.3007, median=0.14499, min=-7.8389 at (0, 123, 4, 0, 0), max=7.7039 at (0, 215, 7, 11, 0), avg-magnitude=1.6873, p90=2.8771, p95=3.6862, p99=4.361
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-7.84  , -6.28  ) |       30.0 | 
                (-6.28  , -4.73  ) |      220.0 | 
                (-4.73  , -3.18  ) |     2864.0 | #######
                (-3.18  , -1.62  ) |     8968.0 | ######################
                (-1.62  , -0.0675) |    14244.0 | ###################################
                (-0.0675, 1.49   ) |    15840.0 | ########################################
                (1.49   , 3.04   ) |    10283.0 | #########################
                (3.04   , 4.6    ) |     4721.0 | ###########
                (4.6    , 6.15   ) |      283.0 | 
                (6.15   , 7.7    ) |      147.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=10.037] OR [rel=99870] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=2.0969, std-dev=1.4853, var=2.2062, median=1.9584, min=3.1531e-05 at (0, 135, 0, 11, 1), max=10.037 at (0, 128, 4, 7, 0), avg-magnitude=2.0969, p90=4.0913, p95=4.6173, p99=5.9926
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (3.15e-05, 1   ) |    17208.0 | ########################################
                    (1       , 2.01) |    12220.0 | ############################
                    (2.01    , 3.01) |    11786.0 | ###########################
                    (3.01    , 4.01) |    10031.0 | #######################
                    (4.01    , 5.02) |     4533.0 | ##########
                    (5.02    , 6.02) |     1264.0 | ##
                    (6.02    , 7.03) |      369.0 | 
                    (7.03    , 8.03) |      135.0 | 
                    (8.03    , 9.03) |       42.0 | 
                    (9.03    , 10  ) |       12.0 | 
[I]             Relative Difference | Stats: mean=6.9856, std-dev=442.8, var=1.9607e+05, median=1.2172, min=4.8976e-05 at (0, 135, 0, 11, 1), max=99870 at (0, 120, 4, 4, 0), avg-magnitude=6.9856, p90=4.2659, p95=8.2202, p99=42.841
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (4.9e-05 , 9.99e+03) |    57596.0 | ########################################
                    (9.99e+03, 2e+04   ) |        2.0 | 
                    (2e+04   , 3e+04   ) |        1.0 | 
                    (3e+04   , 3.99e+04) |        0.0 | 
                    (3.99e+04, 4.99e+04) |        0.0 | 
                    (4.99e+04, 5.99e+04) |        0.0 | 
                    (5.99e+04, 6.99e+04) |        0.0 | 
                    (6.99e+04, 7.99e+04) |        0.0 | 
                    (7.99e+04, 8.99e+04) |        0.0 | 
                    (8.99e+04, 9.99e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [trt-runner-N1-05/19/25-15:35:51] and '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) with '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([-0.4100,  0.5429, -0.8908,  ...,  2.2683, -2.3510,  2.1080])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([ 0.6574,  1.0883,  0.8281,  ..., -0.7307,  3.4448, -1.8215])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 | Stats: mean=0.024758, std-dev=1.7298, var=2.992, median=0.076395, min=-27.564 at (0, 255, 7, 7, 1), max=6.1561 at (0, 255, 6, 0, 1), avg-magnitude=1.1593, p90=1.8458, p95=2.3048, p99=3.3353
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-33.6, -29.2) |        0.0 | 
                (-29.2, -24.9) |        4.0 | 
                (-24.9, -20.5) |       21.0 | 
                (-20.5, -16.1) |       55.0 | 
                (-16.1, -11.7) |      102.0 | 
                (-11.7, -7.36) |       69.0 | 
                (-7.36, -2.99) |      999.0 | 
                (-2.99, 1.39 ) |    46149.0 | ########################################
                (1.39 , 5.76 ) |    10198.0 | ########
                (5.76 , 10.1 ) |        3.0 | 
[I]         onnxrt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 | Stats: mean=0.062898, std-dev=2.5249, var=6.3749, median=0.12922, min=-33.608 at (0, 49, 7, 7, 1), max=10.137 at (0, 272, 1, 11, 1), avg-magnitude=1.8338, p90=3.0449, p95=3.621, p99=4.9222
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-33.6, -29.2) |       12.0 | 
                (-29.2, -24.9) |       21.0 | 
                (-24.9, -20.5) |       49.0 | 
                (-20.5, -16.1) |       81.0 | 
                (-16.1, -11.7) |       69.0 | 
                (-11.7, -7.36) |       43.0 | 
                (-7.36, -2.99) |     4773.0 | #####
                (-2.99, 1.39 ) |    35981.0 | ########################################
                (1.39 , 5.76 ) |    16204.0 | ##################
                (5.76 , 10.1 ) |      367.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=24.361] OR [rel=8.7601e+05] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=2.203, std-dev=1.5714, var=2.4692, median=2.0496, min=9.1735e-06 at (0, 130, 3, 8, 1), max=24.361 at (0, 101, 7, 7, 1), avg-magnitude=2.203, p90=4.1485, p95=4.6211, p99=5.8617
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (9.17e-06, 2.44) |    33320.0 | ########################################
                    (2.44    , 4.87) |    22282.0 | ##########################
                    (4.87    , 7.31) |     1753.0 | ##
                    (7.31    , 9.74) |      131.0 | 
                    (9.74    , 12.2) |       54.0 | 
                    (12.2    , 14.6) |       20.0 | 
                    (14.6    , 17.1) |       14.0 | 
                    (17.1    , 19.5) |       12.0 | 
                    (19.5    , 21.9) |        8.0 | 
                    (21.9    , 24.4) |        6.0 | 
[I]             Relative Difference | Stats: mean=22.974, std-dev=3736.6, var=1.3962e+07, median=1.2494, min=7.6578e-05 at (0, 138, 6, 0, 0), max=8.7601e+05 at (0, 149, 4, 9, 0), avg-magnitude=22.974, p90=3.9801, p95=7.4596, p99=36.231
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (7.66e-05, 8.76e+04) |    57598.0 | ########################################
                    (8.76e+04, 1.75e+05) |        0.0 | 
                    (1.75e+05, 2.63e+05) |        1.0 | 
                    (2.63e+05, 3.5e+05 ) |        0.0 | 
                    (3.5e+05 , 4.38e+05) |        0.0 | 
                    (4.38e+05, 5.26e+05) |        0.0 | 
                    (5.26e+05, 6.13e+05) |        0.0 | 
                    (6.13e+05, 7.01e+05) |        0.0 | 
                    (7.01e+05, 7.88e+05) |        0.0 | 
                    (7.88e+05, 8.76e+05) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [trt-runner-N1-05/19/25-15:35:51] and '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) with '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([-0.9894, -0.2692, -1.7247,  ...,  0.6555, -1.1561,  0.2743])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([-0.4735,  0.0336, -0.1407,  ..., -2.2724,  3.3272, -3.5136])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 | Stats: mean=0.032501, std-dev=1.6585, var=2.7505, median=0.088473, min=-6.8294 at (0, 83, 2, 8, 1), max=8.2032 at (0, 218, 4, 4, 0), avg-magnitude=1.2812, p90=2.0809, p95=2.7358, p99=3.9701
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.32 , -7.57 ) |        0.0 | 
                (-7.57 , -5.82 ) |       93.0 | 
                (-5.82 , -4.07 ) |      389.0 | 
                (-4.07 , -2.31 ) |     4247.0 | ######
                (-2.31 , -0.561) |    14609.0 | ######################
                (-0.561, 1.19  ) |    25428.0 | ########################################
                (1.19  , 2.94  ) |    10606.0 | ################
                (2.94  , 4.7   ) |     1962.0 | ###
                (4.7   , 6.45  ) |      203.0 | 
                (6.45  , 8.2   ) |       63.0 | 
[I]         onnxrt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 | Stats: mean=0.11493, std-dev=2.2279, var=4.9633, median=0.1752, min=-9.3242 at (0, 208, 7, 7, 1), max=7.8344 at (0, 100, 4, 4, 0), avg-magnitude=1.7968, p90=3.0641, p95=3.8392, p99=4.463
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.32 , -7.57 ) |      104.0 | 
                (-7.57 , -5.82 ) |      146.0 | 
                (-5.82 , -4.07 ) |      609.0 | #
                (-4.07 , -2.31 ) |     8005.0 | #################
                (-2.31 , -0.561) |    12336.0 | ###########################
                (-0.561, 1.19  ) |    18046.0 | ########################################
                (1.19  , 2.94  ) |    11995.0 | ##########################
                (2.94  , 4.7   ) |     5958.0 | #############
                (4.7   , 6.45  ) |      377.0 | 
                (6.45  , 8.2   ) |       24.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=10.26] OR [rel=82422] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=2.1479, std-dev=1.449, var=2.0997, median=2.0179, min=5.573e-05 at (0, 91, 4, 9, 1), max=10.26 at (0, 137, 7, 8, 0), avg-magnitude=2.1479, p90=4.0975, p95=4.4671, p99=5.5242
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (5.57e-05, 1.03) |    16838.0 | ########################################
                    (1.03    , 2.05) |    12361.0 | #############################
                    (2.05    , 3.08) |    11232.0 | ##########################
                    (3.08    , 4.1 ) |    11493.0 | ###########################
                    (4.1     , 5.13) |     4622.0 | ##########
                    (5.13    , 6.16) |      832.0 | #
                    (6.16    , 7.18) |      174.0 | 
                    (7.18    , 8.21) |       39.0 | 
                    (8.21    , 9.23) |        7.0 | 
                    (9.23    , 10.3) |        2.0 | 
[I]             Relative Difference | Stats: mean=6.322, std-dev=352.05, var=1.2394e+05, median=1.1831, min=8.0245e-05 at (0, 91, 4, 9, 1), max=82422 at (0, 195, 4, 3, 1), avg-magnitude=6.322, p90=4.8908, p95=9.7208, p99=47.754
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (8.02e-05, 8.24e+03) |    57597.0 | ########################################
                    (8.24e+03, 1.65e+04) |        2.0 | 
                    (1.65e+04, 2.47e+04) |        0.0 | 
                    (2.47e+04, 3.3e+04 ) |        0.0 | 
                    (3.3e+04 , 4.12e+04) |        0.0 | 
                    (4.12e+04, 4.95e+04) |        0.0 | 
                    (4.95e+04, 5.77e+04) |        0.0 | 
                    (5.77e+04, 6.59e+04) |        0.0 | 
                    (6.59e+04, 7.42e+04) |        0.0 | 
                    (7.42e+04, 8.24e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [trt-runner-N1-05/19/25-15:35:51] and '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) with '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([ 4.3236,  2.5326,  3.0488,  ...,  0.7405, -1.0288,  0.3509])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([ 3.8864,  2.6560,  3.0121,  ...,  1.5855, -1.3272,  0.8068])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 | Stats: mean=0.18219, std-dev=1.8799, var=3.5342, median=0.39169, min=-6.2803 at (0, 148, 6, 10), max=6.6913 at (0, 35, 0, 0), avg-magnitude=1.475, p90=2.4817, p95=3.0303, p99=3.9246
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.69 , -5.33 ) |       50.0 | 
                (-5.33 , -3.98 ) |      652.0 | ##
                (-3.98 , -2.62 ) |     2201.0 | ########
                (-2.62 , -1.27 ) |     3046.0 | ###########
                (-1.27 , 0.0886) |     5697.0 | #####################
                (0.0886, 1.44  ) |    10508.0 | ########################################
                (1.44  , 2.8   ) |     4660.0 | #################
                (2.8   , 4.15  ) |     1821.0 | ######
                (4.15  , 5.51  ) |      135.0 | 
                (5.51  , 6.86  ) |       30.0 | 
[I]         onnxrt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 | Stats: mean=0.1808, std-dev=1.8751, var=3.5161, median=0.42876, min=-6.6851 at (0, 123, 6, 10), max=6.8622 at (0, 170, 0, 0), avg-magnitude=1.4761, p90=2.4666, p95=3.0216, p99=3.855
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.69 , -5.33 ) |       61.0 | 
                (-5.33 , -3.98 ) |      580.0 | ##
                (-3.98 , -2.62 ) |     2320.0 | ########
                (-2.62 , -1.27 ) |     3046.0 | ###########
                (-1.27 , 0.0886) |     5462.0 | ####################
                (0.0886, 1.44  ) |    10882.0 | ########################################
                (1.44  , 2.8   ) |     4514.0 | ################
                (2.8   , 4.15  ) |     1781.0 | ######
                (4.15  , 5.51  ) |      124.0 | 
                (5.51  , 6.86  ) |       30.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=8.3143] OR [rel=11143] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=1.0018, std-dev=1.0933, var=1.1953, median=0.59526, min=7.9989e-05 at (0, 288, 6, 3), max=8.3143 at (0, 36, 0, 0), avg-magnitude=1.0018, p90=2.6026, p95=3.3941, p99=4.7992
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (8e-05, 0.832) |    17548.0 | ########################################
                    (0.832, 1.66 ) |     5558.0 | ############
                    (1.66 , 2.49 ) |     2549.0 | #####
                    (2.49 , 3.33 ) |     1604.0 | ###
                    (3.33 , 4.16 ) |      912.0 | ##
                    (4.16 , 4.99 ) |      400.0 | 
                    (4.99 , 5.82 ) |      156.0 | 
                    (5.82 , 6.65 ) |       52.0 | 
                    (6.65 , 7.48 ) |       15.0 | 
                    (7.48 , 8.31 ) |        6.0 | 
[I]             Relative Difference | Stats: mean=4.0136, std-dev=78.863, var=6219.4, median=0.58007, min=4.7637e-05 at (0, 158, 0, 10), max=11143 at (0, 183, 1, 3), avg-magnitude=4.0136, p90=4.3993, p95=8.8208, p99=44.142
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (4.76e-05, 1.11e+03) |    28792.0 | ########################################
                    (1.11e+03, 2.23e+03) |        5.0 | 
                    (2.23e+03, 3.34e+03) |        1.0 | 
                    (3.34e+03, 4.46e+03) |        0.0 | 
                    (4.46e+03, 5.57e+03) |        1.0 | 
                    (5.57e+03, 6.69e+03) |        0.0 | 
                    (6.69e+03, 7.8e+03 ) |        0.0 | 
                    (7.8e+03 , 8.91e+03) |        0.0 | 
                    (8.91e+03, 1e+04   ) |        0.0 | 
                    (1e+04   , 1.11e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [trt-runner-N1-05/19/25-15:35:51] and '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) with '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([ 2.8505,  2.5319,  1.7828,  ...,  1.4751, -1.4106,  1.1131])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([ 2.6173,  2.4923,  1.8669,  ...,  1.1610, -2.3385,  1.4377])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 | Stats: mean=-0.018701, std-dev=1.8563, var=3.4459, median=0.29652, min=-6.8067 at (0, 50, 4, 11), max=4.7412 at (0, 289, 6, 1), avg-magnitude=1.4692, p90=2.1657, p95=2.6096, p99=3.5496
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-6.81, -5.61) |       59.0 | 
                (-5.61, -4.41) |      470.0 | ##
                (-4.41, -3.21) |     1516.0 | #######
                (-3.21, -2.02) |     2428.0 | ###########
                (-2.02, -0.82) |     3856.0 | #################
                (-0.82, 0.378) |     6688.0 | ###############################
                (0.378, 1.58 ) |     8617.0 | ########################################
                (1.58 , 2.77 ) |     4041.0 | ##################
                (2.77 , 3.97 ) |     1054.0 | ####
                (3.97 , 5.17 ) |       71.0 | 
[I]         onnxrt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 | Stats: mean=-0.01428, std-dev=1.8962, var=3.5957, median=0.30371, min=-6.7511 at (0, 50, 4, 11), max=5.1675 at (0, 222, 6, 1), avg-magnitude=1.5028, p90=2.2252, p95=2.7625, p99=3.6673
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-6.81, -5.61) |       44.0 | 
                (-5.61, -4.41) |      526.0 | ##
                (-4.41, -3.21) |     1571.0 | #######
                (-3.21, -2.02) |     2490.0 | ###########
                (-2.02, -0.82) |     3808.0 | #################
                (-0.82, 0.378) |     6518.0 | ##############################
                (0.378, 1.58 ) |     8496.0 | ########################################
                (1.58 , 2.77 ) |     3932.0 | ##################
                (2.77 , 3.97 ) |     1269.0 | #####
                (3.97 , 5.17 ) |      146.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=7.0994] OR [rel=16400] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=1.0074, std-dev=0.966, var=0.93316, median=0.70838, min=0.0001457 at (0, 284, 0, 7), max=7.0994 at (0, 59, 6, 10), avg-magnitude=1.0074, p90=2.2983, p95=2.9776, p99=4.4683
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (0.000146, 0.71) |    14430.0 | ########################################
                    (0.71    , 1.42) |     7331.0 | ####################
                    (1.42    , 2.13) |     3610.0 | ##########
                    (2.13    , 2.84) |     1782.0 | ####
                    (2.84    , 3.55) |      820.0 | ##
                    (3.55    , 4.26) |      458.0 | #
                    (4.26    , 4.97) |      219.0 | 
                    (4.97    , 5.68) |      103.0 | 
                    (5.68    , 6.39) |       39.0 | 
                    (6.39    , 7.1 ) |        8.0 | 
[I]             Relative Difference | Stats: mean=4.4303, std-dev=116.17, var=13495, median=0.60911, min=0.00011639 at (0, 50, 5, 4), max=16400 at (0, 82, 7, 8), avg-magnitude=4.4303, p90=3.9986, p95=8.1697, p99=35.183
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0.000116, 1.64e+03) |    28792.0 | ########################################
                    (1.64e+03, 3.28e+03) |        3.0 | 
                    (3.28e+03, 4.92e+03) |        2.0 | 
                    (4.92e+03, 6.56e+03) |        2.0 | 
                    (6.56e+03, 8.2e+03 ) |        0.0 | 
                    (8.2e+03 , 9.84e+03) |        0.0 | 
                    (9.84e+03, 1.15e+04) |        0.0 | 
                    (1.15e+04, 1.31e+04) |        0.0 | 
                    (1.31e+04, 1.48e+04) |        0.0 | 
                    (1.48e+04, 1.64e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [trt-runner-N1-05/19/25-15:35:51] and '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) with '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([ 3.3788,  3.7244,  2.8865,  ..., -0.7780,  0.8089, -0.5687])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([ 3.9151,  4.2549,  3.1362,  ...,  2.3757,  0.4869, -1.0527])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 | Stats: mean=0.29174, std-dev=1.6751, var=2.806, median=0.49584, min=-7.283 at (0, 156, 1, 11), max=5.2117 at (0, 96, 7, 9), avg-magnitude=1.36, p90=2.3064, p95=2.8103, p99=3.6912
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-7.28  , -5.47  ) |       72.0 | 
                (-5.47  , -3.65  ) |      417.0 | #
                (-3.65  , -1.84  ) |     2903.0 | ########
                (-1.84  , -0.0233) |     7087.0 | #####################
                (-0.0233, 1.79   ) |    13320.0 | ########################################
                (1.79   , 3.61   ) |     4651.0 | #############
                (3.61   , 5.42   ) |      350.0 | #
                (5.42   , 7.24   ) |        0.0 | 
                (7.24   , 9.05   ) |        0.0 | 
                (9.05   , 10.9   ) |        0.0 | 
[I]         onnxrt-runner-N1-05/19/25-15:35:51: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 | Stats: mean=0.29836, std-dev=1.7936, var=3.2169, median=0.486, min=-7.2331 at (0, 151, 1, 11), max=10.866 at (0, 104, 7, 9), avg-magnitude=1.4456, p90=2.4812, p95=3.0639, p99=4.11
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-7.28  , -5.47  ) |       84.0 | 
                (-5.47  , -3.65  ) |      514.0 | #
                (-3.65  , -1.84  ) |     3156.0 | ##########
                (-1.84  , -0.0233) |     7087.0 | ######################
                (-0.0233, 1.79   ) |    12565.0 | ########################################
                (1.79   , 3.61   ) |     4695.0 | ##############
                (3.61   , 5.42   ) |      682.0 | ##
                (5.42   , 7.24   ) |        9.0 | 
                (7.24   , 9.05   ) |        6.0 | 
                (9.05   , 10.9   ) |        2.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=11.784] OR [rel=7312.2] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=1.0552, std-dev=1.0075, var=1.0151, median=0.749, min=2.3365e-05 at (0, 6, 4, 7), max=11.784 at (0, 104, 7, 9), avg-magnitude=1.0552, p90=2.3892, p95=3.0726, p99=4.5205
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (2.34e-05, 1.18) |    19309.0 | ########################################
                    (1.18    , 2.36) |     6503.0 | #############
                    (2.36    , 3.54) |     2095.0 | ####
                    (3.54    , 4.71) |      653.0 | #
                    (4.71    , 5.89) |      169.0 | 
                    (5.89    , 7.07) |       45.0 | 
                    (7.07    , 8.25) |       13.0 | 
                    (8.25    , 9.43) |       10.0 | 
                    (9.43    , 10.6) |        1.0 | 
                    (10.6    , 11.8) |        2.0 | 
[I]             Relative Difference | Stats: mean=3.6452, std-dev=53.331, var=2844.2, median=0.651, min=1.4447e-05 at (0, 6, 4, 7), max=7312.2 at (0, 148, 6, 4), avg-magnitude=3.6452, p90=4.134, p95=8.5824, p99=41.281
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (1.44e-05, 731     ) |    28789.0 | ########################################
                    (731     , 1.46e+03) |        7.0 | 
                    (1.46e+03, 2.19e+03) |        2.0 | 
                    (2.19e+03, 2.92e+03) |        1.0 | 
                    (2.92e+03, 3.66e+03) |        0.0 | 
                    (3.66e+03, 4.39e+03) |        0.0 | 
                    (4.39e+03, 5.12e+03) |        0.0 | 
                    (5.12e+03, 5.85e+03) |        0.0 | 
                    (5.85e+03, 6.58e+03) |        0.0 | 
                    (6.58e+03, 7.31e+03) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [trt-runner-N1-05/19/25-15:35:51] and '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: 'labels' (dtype=int64, shape=torch.Size([1, 300])) with 'labels' (dtype=int64, shape=torch.Size([1, 300]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([67, 67, 67,  0, 67, 67, 67, 67, 67, 67, 67,  8,  8, 24,  0, 67,  8, 67,
                         8,  8,  0, 24, 67, 67,  0, 24,  0,  0,  8,  0,  8,  8,  8, 24,  0,  8,
                         8,  8, 67,  8,  8,  0, 13, 67,  0, 67,  8,  8,  8,  0,  8,  8,  0,  8,
                        26,  8,  8,  8,  0, 67,  8,  8,  8,  8,  8, 13,  0, 67,  8,  8,  8,  0,
                        67,  8,  8,  0,  2,  0,  8,  8,  0,  0,  0,  0,  8,  0, 67,  8,  8, 24,
                         8,  0,  8, 24, 26, 24,  8,  0,  8,  8,  8,  8, 67,  8,  0,  8, 58,  8,
                         8, 13,  8,  8,  0,  0,  8,  0,  8, 24,  0, 13,  0,  8,  0, 39, 74,  0,
                        79,  0,  0,  8, 24,  0, 26, 26,  8,  8,  0,  0,  8, 26,  8, 24,  0,  2,
                        13,  0, 26, 27,  0,  0,  0,  2,  2,  0, 24,  0, 67, 58,  0, 74,  0, 13,
                        24, 13,  8, 26, 26,  0,  0,  0, 79,  2, 13,  8,  8, 13,  8,  0,  1, 13,
                         2, 26, 28, 27,  2,  0,  0,  2,  9, 26, 10,  0, 39,  2, 13, 73,  0,  8,
                         8, 79, 24, 26, 24, 13, 13, 13,  2,  8,  0,  2,  0, 39, 13, 24, 13, 13,
                        13,  8, 24,  0,  0, 13,  0, 58,  9,  0, 13, 52, 13,  0, 43,  0, 79,  2,
                         2, 13, 58, 34, 24,  0, 13,  0, 13, 24, 58, 79, 24,  8])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([ 0,  8,  8, 26,  8,  0,  0, 28,  8,  8,  8,  0,  0, 67,  8,  8,  0, 24,
                        67, 24, 13,  0,  8,  8, 67,  0,  8,  8, 67, 26,  0,  0, 26, 67,  8, 26,
                         0,  0,  8,  0, 13,  8,  8, 28, 13,  8,  0, 28, 24,  8,  0, 26, 28, 26,
                         8, 24,  0, 26,  8, 24,  0, 24,  0, 67,  0, 67, 67,  8, 24,  0, 26, 67,
                         0, 67, 79, 58,  8,  8,  0,  0, 28,  8,  8, 56,  0, 58, 28, 67, 67,  0,
                        76, 26, 24, 73,  8, 60, 24, 28, 24, 13,  0,  0, 24,  0,  8, 28, 24, 11,
                        43,  2, 26,  0, 26,  8, 28,  8, 24,  0, 24, 28, 24,  2, 24, 26, 26, 26,
                        28, 39, 26, 26, 13, 56, 24,  8,  0, 26, 67, 24, 24, 24, 26,  8, 28,  0,
                        28,  2, 24, 24, 13,  8, 56, 67, 26, 24,  0, 26,  0, 76, 24,  9, 34, 58,
                        67, 28, 13, 24, 13,  8,  8, 24, 56, 79, 56, 24,  0, 12, 25, 43, 26,  3,
                         0, 27, 58, 74,  3,  8,  2, 26, 58,  8, 74, 60,  9,  0, 24, 26, 24, 24,
                         0, 11,  8,  0, 13,  0,  0,  0,  1, 79,  8, 28, 24,  1,  2, 59,  0, 58,
                        24, 28,  0,  9, 24,  8, 79,  8, 26, 26, 24,  8, 24, 67,  2,  8,  0, 26,
                        13, 26, 24,  0, 58,  8, 67,  8, 11, 25,  2, 56,  2, 28])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: labels | Stats: mean=16.573, std-dev=21.617, var=467.29, median=8, min=0 at (0, 4), max=79 at (0, 159), avg-magnitude=16.573, p90=67, p95=67, p99=79
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         93 | ##############################
                (7 , 15) |        123 | ########################################
                (15, 23) |          0 | 
                (23, 31) |         40 | #############
                (31, 39) |          4 | #
                (39, 47) |          1 | 
                (47, 55) |          1 | 
                (55, 63) |          5 | #
                (63, 71) |         25 | ########
                (71, 79) |          8 | ##
[I]         onnxrt-runner-N1-05/19/25-15:35:51: labels | Stats: mean=20.497, std-dev=21.41, var=458.4, median=12, min=0 at (0, 0), max=79 at (0, 97), avg-magnitude=20.497, p90=59, p95=67, p99=79
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         73 | ################################
                (7 , 15) |         89 | #######################################
                (15, 23) |          0 | 
                (23, 31) |         90 | ########################################
                (31, 39) |          2 | 
                (39, 47) |          2 | 
                (47, 55) |          0 | 
                (55, 63) |         16 | #######
                (63, 71) |         19 | ########
                (71, 79) |          9 | ####
[I]         Error Metrics: labels
[I]             Minimum Required Tolerance: elemwise error | [abs=79] OR [rel=nan] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=22.05, std-dev=21.563, var=464.95, median=16, min=0 at (0, 1), max=79 at (0, 268), avg-magnitude=22.05, p90=59, p95=67, p99=71
[I]                 ---- Histogram ----
                    Bin Range|  Num Elems | Visualization
                    (0 , 7 ) |         76 | ########################################
                    (7 , 15) |         70 | ####################################
                    (15, 23) |         39 | ####################
                    (23, 31) |         36 | ##################
                    (31, 39) |         13 | ######
                    (39, 47) |         14 | #######
                    (47, 55) |          7 | ###
                    (55, 63) |         25 | #############
                    (63, 71) |         17 | ########
                    (71, 79) |          3 | #
[I]             Relative Difference | Stats: mean=nan, std-dev=nan, var=nan, median=nan, min=nan at (0, 8), max=nan at (0, 8), avg-magnitude=nan, p90=nan, p95=nan, p99=nan
[38;5;13m[V]                 Could not generate histogram. Note: Error was: torch.histogramdd: dimension 0's range [-nan, -nan] is not finite[0m
[I]                 
[38;5;104m[X]         Finished comparing: 'labels' (dtype=int64, shape=torch.Size([1, 300])) [trt-runner-N1-05/19/25-15:35:51] and 'labels' (dtype=int64, shape=torch.Size([1, 300])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: 'labels' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) with 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([356.0063, 142.7068, 379.9229,  ..., 360.3580, 517.6758, 502.9109])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([262.9576,  71.5327, 547.7870,  ..., 398.3175, 518.1219, 478.9788])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: boxes | Stats: mean=233.11, std-dev=124.88, var=15594, median=213.33, min=-26.209 at (0, 202, 0), max=665.73 at (0, 151, 2), avg-magnitude=233.29, p90=378.4, p95=509.73, p99=616.82
[I]             ---- Histogram ----
                Bin Range    |  Num Elems | Visualization
                (-26.2, 43 ) |       66.0 | #####
                (43   , 112) |       67.0 | #####
                (112  , 181) |      233.0 | ###################
                (181  , 251) |      490.0 | ########################################
                (251  , 320) |      127.0 | ##########
                (320  , 389) |      110.0 | ########
                (389  , 458) |       28.0 | ##
                (458  , 527) |       23.0 | #
                (527  , 597) |       19.0 | #
                (597  , 666) |       37.0 | ###
[I]         onnxrt-runner-N1-05/19/25-15:35:51: boxes | Stats: mean=260.78, std-dev=140.6, var=19769, median=224.33, min=-1.3738 at (0, 105, 0), max=648.52 at (0, 276, 2), avg-magnitude=260.79, p90=479.46, p95=594.98, p99=622.91
[I]             ---- Histogram ----
                Bin Range    |  Num Elems | Visualization
                (-26.2, 43 ) |       45.0 | ###
                (43   , 112) |       79.0 | ######
                (112  , 181) |      169.0 | ##############
                (181  , 251) |      455.0 | ########################################
                (251  , 320) |      142.0 | ############
                (320  , 389) |      123.0 | ##########
                (389  , 458) |       40.0 | ###
                (458  , 527) |       52.0 | ####
                (527  , 597) |       40.0 | ###
                (597  , 666) |       55.0 | ####
[I]         Error Metrics: boxes
[I]             Minimum Required Tolerance: elemwise error | [abs=609.57] OR [rel=490.71] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=129.48, std-dev=131.18, var=17209, median=77.692, min=0.022171 at (0, 137, 3), max=609.57 at (0, 45, 0), avg-magnitude=129.48, p90=316.78, p95=380.81, p99=556.13
[I]                 ---- Histogram ----
                    Bin Range     |  Num Elems | Visualization
                    (0.0222, 61 ) |      538.0 | ########################################
                    (61    , 122) |      184.0 | #############
                    (122   , 183) |      123.0 | #########
                    (183   , 244) |      125.0 | #########
                    (244   , 305) |      100.0 | #######
                    (305   , 366) |       62.0 | ####
                    (366   , 427) |       21.0 | #
                    (427   , 488) |       17.0 | #
                    (488   , 549) |       15.0 | #
                    (549   , 610) |       15.0 | #
[I]             Relative Difference | Stats: mean=2.9778, std-dev=26.487, var=701.56, median=0.34897, min=9.8246e-05 at (0, 137, 3), max=490.71 at (0, 35, 0), avg-magnitude=2.9778, p90=1.3848, p95=3.0749, p99=16.559
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (9.82e-05, 49.1) |     1191.0 | ########################################
                    (49.1    , 98.1) |        0.0 | 
                    (98.1    , 147 ) |        1.0 | 
                    (147     , 196 ) |        1.0 | 
                    (196     , 245 ) |        2.0 | 
                    (245     , 294 ) |        1.0 | 
                    (294     , 343 ) |        2.0 | 
                    (343     , 393 ) |        1.0 | 
                    (393     , 442 ) |        0.0 | 
                    (442     , 491 ) |        1.0 | 
[38;5;104m[X]         Finished comparing: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [trt-runner-N1-05/19/25-15:35:51] and 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: 'boxes' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: 'scores' (dtype=float32, shape=torch.Size([1, 300])) with 'scores' (dtype=float32, shape=torch.Size([1, 300]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;104m[X]             trt-runner-N1-05/19/25-15:35:51     | Mismatched values:
                tensor([0.3165, 0.2771, 0.2539, 0.2233, 0.2196, 0.1992, 0.1922, 0.1828, 0.1803,
                        0.1578, 0.1537, 0.1474, 0.1458, 0.1424, 0.1322, 0.1308, 0.1132, 0.1127,
                        0.1094, 0.1087, 0.1076, 0.1073, 0.1053, 0.1036, 0.1019, 0.1015, 0.1013,
                        0.1010, 0.1002, 0.0983, 0.0972, 0.0970, 0.0955, 0.0949, 0.0944, 0.0944,
                        0.0942, 0.0933, 0.0918, 0.0886, 0.0883, 0.0872, 0.0861, 0.0837, 0.0818,
                        0.0804, 0.0795, 0.0793, 0.0788, 0.0787, 0.0764, 0.0757, 0.0756, 0.0751,
                        0.0742, 0.0733, 0.0724, 0.0711, 0.0701, 0.0697, 0.0692, 0.0678, 0.0677,
                        0.0676, 0.0672, 0.0671, 0.0661, 0.0659, 0.0658, 0.0657, 0.0651, 0.0649,
                        0.0648, 0.0640, 0.0634, 0.0626, 0.0624, 0.0622, 0.0613, 0.0611, 0.0606,
                        0.0605, 0.0602, 0.0597, 0.0595, 0.0592, 0.0588, 0.0587, 0.0574, 0.0574,
                        0.0573, 0.0563, 0.0560, 0.0560, 0.0556, 0.0554, 0.0552, 0.0548, 0.0544,
                        0.0543, 0.0542, 0.0540, 0.0537, 0.0534, 0.0533, 0.0532, 0.0531, 0.0528,
                        0.0528, 0.0525, 0.0522, 0.0519, 0.0514, 0.0513, 0.0512, 0.0508, 0.0507,
                        0.0504, 0.0503, 0.0499, 0.0498, 0.0493, 0.0488, 0.0482, 0.0480, 0.0476,
                        0.0475, 0.0472, 0.0469, 0.0463, 0.0462, 0.0461, 0.0457, 0.0454, 0.0453,
                        0.0449, 0.0446, 0.0442, 0.0441, 0.0441, 0.0441, 0.0440, 0.0436, 0.0434,
                        0.0430, 0.0428, 0.0428, 0.0426, 0.0421, 0.0420, 0.0414, 0.0412, 0.0412,
                        0.0410, 0.0409, 0.0407, 0.0405, 0.0405, 0.0404, 0.0400, 0.0395, 0.0394,
                        0.0394, 0.0393, 0.0388, 0.0387, 0.0385, 0.0385, 0.0382, 0.0382, 0.0380,
                        0.0379, 0.0379, 0.0378, 0.0378, 0.0377, 0.0374, 0.0373, 0.0371, 0.0370,
                        0.0370, 0.0367, 0.0367, 0.0366, 0.0366, 0.0363, 0.0362, 0.0360, 0.0359,
                        0.0358, 0.0358, 0.0357, 0.0354, 0.0353, 0.0352, 0.0351, 0.0349, 0.0348,
                        0.0347, 0.0347, 0.0346, 0.0345, 0.0344, 0.0344, 0.0343, 0.0341, 0.0341,
                        0.0341, 0.0341, 0.0340, 0.0340, 0.0340, 0.0339, 0.0335, 0.0333, 0.0333,
                        0.0333, 0.0333, 0.0332, 0.0330, 0.0330, 0.0329, 0.0327, 0.0325, 0.0325,
                        0.0325, 0.0324, 0.0324, 0.0324, 0.0323, 0.0322, 0.0321, 0.0320, 0.0319,
                        0.0318, 0.0318, 0.0318, 0.0318, 0.0316, 0.0316, 0.0316, 0.0315, 0.0313,
                        0.0312, 0.0312, 0.0311, 0.0311, 0.0311, 0.0311, 0.0309, 0.0309, 0.0308,
                        0.0307, 0.0306, 0.0305, 0.0305, 0.0303, 0.0303, 0.0302, 0.0302, 0.0302,
                        0.0302, 0.0301, 0.0301, 0.0299, 0.0299, 0.0297, 0.0297, 0.0296, 0.0296,
                        0.0296, 0.0296, 0.0295, 0.0295, 0.0294, 0.0292, 0.0292, 0.0292, 0.0290,
                        0.0289, 0.0289, 0.0289, 0.0287, 0.0287, 0.0286, 0.0286, 0.0286, 0.0284,
                        0.0283, 0.0283, 0.0282, 0.0282, 0.0281, 0.0279, 0.0279, 0.0278, 0.0278,
                        0.0277, 0.0276, 0.0275])[0m
[38;5;104m[X]             onnxrt-runner-N1-05/19/25-15:35:51  | Mismatched values:
                tensor([0.6030, 0.5362, 0.5060, 0.4716, 0.4550, 0.4172, 0.4056, 0.3967, 0.3598,
                        0.3229, 0.2707, 0.2680, 0.2555, 0.2454, 0.2324, 0.2316, 0.2067, 0.2027,
                        0.2022, 0.2010, 0.1952, 0.1934, 0.1921, 0.1915, 0.1900, 0.1865, 0.1848,
                        0.1837, 0.1807, 0.1792, 0.1760, 0.1732, 0.1698, 0.1648, 0.1636, 0.1600,
                        0.1595, 0.1585, 0.1555, 0.1547, 0.1529, 0.1523, 0.1518, 0.1502, 0.1478,
                        0.1432, 0.1393, 0.1369, 0.1360, 0.1356, 0.1341, 0.1318, 0.1289, 0.1285,
                        0.1260, 0.1236, 0.1187, 0.1185, 0.1182, 0.1180, 0.1179, 0.1175, 0.1174,
                        0.1169, 0.1167, 0.1138, 0.1123, 0.1119, 0.1114, 0.1112, 0.1103, 0.1103,
                        0.1102, 0.1101, 0.1092, 0.1092, 0.1083, 0.1081, 0.1059, 0.1050, 0.1040,
                        0.1031, 0.1025, 0.1021, 0.1009, 0.0988, 0.0972, 0.0961, 0.0952, 0.0948,
                        0.0943, 0.0942, 0.0942, 0.0942, 0.0940, 0.0934, 0.0934, 0.0932, 0.0930,
                        0.0925, 0.0924, 0.0922, 0.0921, 0.0915, 0.0913, 0.0910, 0.0907, 0.0899,
                        0.0898, 0.0883, 0.0872, 0.0868, 0.0864, 0.0864, 0.0859, 0.0849, 0.0835,
                        0.0833, 0.0830, 0.0819, 0.0814, 0.0810, 0.0804, 0.0800, 0.0790, 0.0788,
                        0.0786, 0.0780, 0.0779, 0.0776, 0.0776, 0.0773, 0.0771, 0.0770, 0.0766,
                        0.0765, 0.0760, 0.0759, 0.0752, 0.0751, 0.0749, 0.0748, 0.0744, 0.0742,
                        0.0740, 0.0736, 0.0732, 0.0732, 0.0731, 0.0730, 0.0722, 0.0718, 0.0717,
                        0.0717, 0.0710, 0.0706, 0.0705, 0.0693, 0.0692, 0.0691, 0.0686, 0.0684,
                        0.0680, 0.0678, 0.0677, 0.0677, 0.0672, 0.0670, 0.0656, 0.0656, 0.0655,
                        0.0655, 0.0654, 0.0654, 0.0652, 0.0645, 0.0640, 0.0639, 0.0638, 0.0631,
                        0.0628, 0.0626, 0.0619, 0.0617, 0.0612, 0.0612, 0.0611, 0.0610, 0.0609,
                        0.0606, 0.0606, 0.0604, 0.0604, 0.0604, 0.0604, 0.0601, 0.0597, 0.0597,
                        0.0597, 0.0596, 0.0595, 0.0593, 0.0589, 0.0586, 0.0586, 0.0586, 0.0585,
                        0.0578, 0.0574, 0.0573, 0.0573, 0.0571, 0.0570, 0.0568, 0.0567, 0.0562,
                        0.0557, 0.0556, 0.0555, 0.0555, 0.0555, 0.0544, 0.0542, 0.0535, 0.0533,
                        0.0533, 0.0531, 0.0530, 0.0527, 0.0525, 0.0522, 0.0521, 0.0519, 0.0518,
                        0.0517, 0.0515, 0.0514, 0.0512, 0.0507, 0.0507, 0.0507, 0.0505, 0.0504,
                        0.0503, 0.0502, 0.0502, 0.0499, 0.0496, 0.0495, 0.0487, 0.0487, 0.0485,
                        0.0484, 0.0482, 0.0480, 0.0480, 0.0480, 0.0476, 0.0472, 0.0467, 0.0466,
                        0.0462, 0.0460, 0.0460, 0.0460, 0.0459, 0.0456, 0.0455, 0.0455, 0.0455,
                        0.0454, 0.0451, 0.0451, 0.0445, 0.0444, 0.0444, 0.0443, 0.0441, 0.0441,
                        0.0440, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437, 0.0436, 0.0435,
                        0.0432, 0.0430, 0.0429, 0.0426, 0.0426, 0.0425, 0.0423, 0.0419, 0.0419,
                        0.0419, 0.0417, 0.0417])[0m
[I]         trt-runner-N1-05/19/25-15:35:51: scores | Stats: mean=0.056208, std-dev=0.040044, var=0.0016035, median=0.041683, min=0.027545 at (0, 299), max=0.31648 at (0, 0), avg-magnitude=0.056208, p90=0.097345, p95=0.13087, p99=0.22364
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (0.0275, 0.0851) |      257.0 | ########################################
                (0.0851, 0.143 ) |       30.0 | ####
                (0.143 , 0.2   ) |        8.0 | #
                (0.2   , 0.258 ) |        3.0 | 
                (0.258 , 0.315 ) |        1.0 | 
                (0.315 , 0.373 ) |        1.0 | 
                (0.373 , 0.43  ) |        0.0 | 
                (0.43  , 0.488 ) |        0.0 | 
                (0.488 , 0.545 ) |        0.0 | 
                (0.545 , 0.603 ) |        0.0 | 
[I]         onnxrt-runner-N1-05/19/25-15:35:51: scores | Stats: mean=0.097993, std-dev=0.080413, var=0.0064662, median=0.072622, min=0.041714 at (0, 299), max=0.60297 at (0, 0), avg-magnitude=0.097993, p90=0.17633, p95=0.23168, p99=0.47193
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (0.0275, 0.0851) |      185.0 | ########################################
                (0.0851, 0.143 ) |       69.0 | ##############
                (0.143 , 0.2   ) |       26.0 | #####
                (0.2   , 0.258 ) |        8.0 | #
                (0.258 , 0.315 ) |        2.0 | 
                (0.315 , 0.373 ) |        2.0 | 
                (0.373 , 0.43  ) |        3.0 | 
                (0.43  , 0.488 ) |        2.0 | 
                (0.488 , 0.545 ) |        2.0 | 
                (0.545 , 0.603 ) |        1.0 | 
[I]         Error Metrics: scores
[I]             Minimum Required Tolerance: elemwise error | [abs=0.28649] OR [rel=0.53935] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.041785, std-dev=0.040676, var=0.0016545, median=0.030674, min=0.014102 at (0, 298), max=0.28649 at (0, 0), avg-magnitude=0.041785, p90=0.078938, p95=0.10021, p99=0.24829
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (0.0141, 0.0413) |      215.0 | ########################################
                    (0.0413, 0.0686) |       50.0 | #########
                    (0.0686, 0.0958) |       19.0 | ###
                    (0.0958, 0.123 ) |        6.0 | #
                    (0.123 , 0.15  ) |        0.0 | 
                    (0.15  , 0.178 ) |        1.0 | 
                    (0.178 , 0.205 ) |        1.0 | 
                    (0.205 , 0.232 ) |        3.0 | 
                    (0.232 , 0.259 ) |        4.0 | 
                    (0.259 , 0.286 ) |        1.0 | 
[I]             Relative Difference | Stats: mean=0.4048, std-dev=0.035259, var=0.0012432, median=0.41045, min=0.3366 at (0, 273), max=0.53935 at (0, 7), avg-magnitude=0.4048, p90=0.43745, p95=0.45253, p99=0.52242
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (0.337, 0.357) |       41.0 | ##############
                    (0.357, 0.377) |       16.0 | #####
                    (0.377, 0.397) |       38.0 | #############
                    (0.397, 0.418) |      114.0 | ########################################
                    (0.418, 0.438) |       61.0 | #####################
                    (0.438, 0.458) |       16.0 | #####
                    (0.458, 0.479) |        5.0 | #
                    (0.479, 0.499) |        2.0 | 
                    (0.499, 0.519) |        3.0 | #
                    (0.519, 0.539) |        4.0 | #
[38;5;104m[X]         Finished comparing: 'scores' (dtype=float32, shape=torch.Size([1, 300])) [trt-runner-N1-05/19/25-15:35:51] and 'scores' (dtype=float32, shape=torch.Size([1, 300])) [onnxrt-runner-N1-05/19/25-15:35:51][0m
[38;5;9m[E]         FAILED | Output: 'scores' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;9m[E]     FAILED | Mismatched outputs: ['/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0', '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0', '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0', '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0', '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0', '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0', 'labels', 'boxes', 'scores'][0m
[38;5;104m[X]     Finished comparing trt-runner-N1-05/19/25-15:35:51 with onnxrt-runner-N1-05/19/25-15:35:51[0m
[38;5;9m[E] Accuracy Summary | trt-runner-N1-05/19/25-15:35:51 vs. onnxrt-runner-N1-05/19/25-15:35:51 | Passed: 0/1 iterations | Pass Rate: 0.0%[0m
